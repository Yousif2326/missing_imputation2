{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# import scipy.stats as stats\n",
    "# import statsmodels.api as sm\n",
    "import miceforest as mf\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, Input\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "\n",
    "\n",
    "from tensorflow.keras.layers import Lambda, Dropout, Concatenate\n",
    "from tensorflow.keras import losses\n",
    "from tensorflow.keras import backend as K\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error as sk_mse\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.impute import SimpleImputer\n",
    "import warnings\n",
    "import time\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "import random\n",
    "from sklearn.model_selection import KFold\n",
    "from scipy.stats import ks_2samp\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Import + only Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>operation_date</th>\n",
       "      <th>redcap_event_name</th>\n",
       "      <th>ComplicationDate</th>\n",
       "      <th>dob</th>\n",
       "      <th>qol_date</th>\n",
       "      <th>age_diagnosis</th>\n",
       "      <th>gender</th>\n",
       "      <th>overall_primary_tumour</th>\n",
       "      <th>overall_regional_ln</th>\n",
       "      <th>...</th>\n",
       "      <th>a_e5</th>\n",
       "      <th>a_e6</th>\n",
       "      <th>a_e7</th>\n",
       "      <th>a_c6</th>\n",
       "      <th>a_c2</th>\n",
       "      <th>a_act11</th>\n",
       "      <th>readmission_30d</th>\n",
       "      <th>postop_comp</th>\n",
       "      <th>los</th>\n",
       "      <th>DischargeDate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaT</td>\n",
       "      <td>baseline_arm_1</td>\n",
       "      <td>NaT</td>\n",
       "      <td>2049-08-04</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2012-04-27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2011-08-26</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2011-08-26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>2010-02-20</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>16.0</td>\n",
       "      <td>2010-03-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>2009-02-25</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8.0</td>\n",
       "      <td>2009-03-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>2008-02-22</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13.0</td>\n",
       "      <td>2008-03-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18182</th>\n",
       "      <td>1770</td>\n",
       "      <td>NaT</td>\n",
       "      <td>preoperative_arm_1</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18183</th>\n",
       "      <td>1770</td>\n",
       "      <td>NaT</td>\n",
       "      <td>preoperative_arm_1</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18184</th>\n",
       "      <td>1770</td>\n",
       "      <td>NaT</td>\n",
       "      <td>preoperative_arm_1</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>2025-02-24</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18185</th>\n",
       "      <td>1770</td>\n",
       "      <td>NaT</td>\n",
       "      <td>baseline_arm_1</td>\n",
       "      <td>NaT</td>\n",
       "      <td>1982-10-12</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18186</th>\n",
       "      <td>1770</td>\n",
       "      <td>2025-03-14</td>\n",
       "      <td>surgery_arm_1</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>18187 rows Ã— 70 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         id operation_date   redcap_event_name ComplicationDate        dob  \\\n",
       "0         1            NaT      baseline_arm_1              NaT 2049-08-04   \n",
       "1         1     2011-08-26                 NaN              NaT        NaT   \n",
       "2         1     2010-02-20                 NaN              NaT        NaT   \n",
       "3         1     2009-02-25                 NaN              NaT        NaT   \n",
       "4         1     2008-02-22                 NaN              NaT        NaT   \n",
       "...     ...            ...                 ...              ...        ...   \n",
       "18182  1770            NaT  preoperative_arm_1              NaT        NaT   \n",
       "18183  1770            NaT  preoperative_arm_1              NaT        NaT   \n",
       "18184  1770            NaT  preoperative_arm_1              NaT        NaT   \n",
       "18185  1770            NaT      baseline_arm_1              NaT 1982-10-12   \n",
       "18186  1770     2025-03-14       surgery_arm_1              NaT        NaT   \n",
       "\n",
       "        qol_date  age_diagnosis  gender overall_primary_tumour  \\\n",
       "0            NaT            NaN     1.0                    NaN   \n",
       "1            NaT            NaN     NaN                    NaN   \n",
       "2            NaT            NaN     NaN                    NaN   \n",
       "3            NaT            NaN     NaN                    NaN   \n",
       "4            NaT            NaN     NaN                    NaN   \n",
       "...          ...            ...     ...                    ...   \n",
       "18182        NaT            NaN     NaN                    NaN   \n",
       "18183        NaT            NaN     NaN                    NaN   \n",
       "18184 2025-02-24            NaN     NaN                    NaN   \n",
       "18185        NaT            NaN     1.0                    NaN   \n",
       "18186        NaT            NaN     NaN                    NaN   \n",
       "\n",
       "      overall_regional_ln  ...  a_e5  a_e6  a_e7  a_c6  a_c2  a_act11  \\\n",
       "0                     NaN  ...   NaN   NaN   NaN   NaN   NaN      NaN   \n",
       "1                     NaN  ...   NaN   NaN   NaN   NaN   NaN      NaN   \n",
       "2                     NaN  ...   NaN   NaN   NaN   NaN   NaN      NaN   \n",
       "3                     NaN  ...   NaN   NaN   NaN   NaN   NaN      NaN   \n",
       "4                     NaN  ...   NaN   NaN   NaN   NaN   NaN      NaN   \n",
       "...                   ...  ...   ...   ...   ...   ...   ...      ...   \n",
       "18182                 NaN  ...   NaN   NaN   NaN   NaN   NaN      NaN   \n",
       "18183                 NaN  ...   NaN   NaN   NaN   NaN   NaN      NaN   \n",
       "18184                 NaN  ...   0.0   0.0   0.0   1.0   3.0      3.0   \n",
       "18185                 NaN  ...   NaN   NaN   NaN   NaN   NaN      NaN   \n",
       "18186                 NaN  ...   NaN   NaN   NaN   NaN   NaN      NaN   \n",
       "\n",
       "       readmission_30d  postop_comp   los DischargeDate  \n",
       "0                  NaN          0.0   NaN    2012-04-27  \n",
       "1                  NaN          NaN   NaN    2011-08-26  \n",
       "2                  NaN          NaN  16.0    2010-03-08  \n",
       "3                  NaN          NaN   8.0    2009-03-05  \n",
       "4                  NaN          NaN  13.0    2008-03-06  \n",
       "...                ...          ...   ...           ...  \n",
       "18182              NaN          NaN   NaN           NaT  \n",
       "18183              NaN          NaN   NaN           NaT  \n",
       "18184              NaN          NaN   NaN           NaT  \n",
       "18185              NaN          NaN   NaN           NaT  \n",
       "18186              NaN          NaN   NaN           NaT  \n",
       "\n",
       "[18187 rows x 70 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read the CSV file - linked file \n",
    "file_path = \"Merged_TSQIC_REDCap_ACCESS.xlsx\" \n",
    "df = pd.read_excel(file_path)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                              ge1         ge2         ge3         ge4  \\\n",
      "1_month_postop_arm_1    44.060150   43.909774   45.112782   44.360902   \n",
      "1_year_postop_arm_1      5.820106    5.820106    6.878307    5.291005   \n",
      "2_years_postop_arm_1     3.921569    3.921569    3.921569    3.921569   \n",
      "3_months_postop_arm_1    2.702703    4.729730    4.054054    3.378378   \n",
      "3_years_postop_arm_1     6.410256    8.974359    7.692308    6.410256   \n",
      "4_years_postop_arm_1     4.347826    6.521739    4.347826    4.347826   \n",
      "5_years_postop_arm_1    94.918567   94.983713   94.918567   94.918567   \n",
      "6_months_postop_arm_1    3.174603    3.703704    3.703704    4.232804   \n",
      "baseline_arm_1          47.491448   48.916762   50.969213   48.289624   \n",
      "preoperative_arm_1      74.343832   74.409449   74.540682   74.278215   \n",
      "surgery_arm_1          100.000000  100.000000  100.000000  100.000000   \n",
      "\n",
      "                              ge5         ge6  \n",
      "1_month_postop_arm_1    44.210526   44.060150  \n",
      "1_year_postop_arm_1      5.820106    5.291005  \n",
      "2_years_postop_arm_1     4.901961    3.921569  \n",
      "3_months_postop_arm_1    4.054054    2.702703  \n",
      "3_years_postop_arm_1     7.692308    6.410256  \n",
      "4_years_postop_arm_1     6.521739    4.347826  \n",
      "5_years_postop_arm_1    94.918567   94.918567  \n",
      "6_months_postop_arm_1    4.761905    3.703704  \n",
      "baseline_arm_1          48.346636   48.232611  \n",
      "preoperative_arm_1      74.343832   74.409449  \n",
      "surgery_arm_1          100.000000  100.000000  \n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x800 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# First, let's create a function to calculate missing value percentages\n",
    "def calculate_missing_percentages(df, columns, group_col):\n",
    "    # Group by both id and the specified column to handle multiple rows per (id, group_col)\n",
    "    grouped = df.groupby(['id', group_col])\n",
    "    \n",
    "    # Aggregate by taking the first non-null value for each (id, group_col) combination\n",
    "    aggregated_df = grouped[columns].apply(lambda x: x.bfill().ffill().iloc[0] if len(x) > 0 else pd.Series([None] * len(columns), index=columns))\n",
    "    aggregated_df = aggregated_df.reset_index()\n",
    "    \n",
    "    # Now group by the specified column only\n",
    "    grouped_by_event = aggregated_df.groupby(group_col)\n",
    "    \n",
    "    # Initialize a dictionary to store results\n",
    "    missing_percentages = {}\n",
    "    \n",
    "    # Calculate missing percentages for each group\n",
    "    for name, group in grouped_by_event:\n",
    "        # Calculate percentage of missing values for each column\n",
    "        missing_pct = group[columns].isna().mean() * 100\n",
    "        missing_percentages[name] = missing_pct\n",
    "    \n",
    "    # Convert to DataFrame for better visualization\n",
    "    result_df = pd.DataFrame(missing_percentages)\n",
    "    return result_df.T # Transpose for better readability\n",
    "\n",
    "# Columns of interest\n",
    "ge_columns = ['ge1', 'ge2', 'ge3', 'ge4', 'ge5', 'ge6']\n",
    "\n",
    "# Calculate missing percentages for each redcap_event_name\n",
    "missing_pct_by_event = calculate_missing_percentages(df, ge_columns, 'redcap_event_name')\n",
    "\n",
    "# Display the results\n",
    "print(missing_pct_by_event)\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(missing_pct_by_event, annot=True, cmap='YlGnBu', fmt='.1f')\n",
    "plt.title('Missing Values Percentage by Event and Column')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>operation_date</th>\n",
       "      <th>redcap_event_name</th>\n",
       "      <th>ComplicationDate</th>\n",
       "      <th>dob</th>\n",
       "      <th>qol_date</th>\n",
       "      <th>age_diagnosis</th>\n",
       "      <th>gender</th>\n",
       "      <th>overall_primary_tumour</th>\n",
       "      <th>overall_regional_ln</th>\n",
       "      <th>...</th>\n",
       "      <th>a_e5</th>\n",
       "      <th>a_e6</th>\n",
       "      <th>a_e7</th>\n",
       "      <th>a_c6</th>\n",
       "      <th>a_c2</th>\n",
       "      <th>a_act11</th>\n",
       "      <th>readmission_30d</th>\n",
       "      <th>postop_comp</th>\n",
       "      <th>los</th>\n",
       "      <th>DischargeDate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaT</td>\n",
       "      <td>baseline_arm_1</td>\n",
       "      <td>NaT</td>\n",
       "      <td>2049-08-04</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2012-04-27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1</td>\n",
       "      <td>NaT</td>\n",
       "      <td>baseline_arm_1</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2</td>\n",
       "      <td>NaT</td>\n",
       "      <td>baseline_arm_1</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>2</td>\n",
       "      <td>NaT</td>\n",
       "      <td>baseline_arm_1</td>\n",
       "      <td>NaT</td>\n",
       "      <td>2041-02-25</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>2</td>\n",
       "      <td>NaT</td>\n",
       "      <td>baseline_arm_1</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>2007-01-12</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18178</th>\n",
       "      <td>1769</td>\n",
       "      <td>NaT</td>\n",
       "      <td>baseline_arm_1</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18179</th>\n",
       "      <td>1769</td>\n",
       "      <td>NaT</td>\n",
       "      <td>baseline_arm_1</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>2025-03-10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18180</th>\n",
       "      <td>1769</td>\n",
       "      <td>NaT</td>\n",
       "      <td>baseline_arm_1</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18181</th>\n",
       "      <td>1770</td>\n",
       "      <td>NaT</td>\n",
       "      <td>baseline_arm_1</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>42.054252</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18185</th>\n",
       "      <td>1770</td>\n",
       "      <td>NaT</td>\n",
       "      <td>baseline_arm_1</td>\n",
       "      <td>NaT</td>\n",
       "      <td>1982-10-12</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5230 rows Ã— 70 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         id operation_date redcap_event_name ComplicationDate        dob  \\\n",
       "0         1            NaT    baseline_arm_1              NaT 2049-08-04   \n",
       "14        1            NaT    baseline_arm_1              NaT        NaT   \n",
       "18        2            NaT    baseline_arm_1              NaT        NaT   \n",
       "22        2            NaT    baseline_arm_1              NaT 2041-02-25   \n",
       "25        2            NaT    baseline_arm_1              NaT        NaT   \n",
       "...     ...            ...               ...              ...        ...   \n",
       "18178  1769            NaT    baseline_arm_1              NaT        NaT   \n",
       "18179  1769            NaT    baseline_arm_1              NaT        NaT   \n",
       "18180  1769            NaT    baseline_arm_1              NaT        NaT   \n",
       "18181  1770            NaT    baseline_arm_1              NaT        NaT   \n",
       "18185  1770            NaT    baseline_arm_1              NaT 1982-10-12   \n",
       "\n",
       "        qol_date  age_diagnosis  gender overall_primary_tumour  \\\n",
       "0            NaT            NaN     1.0                    NaN   \n",
       "14           NaT            NaN     NaN                      2   \n",
       "18           NaT            NaN     NaN                      3   \n",
       "22           NaT            NaN     1.0                    NaN   \n",
       "25    2007-01-12            NaN     NaN                    NaN   \n",
       "...          ...            ...     ...                    ...   \n",
       "18178        NaT            NaN     NaN                    NaN   \n",
       "18179 2025-03-10            NaN     NaN                    NaN   \n",
       "18180        NaT            NaN     NaN                      3   \n",
       "18181        NaT      42.054252     NaN                      3   \n",
       "18185        NaT            NaN     1.0                    NaN   \n",
       "\n",
       "      overall_regional_ln  ...  a_e5  a_e6  a_e7  a_c6  a_c2  a_act11  \\\n",
       "0                     NaN  ...   NaN   NaN   NaN   NaN   NaN      NaN   \n",
       "14                      0  ...   NaN   NaN   NaN   NaN   NaN      NaN   \n",
       "18                      1  ...   NaN   NaN   NaN   NaN   NaN      NaN   \n",
       "22                    NaN  ...   NaN   NaN   NaN   NaN   NaN      NaN   \n",
       "25                    NaN  ...   0.0   4.0   0.0   4.0   0.0      1.0   \n",
       "...                   ...  ...   ...   ...   ...   ...   ...      ...   \n",
       "18178                 NaN  ...   NaN   NaN   NaN   NaN   NaN      NaN   \n",
       "18179                 NaN  ...   0.0   0.0   0.0   0.0   1.0      1.0   \n",
       "18180                   1  ...   NaN   NaN   NaN   NaN   NaN      NaN   \n",
       "18181                   0  ...   NaN   NaN   NaN   NaN   NaN      NaN   \n",
       "18185                 NaN  ...   NaN   NaN   NaN   NaN   NaN      NaN   \n",
       "\n",
       "       readmission_30d  postop_comp  los DischargeDate  \n",
       "0                  NaN          0.0  NaN    2012-04-27  \n",
       "14                 NaN          NaN  NaN           NaT  \n",
       "18                 NaN          NaN  NaN           NaT  \n",
       "22                 NaN          NaN  NaN           NaT  \n",
       "25                 NaN          NaN  NaN           NaT  \n",
       "...                ...          ...  ...           ...  \n",
       "18178              NaN          NaN  NaN           NaT  \n",
       "18179              NaN          NaN  NaN           NaT  \n",
       "18180              NaN          NaN  NaN           NaT  \n",
       "18181              NaN          NaN  NaN           NaT  \n",
       "18185              NaN          NaN  NaN           NaT  \n",
       "\n",
       "[5230 rows x 70 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # Leave only baseline\n",
    "df = df[df['redcap_event_name'] == 'baseline_arm_1']\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'operation_date', 'redcap_event_name', 'ComplicationDate', 'dob',\n",
       "       'qol_date', 'age_diagnosis', 'gender', 'overall_primary_tumour',\n",
       "       'overall_regional_ln', 'overall_distant_metastasis', 'neotx___notx',\n",
       "       'neotx___chemo', 'neotx___rads', 'neotx___chemorads', 'neotx___immuno',\n",
       "       'neotx___other', 'procedure123456', 'expectation_treatment',\n",
       "       'path_esoph_primtumour', 'path_esoph_regionalln',\n",
       "       'path_esoph_distantmetast', 'gp1', 'gp2', 'gp3', 'gp4', 'gp5', 'gp6',\n",
       "       'gp7', 'gs1', 'gs2', 'gs3', 'gs4', 'gs5', 'gs6', 'gs7', 'ge1', 'ge2',\n",
       "       'ge3', 'ge4', 'ge5', 'ge6', 'gf1', 'gf2', 'gf3', 'gf4', 'gf5', 'gf6',\n",
       "       'gf7', 'a_hn1', 'a_hn2', 'a_hn3', 'a_hn4', 'a_hn5', 'a_hn7', 'a_hn10',\n",
       "       'a_e1', 'a_e2', 'a_e3', 'a_e4', 'a_e5', 'a_e6', 'a_e7', 'a_c6', 'a_c2',\n",
       "       'a_act11', 'readmission_30d', 'postop_comp', 'los', 'DischargeDate'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'redcap_event_name', 'qol_date', 'age_diagnosis', 'gender',\n",
       "       'overall_primary_tumour', 'overall_regional_ln',\n",
       "       'overall_distant_metastasis', 'neotx___notx', 'neotx___chemo',\n",
       "       'neotx___rads', 'neotx___chemorads', 'neotx___immuno', 'neotx___other',\n",
       "       'procedure123456', 'expectation_treatment', 'path_esoph_primtumour',\n",
       "       'path_esoph_regionalln', 'path_esoph_distantmetast', 'gp1', 'gp2',\n",
       "       'gp3', 'gp4', 'gp5', 'gp6', 'gp7', 'gs1', 'gs2', 'gs3', 'gs4', 'gs5',\n",
       "       'gs6', 'gs7', 'ge1', 'ge2', 'ge3', 'ge4', 'ge5', 'ge6', 'gf1', 'gf2',\n",
       "       'gf3', 'gf4', 'gf5', 'gf6', 'gf7', 'a_hn1', 'a_hn2', 'a_hn3', 'a_hn4',\n",
       "       'a_hn5', 'a_hn7', 'a_hn10', 'a_e1', 'a_e2', 'a_e3', 'a_e4', 'a_e5',\n",
       "       'a_e6', 'a_e7', 'a_c6', 'a_c2', 'a_act11', 'readmission_30d',\n",
       "       'postop_comp', 'los'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#drop dates\n",
    "df = df.drop(columns=[\"ComplicationDate\", \"dob\", \"operation_date\", \"DischargeDate\"]) #qol_date\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>redcap_event_name</th>\n",
       "      <th>qol_date</th>\n",
       "      <th>age_diagnosis</th>\n",
       "      <th>gender</th>\n",
       "      <th>overall_primary_tumour</th>\n",
       "      <th>overall_regional_ln</th>\n",
       "      <th>overall_distant_metastasis</th>\n",
       "      <th>neotx___notx</th>\n",
       "      <th>neotx___chemo</th>\n",
       "      <th>...</th>\n",
       "      <th>a_e4</th>\n",
       "      <th>a_e5</th>\n",
       "      <th>a_e6</th>\n",
       "      <th>a_e7</th>\n",
       "      <th>a_c6</th>\n",
       "      <th>a_c2</th>\n",
       "      <th>a_act11</th>\n",
       "      <th>readmission_30d</th>\n",
       "      <th>postop_comp</th>\n",
       "      <th>los</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2007-01-12</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18178</th>\n",
       "      <td>1769</td>\n",
       "      <td>0</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18179</th>\n",
       "      <td>1769</td>\n",
       "      <td>0</td>\n",
       "      <td>2025-03-10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18180</th>\n",
       "      <td>1769</td>\n",
       "      <td>0</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18181</th>\n",
       "      <td>1770</td>\n",
       "      <td>0</td>\n",
       "      <td>NaT</td>\n",
       "      <td>42.054252</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18185</th>\n",
       "      <td>1770</td>\n",
       "      <td>0</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5230 rows Ã— 66 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         id redcap_event_name   qol_date  age_diagnosis gender  \\\n",
       "0         1                 0        NaT            NaN    1.0   \n",
       "14        1                 0        NaT            NaN    NaN   \n",
       "18        2                 0        NaT            NaN    NaN   \n",
       "22        2                 0        NaT            NaN    1.0   \n",
       "25        2                 0 2007-01-12            NaN    NaN   \n",
       "...     ...               ...        ...            ...    ...   \n",
       "18178  1769                 0        NaT            NaN    NaN   \n",
       "18179  1769                 0 2025-03-10            NaN    NaN   \n",
       "18180  1769                 0        NaT            NaN    NaN   \n",
       "18181  1770                 0        NaT      42.054252    NaN   \n",
       "18185  1770                 0        NaT            NaN    1.0   \n",
       "\n",
       "      overall_primary_tumour overall_regional_ln  overall_distant_metastasis  \\\n",
       "0                        NaN                 NaN                         NaN   \n",
       "14                         2                   0                         NaN   \n",
       "18                         3                   1                         0.0   \n",
       "22                       NaN                 NaN                         NaN   \n",
       "25                       NaN                 NaN                         NaN   \n",
       "...                      ...                 ...                         ...   \n",
       "18178                    NaN                 NaN                         NaN   \n",
       "18179                    NaN                 NaN                         NaN   \n",
       "18180                      3                   1                         0.0   \n",
       "18181                      3                   0                         0.0   \n",
       "18185                    NaN                 NaN                         NaN   \n",
       "\n",
       "      neotx___notx neotx___chemo  ... a_e4 a_e5 a_e6 a_e7 a_c6 a_c2 a_act11  \\\n",
       "0              NaN           NaN  ...  NaN  NaN  NaN  NaN  NaN  NaN     NaN   \n",
       "14             NaN           NaN  ...  NaN  NaN  NaN  NaN  NaN  NaN     NaN   \n",
       "18             NaN           NaN  ...  NaN  NaN  NaN  NaN  NaN  NaN     NaN   \n",
       "22             NaN           NaN  ...  NaN  NaN  NaN  NaN  NaN  NaN     NaN   \n",
       "25             NaN           NaN  ...  3.0  0.0  4.0  0.0  4.0  0.0     1.0   \n",
       "...            ...           ...  ...  ...  ...  ...  ...  ...  ...     ...   \n",
       "18178          NaN           NaN  ...  NaN  NaN  NaN  NaN  NaN  NaN     NaN   \n",
       "18179          NaN           NaN  ...  2.0  0.0  0.0  0.0  0.0  1.0     1.0   \n",
       "18180          NaN           NaN  ...  NaN  NaN  NaN  NaN  NaN  NaN     NaN   \n",
       "18181          NaN           NaN  ...  NaN  NaN  NaN  NaN  NaN  NaN     NaN   \n",
       "18185          NaN           NaN  ...  NaN  NaN  NaN  NaN  NaN  NaN     NaN   \n",
       "\n",
       "      readmission_30d  postop_comp  los  \n",
       "0                 NaN          0.0  NaN  \n",
       "14                NaN          NaN  NaN  \n",
       "18                NaN          NaN  NaN  \n",
       "22                NaN          NaN  NaN  \n",
       "25                NaN          NaN  NaN  \n",
       "...               ...          ...  ...  \n",
       "18178             NaN          NaN  NaN  \n",
       "18179             NaN          NaN  NaN  \n",
       "18180             NaN          NaN  NaN  \n",
       "18181             NaN          NaN  NaN  \n",
       "18185             NaN          NaN  NaN  \n",
       "\n",
       "[5230 rows x 66 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 1: Preprocess categorical variables\n",
    "# Low-cardinality variables: Convert to category dtype\n",
    "low_cardinality_cols = [\n",
    "    'postop_comp', 'readmission_30d', 'gender', 'neotx___notx', 'neotx___chemo',\n",
    "    'neotx___rads', 'neotx___chemorads', 'neotx___immuno', 'neotx___other',\n",
    "    'expectation_treatment'\n",
    "]\n",
    "for col in low_cardinality_cols:\n",
    "    df[col] = df[col].astype('category')\n",
    "\n",
    "# High-cardinality variables: Label encode and convert to category\n",
    "le_redcap = LabelEncoder()\n",
    "df['redcap_event_name'] = df['redcap_event_name'].astype(str)  # Convert to string to handle NaN\n",
    "df['redcap_event_name'] = le_redcap.fit_transform(df['redcap_event_name'])\n",
    "df['redcap_event_name'] = df['redcap_event_name'].astype('category')\n",
    "\n",
    "# procedure123456 is already numerical but should be treated as categorical\n",
    "df['procedure123456'] = df['procedure123456'].astype('category')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>redcap_event_name</th>\n",
       "      <th>qol_date</th>\n",
       "      <th>age_diagnosis</th>\n",
       "      <th>gender</th>\n",
       "      <th>overall_primary_tumour</th>\n",
       "      <th>overall_regional_ln</th>\n",
       "      <th>overall_distant_metastasis</th>\n",
       "      <th>neotx___notx</th>\n",
       "      <th>neotx___chemo</th>\n",
       "      <th>...</th>\n",
       "      <th>a_e4</th>\n",
       "      <th>a_e5</th>\n",
       "      <th>a_e6</th>\n",
       "      <th>a_e7</th>\n",
       "      <th>a_c6</th>\n",
       "      <th>a_c2</th>\n",
       "      <th>a_act11</th>\n",
       "      <th>readmission_30d</th>\n",
       "      <th>postop_comp</th>\n",
       "      <th>los</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>nan</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2007-01-12</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18178</th>\n",
       "      <td>1769</td>\n",
       "      <td>0</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18179</th>\n",
       "      <td>1769</td>\n",
       "      <td>0</td>\n",
       "      <td>2025-03-10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18180</th>\n",
       "      <td>1769</td>\n",
       "      <td>0</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18181</th>\n",
       "      <td>1770</td>\n",
       "      <td>0</td>\n",
       "      <td>NaT</td>\n",
       "      <td>42.054252</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18185</th>\n",
       "      <td>1770</td>\n",
       "      <td>0</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5230 rows Ã— 66 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         id redcap_event_name   qol_date  age_diagnosis gender  \\\n",
       "0         1                 0        NaT            NaN    1.0   \n",
       "14        1                 0        NaT            NaN    NaN   \n",
       "18        2                 0        NaT            NaN    NaN   \n",
       "22        2                 0        NaT            NaN    1.0   \n",
       "25        2                 0 2007-01-12            NaN    NaN   \n",
       "...     ...               ...        ...            ...    ...   \n",
       "18178  1769                 0        NaT            NaN    NaN   \n",
       "18179  1769                 0 2025-03-10            NaN    NaN   \n",
       "18180  1769                 0        NaT            NaN    NaN   \n",
       "18181  1770                 0        NaT      42.054252    NaN   \n",
       "18185  1770                 0        NaT            NaN    1.0   \n",
       "\n",
       "      overall_primary_tumour overall_regional_ln overall_distant_metastasis  \\\n",
       "0                        nan                 nan                        nan   \n",
       "14                         2                   0                        nan   \n",
       "18                         3                   1                        0.0   \n",
       "22                       nan                 nan                        nan   \n",
       "25                       nan                 nan                        nan   \n",
       "...                      ...                 ...                        ...   \n",
       "18178                    nan                 nan                        nan   \n",
       "18179                    nan                 nan                        nan   \n",
       "18180                      3                   1                        0.0   \n",
       "18181                      3                   0                        0.0   \n",
       "18185                    nan                 nan                        nan   \n",
       "\n",
       "      neotx___notx neotx___chemo  ... a_e4 a_e5 a_e6 a_e7 a_c6 a_c2 a_act11  \\\n",
       "0              NaN           NaN  ...  NaN  NaN  NaN  NaN  NaN  NaN     NaN   \n",
       "14             NaN           NaN  ...  NaN  NaN  NaN  NaN  NaN  NaN     NaN   \n",
       "18             NaN           NaN  ...  NaN  NaN  NaN  NaN  NaN  NaN     NaN   \n",
       "22             NaN           NaN  ...  NaN  NaN  NaN  NaN  NaN  NaN     NaN   \n",
       "25             NaN           NaN  ...  3.0  0.0  4.0  0.0  4.0  0.0     1.0   \n",
       "...            ...           ...  ...  ...  ...  ...  ...  ...  ...     ...   \n",
       "18178          NaN           NaN  ...  NaN  NaN  NaN  NaN  NaN  NaN     NaN   \n",
       "18179          NaN           NaN  ...  2.0  0.0  0.0  0.0  0.0  1.0     1.0   \n",
       "18180          NaN           NaN  ...  NaN  NaN  NaN  NaN  NaN  NaN     NaN   \n",
       "18181          NaN           NaN  ...  NaN  NaN  NaN  NaN  NaN  NaN     NaN   \n",
       "18185          NaN           NaN  ...  NaN  NaN  NaN  NaN  NaN  NaN     NaN   \n",
       "\n",
       "      readmission_30d postop_comp  los  \n",
       "0                 NaN         0.0  NaN  \n",
       "14                NaN         NaN  NaN  \n",
       "18                NaN         NaN  NaN  \n",
       "22                NaN         NaN  NaN  \n",
       "25                NaN         NaN  NaN  \n",
       "...               ...         ...  ...  \n",
       "18178             NaN         NaN  NaN  \n",
       "18179             NaN         NaN  NaN  \n",
       "18180             NaN         NaN  NaN  \n",
       "18181             NaN         NaN  NaN  \n",
       "18185             NaN         NaN  NaN  \n",
       "\n",
       "[5230 rows x 66 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 2: Preprocess numerical and ordinal columns\n",
    "# True numerical columns: Ensure float/int dtype\n",
    "numerical_cols = ['los', 'age_diagnosis']\n",
    "for col in numerical_cols:\n",
    "    df[col] = df[col].astype(float)\n",
    "\n",
    "#Step 3\n",
    "# Ordinal columns: Treat as numerical (already float)\n",
    "ordinal_cols = (\n",
    "    [f\"gp{i}\" for i in range(1, 8)] + [f\"gs{i}\" for i in range(1, 8)] +\n",
    "    [f\"ge{i}\" for i in range(1, 7)] + [f\"gf{i}\" for i in range(1, 8)] +\n",
    "    [f\"a_hn{i}\" for i in range(1, 6)] + [\"a_hn7\", \"a_hn10\"] +\n",
    "    [f\"a_e{i}\" for i in range(1, 8)] + [\"a_c6\", \"a_c2\", \"a_act11\"]\n",
    ")\n",
    "# Subset for this example\n",
    "ordinal_cols = [col for col in ordinal_cols if col in df.columns]\n",
    "for col in ordinal_cols:\n",
    "    df[col] = df[col].astype(float)\n",
    "\n",
    "#Step 4\n",
    "# Categorical-like columns: Treat as categorical\n",
    "categorical_like_cols = [\n",
    "    'overall_primary_tumour', 'overall_regional_ln', 'overall_distant_metastasis', \n",
    "    'path_esoph_primtumour', 'path_esoph_regionalln', 'path_esoph_distantmetast'\n",
    "]\n",
    "for col in categorical_like_cols:\n",
    "    df[col] = df[col].astype(str)  # Convert to string to handle mixed types\n",
    "    df[col] = df[col].astype('category')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column 'redcap_event_name' has only one unique value: 0\n",
      "Column 'path_esoph_primtumour' has only one unique value: nan\n",
      "Column 'path_esoph_regionalln' has only one unique value: nan\n",
      "Column 'path_esoph_distantmetast' has only one unique value: nan\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def analyze_categorical_columns(dataframe):\n",
    "    \"\"\"\n",
    "    Analyzes all categorical columns in a dataframe to check if they contain only one unique value.\n",
    "    \n",
    "    Args:\n",
    "        dataframe: pandas DataFrame to analyze\n",
    "        \n",
    "    Returns:\n",
    "        dict: Dictionary with column names as keys and tuples (has_single_category, unique_value) as values\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    \n",
    "    # Identify categorical columns\n",
    "    # This includes object dtype, category dtype, and any other non-numeric columns\n",
    "    categorical_columns = dataframe.select_dtypes(\n",
    "        include=['object', 'category', 'bool']\n",
    "    ).columns.tolist()\n",
    "    \n",
    "    # For each categorical column, check if it has only one unique value\n",
    "    for column in categorical_columns:\n",
    "        # Skip columns with all null values\n",
    "        if dataframe[column].isna().all():\n",
    "            results[column] = (False, None, \"All values are null\")\n",
    "            continue\n",
    "            \n",
    "        # Get unique non-null values\n",
    "        unique_values = dataframe[column].dropna().unique()\n",
    "        \n",
    "        # Check if there's only one unique category\n",
    "        has_single_category = len(unique_values) == 1\n",
    "        \n",
    "        if has_single_category:\n",
    "            results[column] = (True, unique_values[0], None)\n",
    "        else:\n",
    "            results[column] = (False, None, f\"Found {len(unique_values)} unique values\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Example usage:\n",
    "column_analysis = analyze_categorical_columns(df)\n",
    "\n",
    "# Print results\n",
    "for column, (has_single_value, unique_value, message) in column_analysis.items():\n",
    "    if has_single_value:\n",
    "        print(f\"Column '{column}' has only one unique value: {unique_value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropped 4 columns with only one unique value:\n",
      "  - redcap_event_name\n",
      "  - path_esoph_primtumour\n",
      "  - path_esoph_regionalln\n",
      "  - path_esoph_distantmetast\n"
     ]
    }
   ],
   "source": [
    "def drop_single_value_categorical_columns(dataframe, inplace=False):\n",
    "    \"\"\"\n",
    "    Identifies and drops all categorical columns that contain only one unique value.\n",
    "    \n",
    "    Args:\n",
    "        dataframe: pandas DataFrame to process\n",
    "        inplace: Whether to modify the original dataframe (True) or return a copy (False)\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame: Modified dataframe with single-value categorical columns removed\n",
    "        list: List of column names that were dropped\n",
    "    \"\"\"\n",
    "    # Make a copy if not inplace\n",
    "    df = dataframe if inplace else dataframe.copy()\n",
    "    \n",
    "    # Identify categorical columns\n",
    "    categorical_columns = df.select_dtypes(\n",
    "        include=['object', 'category', 'bool']\n",
    "    ).columns.tolist()\n",
    "    \n",
    "    # Track columns to drop\n",
    "    columns_to_drop = []\n",
    "    \n",
    "    # Check each categorical column\n",
    "    for column in categorical_columns:\n",
    "        # Skip columns with all null values\n",
    "        if df[column].isna().all():\n",
    "            continue\n",
    "            \n",
    "        # Get unique non-null values\n",
    "        unique_values = df[column].dropna().unique()\n",
    "        \n",
    "        # If only one unique value, add to drop list\n",
    "        if len(unique_values) == 1:\n",
    "            columns_to_drop.append(column)\n",
    "    \n",
    "    # Drop the identified columns\n",
    "    if columns_to_drop:\n",
    "        df.drop(columns=columns_to_drop, inplace=True)\n",
    "        \n",
    "    return df, columns_to_drop\n",
    "\n",
    "# Example usage:\n",
    "# Drop single-value categorical columns\n",
    "df_cleaned, dropped_columns = drop_single_value_categorical_columns(df, inplace=False)\n",
    "\n",
    "# Report results\n",
    "if dropped_columns:\n",
    "    print(f\"Dropped {len(dropped_columns)} columns with only one unique value:\")\n",
    "    for col in dropped_columns:\n",
    "        print(f\"  - {col}\")\n",
    "else:\n",
    "    print(\"No single-value categorical columns found.\")\n",
    "\n",
    "df = df_cleaned.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification process + metrics "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add these imports at the top of your file\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, roc_auc_score\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "\n",
    "def process_for_classification(values, min_val=0, max_val=4):\n",
    "    \"\"\"\n",
    "    Process imputed values for classification evaluation\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    values : array-like\n",
    "        Raw imputed values\n",
    "    min_val : int\n",
    "        Minimum allowed value (default: 0)\n",
    "    max_val : int  \n",
    "        Maximum allowed value (default: 4)\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    numpy.ndarray\n",
    "        Processed values as integers in range [min_val, max_val]\n",
    "    \"\"\"\n",
    "    # Round to nearest integer\n",
    "    rounded = np.round(values)\n",
    "    # Clip to valid range\n",
    "    clipped = np.clip(rounded, min_val, max_val)\n",
    "    return clipped.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_classification_metrics(y_true, y_pred, classes=None):\n",
    "    \"\"\"\n",
    "    Calculate classification metrics for multi-class problem\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    y_true : array-like\n",
    "        True class labels\n",
    "    y_pred : array-like\n",
    "        Predicted class labels\n",
    "    classes : array-like, optional\n",
    "        Class labels (default: [0, 1, 2, 3, 4])\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    dict\n",
    "        Dictionary containing classification metrics\n",
    "    \"\"\"\n",
    "    if classes is None:\n",
    "        classes = np.array([0, 1, 2, 3, 4])\n",
    "    \n",
    "    try:\n",
    "        # Convert to numpy arrays\n",
    "        y_true = np.array(y_true)\n",
    "        y_pred = np.array(y_pred)\n",
    "        \n",
    "        # Basic accuracy\n",
    "        accuracy = accuracy_score(y_true, y_pred)\n",
    "        \n",
    "        # Macro-averaged metrics (average across classes)\n",
    "        precision_macro = precision_score(y_true, y_pred, average='macro', zero_division=0)\n",
    "        recall_macro = recall_score(y_true, y_pred, average='macro', zero_division=0)\n",
    "        \n",
    "        # Weighted-averaged metrics (weighted by class frequency)\n",
    "        precision_weighted = precision_score(y_true, y_pred, average='weighted', zero_division=0)\n",
    "        recall_weighted = recall_score(y_true, y_pred, average='weighted', zero_division=0)\n",
    "        \n",
    "        # For multi-class AUC, we need to binarize the labels\n",
    "        try:\n",
    "            # Only calculate AUC if we have more than one class present\n",
    "            if len(np.unique(y_true)) > 1:\n",
    "                y_true_bin = label_binarize(y_true, classes=classes)\n",
    "                y_pred_bin = label_binarize(y_pred, classes=classes)\n",
    "                \n",
    "                # If only 2 classes present, reshape\n",
    "                if y_true_bin.shape[1] == 1:\n",
    "                    auc_score = roc_auc_score(y_true_bin, y_pred_bin)\n",
    "                else:\n",
    "                    # Multi-class AUC (macro average)\n",
    "                    auc_score = roc_auc_score(y_true_bin, y_pred_bin, average='macro', multi_class='ovr')\n",
    "            else:\n",
    "                auc_score = np.nan\n",
    "        except:\n",
    "            auc_score = np.nan\n",
    "        \n",
    "        # Per-class metrics\n",
    "        cm = confusion_matrix(y_true, y_pred, labels=classes)\n",
    "        \n",
    "        # Calculate sensitivity (recall) and specificity for each class\n",
    "        per_class_metrics = {}\n",
    "        for i, class_label in enumerate(classes):\n",
    "            if i < cm.shape[0] and i < cm.shape[1]:\n",
    "                tp = cm[i, i] if i < cm.shape[0] and i < cm.shape[1] else 0\n",
    "                fp = cm[:, i].sum() - tp if i < cm.shape[1] else 0\n",
    "                fn = cm[i, :].sum() - tp if i < cm.shape[0] else 0\n",
    "                tn = cm.sum() - tp - fp - fn\n",
    "                \n",
    "                # Sensitivity (recall/true positive rate)\n",
    "                sensitivity = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "                \n",
    "                # Specificity (true negative rate)\n",
    "                specificity = tn / (tn + fp) if (tn + fp) > 0 else 0\n",
    "                \n",
    "                # PPV (precision)\n",
    "                ppv = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "                \n",
    "                # NPV\n",
    "                npv = tn / (tn + fn) if (tn + fn) > 0 else 0\n",
    "                \n",
    "                per_class_metrics[f'class_{class_label}'] = {\n",
    "                    'sensitivity': sensitivity,\n",
    "                    'specificity': specificity,\n",
    "                    'ppv': ppv,\n",
    "                    'npv': npv\n",
    "                }\n",
    "        \n",
    "        # Average across classes for summary\n",
    "        avg_sensitivity = np.mean([metrics['sensitivity'] for metrics in per_class_metrics.values()])\n",
    "        avg_specificity = np.mean([metrics['specificity'] for metrics in per_class_metrics.values()])\n",
    "        avg_ppv = np.mean([metrics['ppv'] for metrics in per_class_metrics.values()])\n",
    "        avg_npv = np.mean([metrics['npv'] for metrics in per_class_metrics.values()])\n",
    "        \n",
    "        return {\n",
    "            'accuracy': accuracy,\n",
    "            'auc_multiclass': auc_score,\n",
    "            'precision_macro': precision_macro,\n",
    "            'recall_macro': recall_macro,\n",
    "            'precision_weighted': precision_weighted,\n",
    "            'recall_weighted': recall_weighted,\n",
    "            'avg_sensitivity': avg_sensitivity,\n",
    "            'avg_specificity': avg_specificity,\n",
    "            'avg_ppv': avg_ppv,\n",
    "            'avg_npv': avg_npv,\n",
    "            'per_class_metrics': per_class_metrics,\n",
    "            'confusion_matrix': cm\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error calculating classification metrics: {e}\")\n",
    "        return {\n",
    "            'accuracy': np.nan,\n",
    "            'auc_multiclass': np.nan,\n",
    "            'precision_macro': np.nan,\n",
    "            'recall_macro': np.nan,\n",
    "            'precision_weighted': np.nan,\n",
    "            'recall_weighted': np.nan,\n",
    "            'avg_sensitivity': np.nan,\n",
    "            'avg_specificity': np.nan,\n",
    "            'avg_ppv': np.nan,\n",
    "            'avg_npv': np.nan,\n",
    "            'per_class_metrics': {},\n",
    "            'confusion_matrix': None\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model - MICE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_mice_imputation(df, columns_to_impute, validation_df=None, validation_masks=None, original_values=None):\n",
    "    \"\"\"\n",
    "    Apply MICE imputation using miceforest package\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pandas.DataFrame\n",
    "        Data with missing values\n",
    "    columns_to_impute : list\n",
    "        List of column names to impute\n",
    "    validation_df : pandas.DataFrame, optional\n",
    "        Validation dataset with artificially missing values\n",
    "    validation_masks : dict, optional\n",
    "        Dictionary of masks for validation data\n",
    "    original_values : dict, optional\n",
    "        Dictionary of original values for validation\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    imputed_df : pandas.DataFrame\n",
    "        Data with imputed values\n",
    "    validation_results : dict, optional\n",
    "        Validation results if validation data provided\n",
    "    \"\"\"\n",
    "\n",
    "    # Set threads for LightGBM\n",
    "    os.environ['OMP_NUM_THREADS'] = '10'\n",
    "    \n",
    "    # Initialize the imputation kernel\n",
    "    kernel = mf.ImputationKernel(\n",
    "        df,\n",
    "        datasets=1,\n",
    "        variable_schema={\n",
    "            col: [c for c in df.columns if c != col] for col in columns_to_impute\n",
    "        },\n",
    "        random_state=42  # Using fixed seed for reproducibility, can be parameterized\n",
    "    )\n",
    "    \n",
    "    # Run imputation\n",
    "    for _ in tqdm(range(5), desc=\"MICE Imputation\"):\n",
    "        kernel.mice(\n",
    "            iterations=1,\n",
    "            verbose=False,\n",
    "            num_boost_round=80,\n",
    "            max_depth=10,\n",
    "            num_threads=10\n",
    "        )\n",
    "    \n",
    "    # Get imputed data\n",
    "    imputed_df = kernel.complete_data(0)\n",
    "    \n",
    "    # Check if there's a label encoder for redcap_event_name that needs inverse transformation\n",
    "    if 'redcap_event_name' in imputed_df.columns:\n",
    "        try:\n",
    "            # This is optional - only execute if le_redcap exists in the global scope\n",
    "            if 'le_redcap' in globals():\n",
    "                # Check if we're dealing with numeric values (could be int or float)\n",
    "                if pd.api.types.is_numeric_dtype(imputed_df['redcap_event_name']) or \\\n",
    "                   (hasattr(imputed_df['redcap_event_name'], 'cat') and pd.api.types.is_numeric_dtype(imputed_df['redcap_event_name'].cat.categories)):\n",
    "                    imputed_df['redcap_event_name'] = globals()['le_redcap'].inverse_transform(imputed_df['redcap_event_name'].astype(int))\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Could not inverse transform redcap_event_name: {e}\")\n",
    "    \n",
    "    # Validate if validation data provided\n",
    "    validation_results = None\n",
    "    if validation_df is not None and validation_masks is not None and original_values is not None:\n",
    "        validation_results = {}\n",
    "        \n",
    "        # Compare imputed values to real values\n",
    "        for col in columns_to_impute:\n",
    "            # Get indices where values were artificially set to NaN\n",
    "            mask = validation_masks[col] & validation_df[col].isna()\n",
    "            \n",
    "            if mask.sum() == 0:\n",
    "                validation_results[col] = {\n",
    "                    'error': \"No artificially missing values\"\n",
    "                }\n",
    "                continue\n",
    "                \n",
    "            real_vals = original_values[col][mask]\n",
    "            imputed_vals = imputed_df[col][mask]\n",
    "            \n",
    "            # Calculate continuous metrics (MAE and RMSE) - NO ROUNDING\n",
    "            mae = mean_absolute_error(real_vals, imputed_vals)\n",
    "            rmse = np.sqrt(mean_squared_error(real_vals, imputed_vals))\n",
    "            \n",
    "            # Calculate classification metrics - WITH ROUNDING\n",
    "            real_vals_class = process_for_classification(real_vals)\n",
    "            imputed_vals_class = process_for_classification(imputed_vals)\n",
    "            \n",
    "            classification_metrics = calculate_classification_metrics(real_vals_class, imputed_vals_class)\n",
    "            \n",
    "            validation_results[col] = {\n",
    "                'mae': mae,\n",
    "                'rmse': rmse,\n",
    "                'accuracy': classification_metrics['accuracy'],\n",
    "                'auc_multiclass': classification_metrics['auc_multiclass'],\n",
    "                'avg_sensitivity': classification_metrics['avg_sensitivity'],\n",
    "                'avg_specificity': classification_metrics['avg_specificity'],\n",
    "                'avg_ppv': classification_metrics['avg_ppv'],\n",
    "                'avg_npv': classification_metrics['avg_npv'],\n",
    "                'precision_macro': classification_metrics['precision_macro'],\n",
    "                'recall_macro': classification_metrics['recall_macro'],\n",
    "                'real_distribution': real_vals.describe(),\n",
    "                'imputed_distribution': imputed_vals.describe()\n",
    "            }\n",
    "    \n",
    "    return imputed_df, validation_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MICE Imputation:   0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "DTypePromotionError",
     "evalue": "The DType <class 'numpy.dtypes.DateTime64DType'> could not be promoted by <class 'numpy.dtypes.Float64DType'>. This means that no common DType exists for the given inputs. For example they cannot be stored in a single array unless the dtype is `object`. The full list of DTypes is: (<class 'numpy.dtypes.Int64DType'>, <class 'numpy.dtypes.DateTime64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Int8DType'>, <class 'numpy.dtypes.Int8DType'>, <class 'numpy.dtypes.Int8DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float32DType'>)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mDTypePromotionError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 8\u001b[0m\n\u001b[1;32m      1\u001b[0m ordinal_cols \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m      2\u001b[0m     [\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgp\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m8\u001b[39m)] \u001b[38;5;241m+\u001b[39m [\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgs\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m8\u001b[39m)] \u001b[38;5;241m+\u001b[39m\n\u001b[1;32m      3\u001b[0m     [\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mge\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m7\u001b[39m)] \u001b[38;5;241m+\u001b[39m [\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgf\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m8\u001b[39m)] \u001b[38;5;241m+\u001b[39m\n\u001b[1;32m      4\u001b[0m     [\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ma_hn\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m6\u001b[39m)] \u001b[38;5;241m+\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ma_hn7\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ma_hn10\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m+\u001b[39m\n\u001b[1;32m      5\u001b[0m     [\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ma_e\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m8\u001b[39m)] \u001b[38;5;241m+\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ma_c6\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ma_c2\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ma_act11\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m      6\u001b[0m )\n\u001b[1;32m      7\u001b[0m columns_to_impute \u001b[38;5;241m=\u001b[39m ordinal_cols\u001b[38;5;66;03m#['ge1', 'ge2', 'ge3', 'ge4', 'ge5', 'ge6']\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m imputed_df_mice, validation_results_mice \u001b[38;5;241m=\u001b[39m \u001b[43mapply_mice_imputation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns_to_impute\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[27], line 41\u001b[0m, in \u001b[0;36mapply_mice_imputation\u001b[0;34m(df, columns_to_impute, validation_df, validation_masks, original_values)\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;66;03m# Run imputation\u001b[39;00m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m5\u001b[39m), desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMICE Imputation\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m---> 41\u001b[0m     \u001b[43mkernel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmice\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     42\u001b[0m \u001b[43m        \u001b[49m\u001b[43miterations\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     43\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     44\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_boost_round\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m80\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     45\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_depth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     46\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_threads\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\n\u001b[1;32m     47\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;66;03m# Get imputed data\u001b[39;00m\n\u001b[1;32m     50\u001b[0m imputed_df \u001b[38;5;241m=\u001b[39m kernel\u001b[38;5;241m.\u001b[39mcomplete_data(\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[0;32m~/jinenv/lib/python3.11/site-packages/miceforest/ImputationKernel.py:1114\u001b[0m, in \u001b[0;36mImputationKernel.mice\u001b[0;34m(self, iterations, verbose, variable_parameters, compile_candidates, **kwlgb)\u001b[0m\n\u001b[1;32m   1112\u001b[0m logger\u001b[38;5;241m.\u001b[39mrecord_time(timed_event\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprepare_xy\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mlog_context)\n\u001b[1;32m   1113\u001b[0m logger\u001b[38;5;241m.\u001b[39mset_start_time()\n\u001b[0;32m-> 1114\u001b[0m current_model \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1115\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlgbpars\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1116\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_set\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_pointer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1117\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_boost_round\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_iterations\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1118\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcategorical_feature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfeature_cat_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1119\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1120\u001b[0m logger\u001b[38;5;241m.\u001b[39mrecord_time(timed_event\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtraining\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mlog_context)\n\u001b[1;32m   1122\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m save_model:\n",
      "File \u001b[0;32m~/jinenv/lib/python3.11/site-packages/lightgbm/engine.py:282\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(params, train_set, num_boost_round, valid_sets, valid_names, feval, init_model, feature_name, categorical_feature, keep_training_booster, callbacks)\u001b[0m\n\u001b[1;32m    280\u001b[0m \u001b[38;5;66;03m# construct booster\u001b[39;00m\n\u001b[1;32m    281\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 282\u001b[0m     booster \u001b[38;5;241m=\u001b[39m \u001b[43mBooster\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_set\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_set\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    283\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_valid_contain_train:\n\u001b[1;32m    284\u001b[0m         booster\u001b[38;5;241m.\u001b[39mset_train_data_name(train_data_name)\n",
      "File \u001b[0;32m~/jinenv/lib/python3.11/site-packages/lightgbm/basic.py:3637\u001b[0m, in \u001b[0;36mBooster.__init__\u001b[0;34m(self, params, train_set, model_file, model_str)\u001b[0m\n\u001b[1;32m   3630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mset_network(\n\u001b[1;32m   3631\u001b[0m         machines\u001b[38;5;241m=\u001b[39mmachines,\n\u001b[1;32m   3632\u001b[0m         local_listen_port\u001b[38;5;241m=\u001b[39mparams[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlocal_listen_port\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m   3633\u001b[0m         listen_time_out\u001b[38;5;241m=\u001b[39mparams\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtime_out\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m120\u001b[39m),\n\u001b[1;32m   3634\u001b[0m         num_machines\u001b[38;5;241m=\u001b[39mparams[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_machines\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m   3635\u001b[0m     )\n\u001b[1;32m   3636\u001b[0m \u001b[38;5;66;03m# construct booster object\u001b[39;00m\n\u001b[0;32m-> 3637\u001b[0m \u001b[43mtrain_set\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconstruct\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3638\u001b[0m \u001b[38;5;66;03m# copy the parameters from train_set\u001b[39;00m\n\u001b[1;32m   3639\u001b[0m params\u001b[38;5;241m.\u001b[39mupdate(train_set\u001b[38;5;241m.\u001b[39mget_params())\n",
      "File \u001b[0;32m~/jinenv/lib/python3.11/site-packages/lightgbm/basic.py:2576\u001b[0m, in \u001b[0;36mDataset.construct\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2571\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_init_score_by_predictor(\n\u001b[1;32m   2572\u001b[0m                 predictor\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_predictor, data\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata, used_indices\u001b[38;5;241m=\u001b[39mused_indices\n\u001b[1;32m   2573\u001b[0m             )\n\u001b[1;32m   2574\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2575\u001b[0m     \u001b[38;5;66;03m# create train\u001b[39;00m\n\u001b[0;32m-> 2576\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_lazy_init\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2577\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2578\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlabel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlabel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2579\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreference\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   2580\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2581\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgroup\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2582\u001b[0m \u001b[43m        \u001b[49m\u001b[43minit_score\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minit_score\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2583\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpredictor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_predictor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2584\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfeature_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeature_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2585\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcategorical_feature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcategorical_feature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2586\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2587\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mposition\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2588\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2589\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfree_raw_data:\n\u001b[1;32m   2590\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/jinenv/lib/python3.11/site-packages/lightgbm/basic.py:2106\u001b[0m, in \u001b[0;36mDataset._lazy_init\u001b[0;34m(self, data, label, reference, weight, group, init_score, predictor, feature_name, categorical_feature, params, position)\u001b[0m\n\u001b[1;32m   2104\u001b[0m     categorical_feature \u001b[38;5;241m=\u001b[39m reference\u001b[38;5;241m.\u001b[39mcategorical_feature\n\u001b[1;32m   2105\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, pd_DataFrame):\n\u001b[0;32m-> 2106\u001b[0m     data, feature_name, categorical_feature, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpandas_categorical \u001b[38;5;241m=\u001b[39m \u001b[43m_data_from_pandas\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2107\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2108\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfeature_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfeature_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2109\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcategorical_feature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcategorical_feature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2110\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpandas_categorical\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpandas_categorical\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2111\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2113\u001b[0m \u001b[38;5;66;03m# process for args\u001b[39;00m\n\u001b[1;32m   2114\u001b[0m params \u001b[38;5;241m=\u001b[39m {} \u001b[38;5;28;01mif\u001b[39;00m params \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m params\n",
      "File \u001b[0;32m~/jinenv/lib/python3.11/site-packages/lightgbm/basic.py:845\u001b[0m, in \u001b[0;36m_data_from_pandas\u001b[0;34m(data, feature_name, categorical_feature, pandas_categorical)\u001b[0m\n\u001b[1;32m    843\u001b[0m \u001b[38;5;66;03m# so that the target dtype considers floats\u001b[39;00m\n\u001b[1;32m    844\u001b[0m df_dtypes\u001b[38;5;241m.\u001b[39mappend(np\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[0;32m--> 845\u001b[0m target_dtype \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult_type\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mdf_dtypes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    847\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[1;32m    848\u001b[0m     _pandas_to_numpy(data, target_dtype\u001b[38;5;241m=\u001b[39mtarget_dtype),\n\u001b[1;32m    849\u001b[0m     feature_name,\n\u001b[1;32m    850\u001b[0m     categorical_feature,\n\u001b[1;32m    851\u001b[0m     pandas_categorical,\n\u001b[1;32m    852\u001b[0m )\n",
      "\u001b[0;31mDTypePromotionError\u001b[0m: The DType <class 'numpy.dtypes.DateTime64DType'> could not be promoted by <class 'numpy.dtypes.Float64DType'>. This means that no common DType exists for the given inputs. For example they cannot be stored in a single array unless the dtype is `object`. The full list of DTypes is: (<class 'numpy.dtypes.Int64DType'>, <class 'numpy.dtypes.DateTime64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Int8DType'>, <class 'numpy.dtypes.Int8DType'>, <class 'numpy.dtypes.Int8DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float32DType'>)"
     ]
    }
   ],
   "source": [
    "ordinal_cols = (\n",
    "    [f\"gp{i}\" for i in range(1, 8)] + [f\"gs{i}\" for i in range(1, 8)] +\n",
    "    [f\"ge{i}\" for i in range(1, 7)] + [f\"gf{i}\" for i in range(1, 8)] +\n",
    "    [f\"a_hn{i}\" for i in range(1, 6)] + [\"a_hn7\", \"a_hn10\"] +\n",
    "    [f\"a_e{i}\" for i in range(1, 8)] + [\"a_c6\", \"a_c2\", \"a_act11\"]\n",
    ")\n",
    "columns_to_impute = ordinal_cols#['ge1', 'ge2', 'ge3', 'ge4', 'ge5', 'ge6']\n",
    "imputed_df_mice, validation_results_mice = apply_mice_imputation(df, columns_to_impute)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model - VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import sys\n",
    "\n",
    "class VAE(nn.Module):\n",
    "    \"\"\"\n",
    "    Variational VAE optimized for CPU usage\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim, hidden_dims=(64, 32, 16), latent_dim=8, dropout_rate=0.1):\n",
    "        \"\"\"\n",
    "        Initialize VAE model\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        input_dim : int\n",
    "            Dimension of input feature space\n",
    "        hidden_dims : tuple\n",
    "            Sizes of hidden layers in the encoder and decoder\n",
    "        latent_dim : int\n",
    "            Dimension of the latent space\n",
    "        dropout_rate : float\n",
    "            Dropout rate for regularization\n",
    "        \"\"\"\n",
    "        super(VAE, self).__init__()\n",
    "        \n",
    "        # Determine if we should use batch normalization (can be slow on CPU)\n",
    "        self.use_cpu_efficient = True  # Flag for CPU optimization\n",
    "        \n",
    "        # Encoder layers\n",
    "        encoder_layers = []\n",
    "        prev_dim = input_dim\n",
    "        \n",
    "        for h_dim in hidden_dims:\n",
    "            layer_components = [nn.Linear(prev_dim, h_dim)]\n",
    "            \n",
    "            # Use layer norm instead of batch norm if optimizing for CPU\n",
    "            if self.use_cpu_efficient:\n",
    "                layer_components.append(nn.LayerNorm(h_dim))\n",
    "            else:\n",
    "                layer_components.append(nn.BatchNorm1d(h_dim))\n",
    "                \n",
    "            layer_components.extend([\n",
    "                nn.LeakyReLU(),\n",
    "                nn.Dropout(dropout_rate)\n",
    "            ])\n",
    "            \n",
    "            encoder_layers.append(nn.Sequential(*layer_components))\n",
    "            prev_dim = h_dim\n",
    "        \n",
    "        self.encoder_layers = nn.ModuleList(encoder_layers)\n",
    "        \n",
    "        # Latent space mapping\n",
    "        self.mu_layer = nn.Linear(hidden_dims[-1], latent_dim)\n",
    "        self.logvar_layer = nn.Linear(hidden_dims[-1], latent_dim)\n",
    "        \n",
    "        # Decoder layers\n",
    "        decoder_layers = []\n",
    "        decoder_dims = list(reversed(hidden_dims))\n",
    "        \n",
    "        # First decoder layer from latent space\n",
    "        first_layer_components = [nn.Linear(latent_dim, decoder_dims[0])]\n",
    "        \n",
    "        # Use layer norm instead of batch norm if optimizing for CPU\n",
    "        if self.use_cpu_efficient:\n",
    "            first_layer_components.append(nn.LayerNorm(decoder_dims[0]))\n",
    "        else:\n",
    "            first_layer_components.append(nn.BatchNorm1d(decoder_dims[0]))\n",
    "            \n",
    "        first_layer_components.extend([\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Dropout(dropout_rate)\n",
    "        ])\n",
    "        \n",
    "        decoder_layers.append(nn.Sequential(*first_layer_components))\n",
    "        \n",
    "        # Remaining decoder layers\n",
    "        for i in range(len(decoder_dims) - 1):\n",
    "            layer_components = [nn.Linear(decoder_dims[i], decoder_dims[i+1])]\n",
    "            \n",
    "            # Use layer norm instead of batch norm if optimizing for CPU\n",
    "            if self.use_cpu_efficient:\n",
    "                layer_components.append(nn.LayerNorm(decoder_dims[i+1]))\n",
    "            else:\n",
    "                layer_components.append(nn.BatchNorm1d(decoder_dims[i+1]))\n",
    "                \n",
    "            layer_components.extend([\n",
    "                nn.LeakyReLU(),\n",
    "                nn.Dropout(dropout_rate)\n",
    "            ])\n",
    "            \n",
    "            decoder_layers.append(nn.Sequential(*layer_components))\n",
    "        \n",
    "        # Output layer\n",
    "        decoder_layers.append(nn.Linear(decoder_dims[-1], input_dim))\n",
    "        \n",
    "        self.decoder_layers = nn.ModuleList(decoder_layers)\n",
    "        \n",
    "    def encode(self, x):\n",
    "        \"\"\"\n",
    "        Encode input to latent parameters\n",
    "        \"\"\"\n",
    "        h = x\n",
    "        for layer in self.encoder_layers:\n",
    "            h = layer(h)\n",
    "            \n",
    "        mu = self.mu_layer(h)\n",
    "        logvar = self.logvar_layer(h)\n",
    "        \n",
    "        return mu, logvar\n",
    "    \n",
    "    def reparameterize(self, mu, logvar):\n",
    "        \"\"\"\n",
    "        Reparameterization trick\n",
    "        \"\"\"\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        z = mu + eps * std\n",
    "        return z\n",
    "    \n",
    "    def decode(self, z):\n",
    "        \"\"\"\n",
    "        Decode latent vector to reconstructed input\n",
    "        \"\"\"\n",
    "        h = z\n",
    "        for layer in self.decoder_layers:\n",
    "            h = layer(h)\n",
    "            \n",
    "        return h\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass through the VAE\n",
    "        \"\"\"\n",
    "        mu, logvar = self.encode(x)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        x_reconstructed = self.decode(z)\n",
    "        \n",
    "        return x_reconstructed, mu, logvar\n",
    "    \n",
    "    \n",
    "class MaskedMSELoss(nn.Module):\n",
    "    \"\"\"\n",
    "    MSE loss that only considers non-missing values\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(MaskedMSELoss, self).__init__()\n",
    "        \n",
    "    def forward(self, pred, target, mask):\n",
    "        \"\"\"\n",
    "        Calculate MSE loss ignoring missing values\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        pred : torch.Tensor\n",
    "            Predicted values\n",
    "        target : torch.Tensor\n",
    "            Target values\n",
    "        mask : torch.Tensor\n",
    "            Binary mask (1 for observed, 0 for missing)\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        loss : torch.Tensor\n",
    "            Masked MSE loss\n",
    "        \"\"\"\n",
    "        # Only calculate loss for observed values\n",
    "        masked_pred = pred * mask\n",
    "        masked_target = target * mask\n",
    "        \n",
    "        # Calculate squared error\n",
    "        se = (masked_pred - masked_target) ** 2\n",
    "        \n",
    "        # Sum squared error and count observed values\n",
    "        sse = torch.sum(se)\n",
    "        count = torch.sum(mask)\n",
    "        \n",
    "        # Return MSE\n",
    "        return sse / (count + 1e-8)\n",
    "\n",
    "\n",
    "def kl_divergence_loss(mu, logvar):\n",
    "    \"\"\"\n",
    "    Calculate KL divergence loss\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    mu : torch.Tensor\n",
    "        Mean of the latent space\n",
    "    logvar : torch.Tensor\n",
    "        Log variance of the latent space\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    kl_loss : torch.Tensor\n",
    "        KL divergence loss\n",
    "    \"\"\"\n",
    "    # KL divergence between q(z|x) and p(z)\n",
    "    kl_loss = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    return kl_loss\n",
    "\n",
    "\n",
    "class VAEImputer:\n",
    "    \"\"\"\n",
    "    Variational VAE Imputation model using PyTorch\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 hidden_dims=(64, 32, 16),\n",
    "                 latent_dim=8,\n",
    "                 batch_size=64,\n",
    "                 learning_rate=0.001,\n",
    "                 epochs=100,\n",
    "                 beta=1.0,\n",
    "                 device=None,\n",
    "                 verbose=True):\n",
    "        \"\"\"\n",
    "        Initialize VAE imputer\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        hidden_dims : tuple\n",
    "            Sizes of hidden layers in the encoder and decoder\n",
    "        latent_dim : int\n",
    "            Dimension of the latent space\n",
    "        batch_size : int\n",
    "            Batch size for training\n",
    "        learning_rate : float\n",
    "            Learning rate for optimizer\n",
    "        epochs : int\n",
    "            Number of training epochs\n",
    "        beta : float\n",
    "            Weight of the KL divergence loss (beta-VAE)\n",
    "        device : torch.device\n",
    "            Device to use for training (CPU or GPU)\n",
    "        verbose : bool\n",
    "            Whether to print progress\n",
    "        \"\"\"\n",
    "        self.hidden_dims = hidden_dims\n",
    "        self.latent_dim = latent_dim\n",
    "        self.batch_size = batch_size\n",
    "        self.learning_rate = learning_rate\n",
    "        self.epochs = epochs\n",
    "        self.beta = beta\n",
    "        self.verbose = verbose\n",
    "        \n",
    "        # Set device - Compute Canada specific approach\n",
    "        if device is None:\n",
    "            # Check for SLURM environment variables that indicate we're on a cluster\n",
    "            slurm_job_id = os.environ.get('SLURM_JOB_ID')\n",
    "            \n",
    "            if slurm_job_id:\n",
    "                print(f\"Running on Compute Canada cluster (Job ID: {slurm_job_id})\")\n",
    "                \n",
    "                # Check if GPUs were allocated\n",
    "                slurm_gpus = os.environ.get('SLURM_GPUS')\n",
    "                if slurm_gpus:\n",
    "                    print(f\"GPUs allocated: {slurm_gpus}\")\n",
    "                    \n",
    "                # On Compute Canada, we need to look at the environment variables\n",
    "                visible_devices = os.environ.get('CUDA_VISIBLE_DEVICES')\n",
    "                print(f\"CUDA_VISIBLE_DEVICES: {visible_devices}\")\n",
    "                \n",
    "                if torch.cuda.is_available():\n",
    "                    self.device = torch.device('cuda')\n",
    "                    n_gpus = torch.cuda.device_count()\n",
    "                    print(f\"PyTorch sees {n_gpus} GPUs\")\n",
    "                    if n_gpus > 0:\n",
    "                        print(f\"Using GPU: {torch.cuda.get_device_name(0)}\")\n",
    "                        # Show GPU memory\n",
    "                        print(f\"GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "                    else:\n",
    "                        print(\"Warning: torch.cuda.is_available() is True but device_count() is 0\")\n",
    "                        self.device = torch.device('cpu')\n",
    "                else:\n",
    "                    print(\"CUDA not available according to PyTorch\")\n",
    "                    # Try to provide more diagnostics\n",
    "                    try:\n",
    "                        if os.path.exists('/usr/bin/nvidia-smi'):\n",
    "                            import subprocess\n",
    "                            result = subprocess.run(['nvidia-smi'], stdout=subprocess.PIPE)\n",
    "                            print(\"nvidia-smi output:\")\n",
    "                            print(result.stdout.decode('utf-8'))\n",
    "                        else:\n",
    "                            print(\"nvidia-smi not found in /usr/bin\")\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error checking for nvidia-smi: {e}\")\n",
    "                    \n",
    "                    self.device = torch.device('cpu')\n",
    "            else:\n",
    "                # Not on a cluster, use standard detection\n",
    "                if torch.cuda.is_available():\n",
    "                    self.device = torch.device('cuda')\n",
    "                    print(f\"Using GPU: {torch.cuda.get_device_name(0)}\")\n",
    "                else:\n",
    "                    self.device = torch.device('cpu')\n",
    "                    print(\"Using CPU\")\n",
    "        else:\n",
    "            self.device = device\n",
    "            \n",
    "        # Initialize model, scalers and masks\n",
    "        self.model = None\n",
    "        self.scaler = None\n",
    "        self.columns = None\n",
    "        \n",
    "    def fit(self, X, columns_to_impute=None):\n",
    "        \"\"\"\n",
    "        Fit VAE model for imputation\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X : pandas.DataFrame\n",
    "            Data with missing values\n",
    "        columns_to_impute : list, optional\n",
    "            List of column names to impute. If None, all columns with missing values.\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        self : object\n",
    "            Returns self\n",
    "        \"\"\"\n",
    "        # Identify columns to impute if not specified\n",
    "        if columns_to_impute is None:\n",
    "            columns_to_impute = [col for col in X.columns if X[col].isna().any()]\n",
    "        \n",
    "        # Store list of columns to impute and all columns\n",
    "        self.columns_to_impute = columns_to_impute\n",
    "        self.columns = list(X.columns)\n",
    "        \n",
    "        # Create missing value mask (1 for observed, 0 for missing)\n",
    "        missing_mask = ~X.isna()\n",
    "        \n",
    "        # Initialize scaler\n",
    "        self.scaler = StandardScaler()\n",
    "        \n",
    "        # Simple imputer for initial values (will be refined by VAE)\n",
    "        simple_imputer = SimpleImputer(strategy='mean')\n",
    "        X_simple_imputed = pd.DataFrame(\n",
    "            simple_imputer.fit_transform(X),\n",
    "            columns=X.columns,\n",
    "            index=X.index\n",
    "        )\n",
    "        \n",
    "        # Scale data\n",
    "        X_scaled = self.scaler.fit_transform(X_simple_imputed)\n",
    "        \n",
    "        # Convert to PyTorch tensors\n",
    "        X_tensor = torch.FloatTensor(X_scaled)\n",
    "        mask_tensor = torch.FloatTensor(missing_mask.values)\n",
    "        \n",
    "        # Create dataset and dataloader\n",
    "        dataset = TensorDataset(X_tensor, mask_tensor)\n",
    "        # Use multiple workers if on CPU for better performance\n",
    "        num_workers = 0\n",
    "        if self.device.type == 'cpu':\n",
    "            import multiprocessing\n",
    "            num_workers = min(2, multiprocessing.cpu_count() // 2)  # Use at most half the cores\n",
    "        \n",
    "        dataloader = DataLoader(\n",
    "            dataset, \n",
    "            batch_size=self.batch_size, \n",
    "            shuffle=True,\n",
    "            num_workers=num_workers,\n",
    "            pin_memory=(self.device.type == 'cuda')  # Only pin memory if using CUDA\n",
    "        )\n",
    "        \n",
    "        # Initialize model\n",
    "        input_dim = X.shape[1]\n",
    "        self.model = VAE(\n",
    "            input_dim=input_dim,\n",
    "            hidden_dims=self.hidden_dims,\n",
    "            latent_dim=self.latent_dim\n",
    "        ).to(self.device)\n",
    "        \n",
    "        # Initialize optimizer\n",
    "        optimizer = optim.Adam(self.model.parameters(), lr=self.learning_rate)\n",
    "        \n",
    "        # Initialize loss function\n",
    "        recon_loss_fn = MaskedMSELoss()\n",
    "        \n",
    "        # Training loop\n",
    "        self.model.train()\n",
    "        \n",
    "        # Create progress bar if verbose\n",
    "        pbar = range(self.epochs)\n",
    "        if self.verbose:\n",
    "            pbar = tqdm(pbar, desc=\"Training VAE\")\n",
    "            \n",
    "        for epoch in pbar:\n",
    "            epoch_loss = 0.0\n",
    "            epoch_recon_loss = 0.0\n",
    "            epoch_kl_loss = 0.0\n",
    "            \n",
    "            for x, mask in dataloader:\n",
    "                # Move tensors to device\n",
    "                x = x.to(self.device)\n",
    "                mask = mask.to(self.device)\n",
    "                \n",
    "                # Forward pass\n",
    "                x_reconstructed, mu, logvar = self.model(x)\n",
    "                \n",
    "                # Calculate losses\n",
    "                recon_loss = recon_loss_fn(x_reconstructed, x, mask)\n",
    "                kl_loss = kl_divergence_loss(mu, logvar) / x.size(0)  # Normalize by batch size\n",
    "                \n",
    "                # Total loss (beta-VAE formulation)\n",
    "                loss = recon_loss + self.beta * kl_loss\n",
    "                \n",
    "                # Backward pass and optimize\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "                # Accumulate losses\n",
    "                epoch_loss += loss.item()\n",
    "                epoch_recon_loss += recon_loss.item()\n",
    "                epoch_kl_loss += kl_loss.item()\n",
    "            \n",
    "            # Update progress bar\n",
    "            if self.verbose:\n",
    "                pbar.set_postfix({\n",
    "                    \"loss\": epoch_loss / len(dataloader),\n",
    "                    \"recon\": epoch_recon_loss / len(dataloader),\n",
    "                    \"kl\": epoch_kl_loss / len(dataloader)\n",
    "                })\n",
    "                \n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        \"\"\"\n",
    "        Impute missing values using trained VAE model\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X : pandas.DataFrame\n",
    "            Data with missing values\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        imputed_df : pandas.DataFrame\n",
    "            Data with imputed values\n",
    "        \"\"\"\n",
    "        # Create copy to avoid modifying original\n",
    "        imputed_df = X.copy()\n",
    "        \n",
    "        # Get missing value mask\n",
    "        missing_mask = X.isna()\n",
    "        \n",
    "        # Use simple imputation for initial values\n",
    "        simple_imputer = SimpleImputer(strategy='mean')\n",
    "        X_simple_imputed = pd.DataFrame(\n",
    "            simple_imputer.fit_transform(X),\n",
    "            columns=X.columns,\n",
    "            index=X.index\n",
    "        )\n",
    "        \n",
    "        # Scale data\n",
    "        X_scaled = self.scaler.transform(X_simple_imputed)\n",
    "        \n",
    "        # Convert to PyTorch tensor\n",
    "        X_tensor = torch.FloatTensor(X_scaled).to(self.device)\n",
    "        \n",
    "        # Set model to evaluation mode\n",
    "        self.model.eval()\n",
    "        \n",
    "        # Disable gradient calculation\n",
    "        with torch.no_grad():\n",
    "            # Forward pass through VAE\n",
    "            X_reconstructed, _, _ = self.model(X_tensor)\n",
    "            \n",
    "            # Convert reconstructed values to numpy\n",
    "            X_reconstructed_np = X_reconstructed.cpu().numpy()\n",
    "            \n",
    "            # Inverse transform to original scale\n",
    "            X_reconstructed_orig = self.scaler.inverse_transform(X_reconstructed_np)\n",
    "            \n",
    "            # Create DataFrame with reconstructed values\n",
    "            X_reconstructed_df = pd.DataFrame(\n",
    "                X_reconstructed_orig,\n",
    "                columns=X.columns,\n",
    "                index=X.index\n",
    "            )\n",
    "            \n",
    "            # Replace missing values with imputed values for specified columns\n",
    "            for col in self.columns_to_impute:\n",
    "                # Only replace missing values\n",
    "                imputed_df.loc[missing_mask[col], col] = X_reconstructed_df.loc[missing_mask[col], col]\n",
    "        \n",
    "        return imputed_df\n",
    "    \n",
    "    def fit_transform(self, X, columns_to_impute=None):\n",
    "        \"\"\"\n",
    "        Fit model and impute missing values\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X : pandas.DataFrame\n",
    "            Data with missing values\n",
    "        columns_to_impute : list, optional\n",
    "            List of column names to impute. If None, all columns with missing values.\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        imputed_df : pandas.DataFrame\n",
    "            Data with imputed values\n",
    "        \"\"\"\n",
    "        self.fit(X, columns_to_impute)\n",
    "        return self.transform(X)\n",
    "\n",
    "\n",
    "def apply_vae_imputation(df, columns_to_impute, validation_df=None, validation_masks=None, original_values=None,\n",
    "                        hidden_dims=(64, 32, 16), latent_dim=8, batch_size=64, epochs=100, learning_rate=0.001):\n",
    "    \"\"\"\n",
    "    Apply VAE imputation to data\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pandas.DataFrame\n",
    "        Data with missing values\n",
    "    columns_to_impute : list\n",
    "        List of column names to impute\n",
    "    validation_df : pandas.DataFrame, optional\n",
    "        Validation dataset with artificially missing values\n",
    "    validation_masks : dict, optional\n",
    "        Dictionary of masks for validation data\n",
    "    original_values : dict, optional\n",
    "        Dictionary of original values for validation\n",
    "    hidden_dims : tuple\n",
    "        Sizes of hidden layers\n",
    "    latent_dim : int\n",
    "        Dimension of the latent space\n",
    "    batch_size : int\n",
    "        Batch size for training\n",
    "    epochs : int\n",
    "        Number of training epochs\n",
    "    learning_rate : float\n",
    "        Learning rate for optimizer\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    imputed_df : pandas.DataFrame\n",
    "        Data with imputed values\n",
    "    validation_results : dict, optional\n",
    "        Validation results if validation data provided\n",
    "    \"\"\"\n",
    "    # Extract relevant columns including potential predictors\n",
    "    # Use all columns except those with excessive missing values\n",
    "    threshold = 0.5  # Columns with more than 50% missing values are excluded\n",
    "    columns_to_use = [col for col in df.columns \n",
    "                        if df[col].isna().mean() < threshold]\n",
    "    \n",
    "    # Ensure all columns_to_impute are included\n",
    "    for col in columns_to_impute:\n",
    "        if col not in columns_to_use:\n",
    "            columns_to_use.append(col)\n",
    "    \n",
    "    # Extract subset of data\n",
    "    X = df[columns_to_use].copy()\n",
    "    \n",
    "    # Ensure all columns are numeric\n",
    "    for col in X.columns:\n",
    "        X[col] = pd.to_numeric(X[col], errors='coerce')\n",
    "    \n",
    "    # Compute Canada specific detection\n",
    "    print(\"\\n--- Compute Canada / Cluster Environment Detection ---\")\n",
    "    slurm_info = {\n",
    "        'SLURM_JOB_ID': os.environ.get('SLURM_JOB_ID', 'Not found'),\n",
    "        'SLURM_JOB_GPUS': os.environ.get('SLURM_JOB_GPUS', 'Not found'),\n",
    "        'SLURM_GPUS': os.environ.get('SLURM_GPUS', 'Not found'), \n",
    "        'CUDA_VISIBLE_DEVICES': os.environ.get('CUDA_VISIBLE_DEVICES', 'Not found'),\n",
    "        'SLURM_JOB_PARTITION': os.environ.get('SLURM_JOB_PARTITION', 'Not found'),\n",
    "    }\n",
    "    \n",
    "    for key, value in slurm_info.items():\n",
    "        print(f\"{key}: {value}\")\n",
    "    \n",
    "    # Check CUDA availability with Compute Canada settings\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device('cuda')\n",
    "        n_gpus = torch.cuda.device_count()\n",
    "        print(f\"\\nPyTorch detects {n_gpus} GPUs\")\n",
    "        \n",
    "        # On Compute Canada clusters, sometimes device_count is incorrect\n",
    "        # but we can still use cuda:0 if CUDA_VISIBLE_DEVICES is set\n",
    "        if n_gpus == 0 and os.environ.get('CUDA_VISIBLE_DEVICES') is not None:\n",
    "            print(\"Detected potential Compute Canada environment mismatch\")\n",
    "            print(\"Attempting to force GPU usage...\")\n",
    "            try:\n",
    "                # Try to explicitly set the device\n",
    "                device = torch.device('cuda:0')\n",
    "                # Test if it works\n",
    "                test_tensor = torch.zeros(1).to(device)\n",
    "                print(\"Successfully forced GPU usage!\")\n",
    "            except Exception as e:\n",
    "                print(f\"Force attempt failed: {e}\")\n",
    "                device = torch.device('cpu')\n",
    "                print(\"Falling back to CPU\")\n",
    "        else:\n",
    "            for i in range(n_gpus):\n",
    "                print(f\"GPU #{i}: {torch.cuda.get_device_name(i)}\")\n",
    "                props = torch.cuda.get_device_properties(i)\n",
    "                print(f\"  Memory: {props.total_memory / 1e9:.2f} GB\")\n",
    "                print(f\"  Compute Capability: {props.major}.{props.minor}\")\n",
    "    else:\n",
    "        device = torch.device('cpu')\n",
    "        print(\"\\nCUDA not available, using CPU\")\n",
    "        \n",
    "        # Check if we're on a compute node that should have GPUs\n",
    "        if 'SLURM_JOB_GPUS' in os.environ or 'SLURM_GPUS' in os.environ:\n",
    "            print(\"Warning: GPUs were allocated in SLURM but PyTorch cannot detect them\")\n",
    "            print(\"This might be due to a configuration issue.\")\n",
    "            print(\"Try running 'module load cuda' before starting your script\")\n",
    "    \n",
    "    print(\"\\n--- End of Environment Detection ---\\n\")\n",
    "    \n",
    "    # Initialize VAE imputer\n",
    "    imputer = VAEImputer(\n",
    "        hidden_dims=hidden_dims,\n",
    "        latent_dim=latent_dim,\n",
    "        batch_size=batch_size,\n",
    "        learning_rate=learning_rate,\n",
    "        epochs=epochs,\n",
    "        device=device,\n",
    "        verbose=True\n",
    "    )\n",
    "    \n",
    "    # Fit and transform\n",
    "    print(\"Training VAE imputation model...\")\n",
    "    X_imputed = imputer.fit_transform(X, columns_to_impute)\n",
    "    \n",
    "    # Create imputed dataframe\n",
    "    imputed_df = df.copy()\n",
    "    imputed_df[columns_to_impute] = X_imputed[columns_to_impute]\n",
    "    \n",
    "    # Validate if validation data provided\n",
    "    validation_results = None\n",
    "    if validation_df is not None and validation_masks is not None and original_values is not None:\n",
    "        validation_results = {}\n",
    "        \n",
    "        # Extract validation data\n",
    "        X_val = validation_df[columns_to_use].copy()\n",
    "        \n",
    "        # Ensure all validation columns are numeric\n",
    "        for col in X_val.columns:\n",
    "            X_val[col] = pd.to_numeric(X_val[col], errors='coerce')\n",
    "            \n",
    "        # Impute validation data\n",
    "        X_val_imputed = imputer.transform(X_val)\n",
    "        \n",
    "        # Compare imputed values to real values\n",
    "        with tqdm(columns_to_impute, desc=\"Validating results\") as pbar:\n",
    "            for col in pbar:\n",
    "                pbar.set_description(f\"Validating {col}\")\n",
    "                # Get indices where values were artificially set to NaN\n",
    "                mask = validation_masks[col] & validation_df[col].isna()\n",
    "                \n",
    "                if mask.sum() == 0:\n",
    "                    validation_results[col] = {\n",
    "                        'error': \"No artificially missing values\"\n",
    "                    }\n",
    "                    continue\n",
    "                    \n",
    "                real_vals = original_values[col][mask]\n",
    "                imputed_vals = X_val_imputed.loc[mask, col]\n",
    "                \n",
    "                # Calculate continuous metrics (MAE and RMSE) - NO ROUNDING\n",
    "                mae = mean_absolute_error(real_vals, imputed_vals)\n",
    "                rmse = np.sqrt(mean_squared_error(real_vals, imputed_vals))\n",
    "                \n",
    "                # Calculate classification metrics - WITH ROUNDING\n",
    "                real_vals_class = process_for_classification(real_vals)\n",
    "                imputed_vals_class = process_for_classification(imputed_vals)\n",
    "                \n",
    "                classification_metrics = calculate_classification_metrics(real_vals_class, imputed_vals_class)\n",
    "                \n",
    "                validation_results[col] = {\n",
    "                    'mae': mae,\n",
    "                    'rmse': rmse,\n",
    "                    'accuracy': classification_metrics['accuracy'],\n",
    "                    'auc_multiclass': classification_metrics['auc_multiclass'],\n",
    "                    'avg_sensitivity': classification_metrics['avg_sensitivity'],\n",
    "                    'avg_specificity': classification_metrics['avg_specificity'],\n",
    "                    'avg_ppv': classification_metrics['avg_ppv'],\n",
    "                    'avg_npv': classification_metrics['avg_npv'],\n",
    "                    'precision_macro': classification_metrics['precision_macro'],\n",
    "                    'recall_macro': classification_metrics['recall_macro'],\n",
    "                    'real_distribution': real_vals.describe(),\n",
    "                    'imputed_distribution': pd.Series(imputed_vals).describe()\n",
    "                }\n",
    "                \n",
    "                # Update progress\n",
    "                pbar.set_postfix({\"MAE\": f\"{mae:.4f}\", \"RMSE\": f\"{rmse:.4f}\", \"Acc\": f\"{classification_metrics['accuracy']:.4f}\"})\n",
    "    \n",
    "    return imputed_df, validation_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== CUDA Diagnostics ===\n",
      "PyTorch version: 2.6.0\n",
      "CUDA version: 12.2\n",
      "\n",
      "CUDA availability check:\n",
      "torch.cuda.is_available(): False\n",
      "Could not run nvidia-smi\n",
      "\n",
      "CUDA Environment Variables:\n",
      "CUDA_VISIBLE_DEVICES: Not set\n",
      "CUDA_DEVICE_ORDER: Not set\n",
      "CUDA_HOME: Not set\n",
      "LD_LIBRARY_PATH: Not set\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "No GPUs detected by PyTorch.\n",
      "If you see GPUs in nvidia-smi but not here, check CUDA version compatibility.\n",
      "\n",
      "=== End of Diagnostics ===\n",
      "\n",
      "\n",
      "--- Compute Canada / Cluster Environment Detection ---\n",
      "SLURM_JOB_ID: Not found\n",
      "SLURM_JOB_GPUS: Not found\n",
      "SLURM_GPUS: Not found\n",
      "CUDA_VISIBLE_DEVICES: Not found\n",
      "SLURM_JOB_PARTITION: Not found\n",
      "\n",
      "CUDA not available, using CPU\n",
      "\n",
      "--- End of Environment Detection ---\n",
      "\n",
      "Training VAE imputation model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training VAE:   0%|          | 0/100 [00:07<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[30], line 72\u001b[0m\n\u001b[1;32m     69\u001b[0m columns_to_impute \u001b[38;5;241m=\u001b[39m ordinal_cols\u001b[38;5;66;03m#['ge1', 'ge2', 'ge3', 'ge4', 'ge5', 'ge6']\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;66;03m# Apply VAE imputation\u001b[39;00m\n\u001b[0;32m---> 72\u001b[0m imputed_df_vae, validation_results_vae  \u001b[38;5;241m=\u001b[39m \u001b[43mapply_vae_imputation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns_to_impute\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[29], line 638\u001b[0m, in \u001b[0;36mapply_vae_imputation\u001b[0;34m(df, columns_to_impute, validation_df, validation_masks, original_values, hidden_dims, latent_dim, batch_size, epochs, learning_rate)\u001b[0m\n\u001b[1;32m    636\u001b[0m \u001b[38;5;66;03m# Fit and transform\u001b[39;00m\n\u001b[1;32m    637\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining VAE imputation model...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 638\u001b[0m X_imputed \u001b[38;5;241m=\u001b[39m \u001b[43mimputer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns_to_impute\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    640\u001b[0m \u001b[38;5;66;03m# Create imputed dataframe\u001b[39;00m\n\u001b[1;32m    641\u001b[0m imputed_df \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mcopy()\n",
      "Cell \u001b[0;32mIn[29], line 516\u001b[0m, in \u001b[0;36mVAEImputer.fit_transform\u001b[0;34m(self, X, columns_to_impute)\u001b[0m\n\u001b[1;32m    500\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mfit_transform\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, columns_to_impute\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    501\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    502\u001b[0m \u001b[38;5;124;03m    Fit model and impute missing values\u001b[39;00m\n\u001b[1;32m    503\u001b[0m \u001b[38;5;124;03m    \u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    514\u001b[0m \u001b[38;5;124;03m        Data with imputed values\u001b[39;00m\n\u001b[1;32m    515\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 516\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns_to_impute\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    517\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform(X)\n",
      "Cell \u001b[0;32mIn[29], line 420\u001b[0m, in \u001b[0;36mVAEImputer.fit\u001b[0;34m(self, X, columns_to_impute)\u001b[0m\n\u001b[1;32m    418\u001b[0m \u001b[38;5;66;03m# Backward pass and optimize\u001b[39;00m\n\u001b[1;32m    419\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m--> 420\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    421\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m    423\u001b[0m \u001b[38;5;66;03m# Accumulate losses\u001b[39;00m\n",
      "File \u001b[0;32m~/jinenv/lib/python3.11/site-packages/torch/_tensor.py:626\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    616\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    617\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    618\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    619\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    624\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    625\u001b[0m     )\n\u001b[0;32m--> 626\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    627\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    628\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/jinenv/lib/python3.11/site-packages/torch/autograd/__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/jinenv/lib/python3.11/site-packages/torch/autograd/graph.py:823\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    821\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    822\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 823\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    824\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    825\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    826\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    827\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # More detailed CUDA diagnostics\n",
    "    print(\"\\n=== CUDA Diagnostics ===\")\n",
    "    print(f\"PyTorch version: {torch.__version__}\")\n",
    "    print(f\"CUDA version: {torch.version.cuda}\")\n",
    "    \n",
    "    # Check CUDA availability\n",
    "    print(f\"\\nCUDA availability check:\")\n",
    "    print(f\"torch.cuda.is_available(): {torch.cuda.is_available()}\")\n",
    "    \n",
    "    # Show NVIDIA driver info if available\n",
    "    try:\n",
    "        import subprocess\n",
    "        nvidia_info = subprocess.check_output(\"nvidia-smi\", shell=True).decode()\n",
    "        print(\"\\nNVIDIA-SMI output:\")\n",
    "        print(nvidia_info)\n",
    "    except:\n",
    "        print(\"Could not run nvidia-smi\")\n",
    "    \n",
    "    # CUDA environment variables\n",
    "    print(\"\\nCUDA Environment Variables:\")\n",
    "    import os\n",
    "    cuda_vars = [\n",
    "        \"CUDA_VISIBLE_DEVICES\",\n",
    "        \"CUDA_DEVICE_ORDER\",\n",
    "        \"CUDA_HOME\",\n",
    "        \"LD_LIBRARY_PATH\"\n",
    "    ]\n",
    "    for var in cuda_vars:\n",
    "        print(f\"{var}: {os.environ.get(var, 'Not set')}\")\n",
    "    \n",
    "    # Try to force CUDA if available\n",
    "    if torch.cuda.device_count() > 0:\n",
    "        print(\"\\nGPUs detected by PyTorch:\")\n",
    "        for i in range(torch.cuda.device_count()):\n",
    "            print(f\"  GPU #{i}: {torch.cuda.get_device_name(i)}\")\n",
    "            \n",
    "        # Try to force CUDA initialization\n",
    "        try:\n",
    "            print(\"\\nAttempting to force CUDA initialization...\")\n",
    "            x = torch.zeros(1).cuda()\n",
    "            print(f\"  Success! Test tensor on: {x.device}\")\n",
    "        except Exception as e:\n",
    "            print(f\"  Failed to initialize CUDA: {e}\")\n",
    "\n",
    "            # Try enabling TF32 or reduced precision\n",
    "            try:\n",
    "                print(\"\\nTrying to enable TF32...\")\n",
    "                torch.backends.cuda.matmul.allow_tf32 = True\n",
    "                torch.backends.cudnn.allow_tf32 = True\n",
    "                x = torch.zeros(1).cuda()\n",
    "                print(f\"  Success with TF32! Test tensor on: {x.device}\")\n",
    "            except Exception as e:\n",
    "                print(f\"  Failed with TF32: {e}\")\n",
    "    else:\n",
    "        print(\"\\nNo GPUs detected by PyTorch.\")\n",
    "        print(\"If you see GPUs in nvidia-smi but not here, check CUDA version compatibility.\")\n",
    "    \n",
    "    print(\"\\n=== End of Diagnostics ===\\n\")\n",
    "    \n",
    "    \n",
    "    ordinal_cols = (\n",
    "    [f\"gp{i}\" for i in range(1, 8)] + [f\"gs{i}\" for i in range(1, 8)] +\n",
    "    [f\"ge{i}\" for i in range(1, 7)] + [f\"gf{i}\" for i in range(1, 8)] +\n",
    "    [f\"a_hn{i}\" for i in range(1, 6)] + [\"a_hn7\", \"a_hn10\"] +\n",
    "    [f\"a_e{i}\" for i in range(1, 8)] + [\"a_c6\", \"a_c2\", \"a_act11\"]\n",
    "    )\n",
    "    columns_to_impute = ordinal_cols#['ge1', 'ge2', 'ge3', 'ge4', 'ge5', 'ge6']\n",
    "\n",
    "    # Apply VAE imputation\n",
    "    imputed_df_vae, validation_results_vae  = apply_vae_imputation(df, columns_to_impute)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model - Denoising Autoencoder (DAE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SklearnDAE(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Denoising VAE implementation using scikit-learn's MLPRegressor\n",
    "    This avoids TensorFlow execution mode issues completely\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                    hidden_layer_sizes=(64, 32, 16, 32, 64),\n",
    "                    activation='relu',\n",
    "                    max_iter=200,\n",
    "                    learning_rate='adaptive',\n",
    "                    learning_rate_init=0.001,\n",
    "                    noise_factor=0.1,\n",
    "                    alpha=0.0001,\n",
    "                    verbose=True):\n",
    "        \"\"\"\n",
    "        Initialize DAE model parameters\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        hidden_layer_sizes : tuple\n",
    "            Size of hidden layers\n",
    "        activation : str\n",
    "            Activation function ('relu', 'tanh', 'logistic')\n",
    "        max_iter : int\n",
    "            Maximum number of iterations\n",
    "        learning_rate : str\n",
    "            Learning rate schedule\n",
    "        learning_rate_init : float\n",
    "            Initial learning rate\n",
    "        noise_factor : float\n",
    "            Amount of noise to add for denoising effect\n",
    "        alpha : float\n",
    "            L2 regularization parameter\n",
    "        verbose : bool\n",
    "            Whether to print progress\n",
    "        \"\"\"\n",
    "        self.hidden_layer_sizes = hidden_layer_sizes\n",
    "        self.activation = activation\n",
    "        self.max_iter = max_iter\n",
    "        self.learning_rate = learning_rate\n",
    "        self.learning_rate_init = learning_rate_init\n",
    "        self.noise_factor = noise_factor\n",
    "        self.alpha = alpha\n",
    "        self.verbose = verbose\n",
    "        self.model = None\n",
    "        self.scaler_X = StandardScaler()\n",
    "        \n",
    "    def fit(self, X, missing_mask=None):\n",
    "        \"\"\"\n",
    "        Fit the DAE model to input data\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X : numpy.ndarray or pandas.DataFrame\n",
    "            Input data with missing values (NaN)\n",
    "        missing_mask : numpy.ndarray\n",
    "            Binary mask where 1 indicates observed value, 0 for missing\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        self : object\n",
    "            Returns self\n",
    "        \"\"\"\n",
    "        # Convert to numpy array if DataFrame\n",
    "        is_df = isinstance(X, pd.DataFrame)\n",
    "        if is_df:\n",
    "            if self.verbose:\n",
    "                print(\"Converting DataFrame to numpy array\")\n",
    "            X_values = X.values\n",
    "        else:\n",
    "            X_values = X\n",
    "        \n",
    "        # Create missing mask if not provided\n",
    "        if missing_mask is None:\n",
    "            missing_mask = ~np.isnan(X_values)\n",
    "        \n",
    "        # Initial imputation (replace missing values with column means)\n",
    "        X_imputed = np.copy(X_values)\n",
    "        col_means = np.nanmean(X_values, axis=0)\n",
    "        for col in range(X_values.shape[1]):\n",
    "            mask = np.isnan(X_values[:, col])\n",
    "            X_imputed[mask, col] = col_means[col]\n",
    "        \n",
    "        # Scale the data\n",
    "        X_scaled = self.scaler_X.fit_transform(X_imputed)\n",
    "        \n",
    "        # Add noise to create denoising effect\n",
    "        X_noisy = X_scaled + np.random.normal(0, self.noise_factor, X_scaled.shape)\n",
    "        \n",
    "        # Apply mask to noise (only add noise to observed values)\n",
    "        X_noisy = X_noisy * missing_mask + X_scaled * (~missing_mask)\n",
    "        \n",
    "        # Create the model\n",
    "        self.model = MLPRegressor(\n",
    "            hidden_layer_sizes=self.hidden_layer_sizes,\n",
    "            activation=self.activation,\n",
    "            solver='adam',\n",
    "            alpha=self.alpha,\n",
    "            batch_size='auto',\n",
    "            learning_rate=self.learning_rate,\n",
    "            learning_rate_init=self.learning_rate_init,\n",
    "            max_iter=self.max_iter,\n",
    "            shuffle=True,\n",
    "            random_state=42,\n",
    "            verbose=self.verbose\n",
    "        )\n",
    "        \n",
    "        if self.verbose:\n",
    "            print(\"Fitting DAE model...\")\n",
    "        \n",
    "        # Train the model to reconstruct the original data\n",
    "        self.model.fit(X_noisy, X_scaled)\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, missing_mask=None):\n",
    "        \"\"\"\n",
    "        Impute missing values\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X : numpy.ndarray or pandas.DataFrame\n",
    "            Data with missing values (NaN)\n",
    "        missing_mask : numpy.ndarray\n",
    "            Binary mask where 1 indicates observed value, 0 for missing\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        X_imputed : numpy.ndarray or pandas.DataFrame\n",
    "            Data with imputed values\n",
    "        \"\"\"\n",
    "        # Handle DataFrame input\n",
    "        is_df = isinstance(X, pd.DataFrame)\n",
    "        if is_df:\n",
    "            columns = X.columns\n",
    "            index = X.index\n",
    "            X_values = X.values\n",
    "        else:\n",
    "            X_values = X\n",
    "        \n",
    "        # Create missing mask if not provided\n",
    "        if missing_mask is None:\n",
    "            missing_mask = ~np.isnan(X_values)\n",
    "        \n",
    "        # Initial imputation for model input\n",
    "        X_imputed = np.copy(X_values)\n",
    "        col_means = np.nanmean(X_values, axis=0)\n",
    "        for col in range(X_values.shape[1]):\n",
    "            mask = np.isnan(X_values[:, col])\n",
    "            X_imputed[mask, col] = col_means[col]\n",
    "        \n",
    "        # Scale data\n",
    "        X_scaled = self.scaler_X.transform(X_imputed)\n",
    "        \n",
    "        # Reconstruct data\n",
    "        X_reconstructed_scaled = self.model.predict(X_scaled)\n",
    "        \n",
    "        # Unscale data\n",
    "        X_reconstructed = self.scaler_X.inverse_transform(X_reconstructed_scaled)\n",
    "        \n",
    "        # Only replace missing values with reconstructed values\n",
    "        X_final = np.copy(X_values)\n",
    "        mask_missing = ~missing_mask\n",
    "        X_final[mask_missing] = X_reconstructed[mask_missing]\n",
    "        \n",
    "        # Return DataFrame if input was DataFrame\n",
    "        if is_df:\n",
    "            return pd.DataFrame(X_final, index=index, columns=columns)\n",
    "        \n",
    "        return X_final\n",
    "    \n",
    "    def fit_transform(self, X, missing_mask=None):\n",
    "        \"\"\"\n",
    "        Fit the model and impute missing values\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X : numpy.ndarray or pandas.DataFrame\n",
    "            Data with missing values (NaN)\n",
    "        missing_mask : numpy.ndarray\n",
    "            Binary mask where 1 indicates observed value, 0 for missing\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        X_imputed : numpy.ndarray or pandas.DataFrame\n",
    "            Data with imputed values\n",
    "        \"\"\"\n",
    "        return self.fit(X, missing_mask).transform(X, missing_mask)\n",
    "\n",
    "\n",
    "def apply_sklearn_dae_imputation(df, columns_to_impute, validation_df=None, validation_masks=None, original_values=None):\n",
    "    \"\"\"\n",
    "    Apply scikit-learn based DAE imputation to data using feature importance selection\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pandas.DataFrame\n",
    "        Data with missing values\n",
    "    columns_to_impute : list\n",
    "        List of column names to impute\n",
    "    validation_df : pandas.DataFrame, optional\n",
    "        Validation data with artificially missing values\n",
    "    validation_masks : dict, optional\n",
    "        Dictionary of masks for validation\n",
    "    original_values : dict, optional\n",
    "        Dictionary of original values for validation\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    imputed_df : pandas.DataFrame\n",
    "        Data with imputed values\n",
    "    validation_results : dict, optional\n",
    "        Validation results if validation data provided\n",
    "    \"\"\"\n",
    "    try:\n",
    "\n",
    "        # Create a copy of the dataframe to avoid modifying the original\n",
    "        work_df = df.copy()\n",
    "        \n",
    "        # Ensure all columns are numeric\n",
    "        for col in work_df.columns:\n",
    "            work_df[col] = pd.to_numeric(work_df[col], errors='coerce')\n",
    "        \n",
    "        print(f\"Original data shape: {work_df.shape}\")\n",
    "        print(f\"Missing values: {work_df.isna().sum().sum()}\")\n",
    "        print(f\"Columns to impute: {columns_to_impute}\")\n",
    "        \n",
    "        # Step 1: Identify important features for each target column\n",
    "        feature_importances = {}\n",
    "        max_features_per_target = 20  # Maximum number of features to select per target\n",
    "        min_importance_threshold = 0.01  # Minimum importance score to include a feature\n",
    "        \n",
    "        print(\"Identifying important features for each target column...\")\n",
    "        \n",
    "        # For each target column, find the most important predictors\n",
    "        for target in columns_to_impute:\n",
    "            # Create temporary dataset without missing values in the target\n",
    "            temp_df = work_df.dropna(subset=[target]).copy()\n",
    "            \n",
    "            # Skip if there are too few samples\n",
    "            if len(temp_df) < 30:\n",
    "                print(f\"  Skipping feature selection for {target}: insufficient samples\")\n",
    "                # Use all columns except other targets as predictors\n",
    "                non_target_cols = [col for col in work_df.columns if col not in columns_to_impute or col == target]\n",
    "                feature_importances[target] = non_target_cols\n",
    "                continue\n",
    "            \n",
    "            # Separate predictors and target\n",
    "            other_targets = [col for col in columns_to_impute if col != target]\n",
    "            X_temp = temp_df.drop(columns=other_targets)\n",
    "            potential_predictors = [col for col in X_temp.columns if col != target]\n",
    "            \n",
    "            # Skip if there are no potential predictors\n",
    "            if len(potential_predictors) == 0:\n",
    "                print(f\"  No potential predictors for {target}\")\n",
    "                feature_importances[target] = [target]  # Use only the target itself\n",
    "                continue\n",
    "            \n",
    "            # Use potential predictors that have fewer than 30% missing values\n",
    "            valid_predictors = []\n",
    "            for col in potential_predictors:\n",
    "                if X_temp[col].isna().mean() < 0.3 and col != target:\n",
    "                    valid_predictors.append(col)\n",
    "            \n",
    "            # Skip feature selection if there are too few valid predictors\n",
    "            if len(valid_predictors) <= 3:\n",
    "                print(f\"  Too few valid predictors for {target}, using all available\")\n",
    "                feature_importances[target] = valid_predictors + [target]\n",
    "                continue\n",
    "            \n",
    "            # Create dataset for feature selection\n",
    "            X_select = X_temp[valid_predictors].copy()\n",
    "            y_select = X_temp[target].copy()\n",
    "            \n",
    "            # Fill any remaining NaNs in predictors with mean\n",
    "            for col in X_select.columns:\n",
    "                if X_select[col].isna().any():\n",
    "                    X_select[col] = X_select[col].fillna(X_select[col].mean())\n",
    "            \n",
    "            # Train random forest to identify important features\n",
    "            print(f\"  Training feature selector for {target} using {len(X_select)} samples and {len(valid_predictors)} predictors\")\n",
    "            rf = RandomForestRegressor(n_estimators=50, max_depth=10, random_state=42)\n",
    "            rf.fit(X_select, y_select)\n",
    "            \n",
    "            # Get important features\n",
    "            importances = rf.feature_importances_\n",
    "            features_with_importance = list(zip(valid_predictors, importances))\n",
    "            \n",
    "            # Sort by importance\n",
    "            features_with_importance.sort(key=lambda x: x[1], reverse=True)\n",
    "            \n",
    "            # Select top features with importance above threshold\n",
    "            top_features = []\n",
    "            for feat, imp in features_with_importance:\n",
    "                if imp >= min_importance_threshold and len(top_features) < max_features_per_target:\n",
    "                    top_features.append(feat)\n",
    "            \n",
    "            # Always include the target itself\n",
    "            if target not in top_features:\n",
    "                top_features.append(target)\n",
    "            \n",
    "            print(f\"  Selected {len(top_features)} features for {target}\")\n",
    "            feature_importances[target] = top_features\n",
    "        \n",
    "        # Step 2: Combine all important features\n",
    "        selected_columns = set()\n",
    "        for target, features in feature_importances.items():\n",
    "            selected_columns.update(features)\n",
    "        \n",
    "        # Ensure all target columns are included\n",
    "        for col in columns_to_impute:\n",
    "            selected_columns.add(col)\n",
    "        \n",
    "        selected_columns = list(selected_columns)\n",
    "        print(f\"Using {len(selected_columns)} columns for imputation: {selected_columns}\")\n",
    "        \n",
    "        # Step 3: Extract data with selected columns\n",
    "        X = work_df[selected_columns].copy()\n",
    "        \n",
    "        # Create missing mask\n",
    "        missing_mask = ~X.isna()\n",
    "        \n",
    "        # Step 4: Initialize DAE model with adaptive parameters based on data size\n",
    "        neurons_per_layer = min(128, max(16, X.shape[0] // 100))\n",
    "        print(f\"Using {neurons_per_layer} neurons per layer\")\n",
    "        \n",
    "        # Initialize the SklearnDAE model\n",
    "        dae = SklearnDAE(\n",
    "            hidden_layer_sizes=(neurons_per_layer, neurons_per_layer//2, neurons_per_layer//4, neurons_per_layer//2, neurons_per_layer),\n",
    "            activation='relu',\n",
    "            max_iter=200,\n",
    "            learning_rate='adaptive',\n",
    "            learning_rate_init=0.001,\n",
    "            noise_factor=0.1,\n",
    "            alpha=0.0001,\n",
    "            verbose=True\n",
    "        )\n",
    "        \n",
    "        # Step 5: Fit and transform using selected features\n",
    "        X_imputed = dae.fit_transform(X, missing_mask.values)\n",
    "        \n",
    "        # Step 6: Create imputed dataframe, only updating the target columns\n",
    "        imputed_df = df.copy()\n",
    "        imputed_df[columns_to_impute] = X_imputed[columns_to_impute]\n",
    "        \n",
    "        # Step 7: Validate if required\n",
    "        validation_results = None\n",
    "        if validation_df is not None and validation_masks is not None and original_values is not None:\n",
    "            validation_results = {}\n",
    "            \n",
    "            # Extract validation data\n",
    "            X_val = validation_df[selected_columns].copy()\n",
    "            \n",
    "            # Ensure numeric\n",
    "            for col in X_val.columns:\n",
    "                X_val[col] = pd.to_numeric(X_val[col], errors='coerce')\n",
    "                \n",
    "            # Create validation mask\n",
    "            val_mask = ~X_val.isna()\n",
    "            \n",
    "            # Impute validation data\n",
    "            X_val_imputed = dae.transform(X_val, val_mask.values)\n",
    "            \n",
    "            # Compare imputed values to real values\n",
    "            for col in columns_to_impute:\n",
    "                mask = validation_masks[col] & validation_df[col].isna()\n",
    "                \n",
    "                if mask.sum() == 0:\n",
    "                    validation_results[col] = {'error': \"No artificially missing values\"}\n",
    "                    continue\n",
    "                    \n",
    "                real_vals = original_values[col][mask]\n",
    "                imputed_vals = X_val_imputed[mask][col]\n",
    "                \n",
    "                # Calculate continuous metrics (MAE and RMSE) - NO ROUNDING\n",
    "                mae = mean_absolute_error(real_vals, imputed_vals)\n",
    "                rmse = np.sqrt(mean_squared_error(real_vals, imputed_vals))\n",
    "                \n",
    "                # Calculate classification metrics - WITH ROUNDING\n",
    "                real_vals_class = process_for_classification(real_vals)\n",
    "                imputed_vals_class = process_for_classification(imputed_vals)\n",
    "                \n",
    "                classification_metrics = calculate_classification_metrics(real_vals_class, imputed_vals_class)\n",
    "                \n",
    "                validation_results[col] = {\n",
    "                    'mae': mae,\n",
    "                    'rmse': rmse,\n",
    "                    'accuracy': classification_metrics['accuracy'],\n",
    "                    'auc_multiclass': classification_metrics['auc_multiclass'],\n",
    "                    'avg_sensitivity': classification_metrics['avg_sensitivity'],\n",
    "                    'avg_specificity': classification_metrics['avg_specificity'],\n",
    "                    'avg_ppv': classification_metrics['avg_ppv'],\n",
    "                    'avg_npv': classification_metrics['avg_npv'],\n",
    "                    'precision_macro': classification_metrics['precision_macro'],\n",
    "                    'recall_macro': classification_metrics['recall_macro'],\n",
    "                    'real_distribution': real_vals.describe(),\n",
    "                    'imputed_distribution': imputed_vals.describe()\n",
    "                }\n",
    "                \n",
    "                print(f\"Validation for {col}: MAE={mae:.4f}, RMSE={rmse:.4f}, Acc={classification_metrics['accuracy']:.4f}\")\n",
    "        \n",
    "        return imputed_df, validation_results\n",
    "        \n",
    "    except Exception as e:\n",
    "        import traceback\n",
    "        print(f\"Error in scikit-learn DAE imputation: {str(e)}\")\n",
    "        print(traceback.format_exc())\n",
    "        \n",
    "        # Fallback to simple imputation\n",
    "        print(\"Falling back to simple mean imputation\")\n",
    "        result_df = df.copy()\n",
    "        for col in columns_to_impute:\n",
    "            result_df[col] = pd.to_numeric(result_df[col], errors='coerce')\n",
    "            result_df[col] = result_df[col].fillna(result_df[col].mean())\n",
    "        return result_df, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original data shape: (5230, 62)\n",
      "Missing values: 262139\n",
      "Columns to impute: ['gp1', 'gp2', 'gp3', 'gp4', 'gp5', 'gp6', 'gp7', 'gs1', 'gs2', 'gs3', 'gs4', 'gs5', 'gs6', 'gs7', 'ge1', 'ge2', 'ge3', 'ge4', 'ge5', 'ge6', 'gf1', 'gf2', 'gf3', 'gf4', 'gf5', 'gf6', 'gf7', 'a_hn1', 'a_hn2', 'a_hn3', 'a_hn4', 'a_hn5', 'a_hn7', 'a_hn10', 'a_e1', 'a_e2', 'a_e3', 'a_e4', 'a_e5', 'a_e6', 'a_e7', 'a_c6', 'a_c2', 'a_act11']\n",
      "Identifying important features for each target column...\n",
      "  Too few valid predictors for gp1, using all available\n",
      "  Too few valid predictors for gp2, using all available\n",
      "  Too few valid predictors for gp3, using all available\n",
      "  Too few valid predictors for gp4, using all available\n",
      "  Too few valid predictors for gp5, using all available\n",
      "  Too few valid predictors for gp6, using all available\n",
      "  Too few valid predictors for gp7, using all available\n",
      "  Too few valid predictors for gs1, using all available\n",
      "  Too few valid predictors for gs2, using all available\n",
      "  Too few valid predictors for gs3, using all available\n",
      "  Too few valid predictors for gs4, using all available\n",
      "  Too few valid predictors for gs5, using all available\n",
      "  Too few valid predictors for gs6, using all available\n",
      "  Too few valid predictors for gs7, using all available\n",
      "  Too few valid predictors for ge1, using all available\n",
      "  Too few valid predictors for ge2, using all available\n",
      "  Too few valid predictors for ge3, using all available\n",
      "  Too few valid predictors for ge4, using all available\n",
      "  Too few valid predictors for ge5, using all available\n",
      "  Too few valid predictors for ge6, using all available\n",
      "  Too few valid predictors for gf1, using all available\n",
      "  Too few valid predictors for gf2, using all available\n",
      "  Too few valid predictors for gf3, using all available\n",
      "  Too few valid predictors for gf4, using all available\n",
      "  Too few valid predictors for gf5, using all available\n",
      "  Too few valid predictors for gf6, using all available\n",
      "  Too few valid predictors for gf7, using all available\n",
      "  Too few valid predictors for a_hn1, using all available\n",
      "  Too few valid predictors for a_hn2, using all available\n",
      "  Too few valid predictors for a_hn3, using all available\n",
      "  Too few valid predictors for a_hn4, using all available\n",
      "  Too few valid predictors for a_hn5, using all available\n",
      "  Too few valid predictors for a_hn7, using all available\n",
      "  Too few valid predictors for a_hn10, using all available\n",
      "  Too few valid predictors for a_e1, using all available\n",
      "  Too few valid predictors for a_e2, using all available\n",
      "  Too few valid predictors for a_e3, using all available\n",
      "  Too few valid predictors for a_e4, using all available\n",
      "  Too few valid predictors for a_e5, using all available\n",
      "  Too few valid predictors for a_e6, using all available\n",
      "  Too few valid predictors for a_e7, using all available\n",
      "  Too few valid predictors for a_c6, using all available\n",
      "  Too few valid predictors for a_c2, using all available\n",
      "  Too few valid predictors for a_act11, using all available\n",
      "Using 46 columns for imputation: ['a_e3', 'ge4', 'id', 'gp7', 'gf1', 'gf4', 'gp1', 'gp5', 'gf6', 'qol_date', 'a_c2', 'a_hn3', 'a_hn1', 'gs1', 'ge1', 'a_hn10', 'a_hn4', 'a_e7', 'gf5', 'gs6', 'gp6', 'a_hn2', 'a_e4', 'a_e5', 'ge3', 'gp3', 'a_e1', 'gs5', 'ge6', 'a_hn7', 'gp4', 'gp2', 'gs4', 'ge2', 'gs2', 'gf7', 'a_act11', 'gs7', 'gf2', 'a_c6', 'ge5', 'gf3', 'a_hn5', 'a_e6', 'gs3', 'a_e2']\n",
      "Using 52 neurons per layer\n",
      "Converting DataFrame to numpy array\n",
      "Fitting DAE model...\n",
      "Iteration 1, loss = 0.50782280\n",
      "Iteration 2, loss = 0.47015885\n",
      "Iteration 3, loss = 0.39437942\n",
      "Iteration 4, loss = 0.35679191\n",
      "Iteration 5, loss = 0.33646489\n",
      "Iteration 6, loss = 0.31753749\n",
      "Iteration 7, loss = 0.29800819\n",
      "Iteration 8, loss = 0.28433592\n",
      "Iteration 9, loss = 0.27510111\n",
      "Iteration 10, loss = 0.26860913\n",
      "Iteration 11, loss = 0.26148130\n",
      "Iteration 12, loss = 0.25507719\n",
      "Iteration 13, loss = 0.24961055\n",
      "Iteration 14, loss = 0.24396962\n",
      "Iteration 15, loss = 0.23887388\n",
      "Iteration 16, loss = 0.23556404\n",
      "Iteration 17, loss = 0.23192369\n",
      "Iteration 18, loss = 0.22948431\n",
      "Iteration 19, loss = 0.22823471\n",
      "Iteration 20, loss = 0.22473925\n",
      "Iteration 21, loss = 0.22343018\n",
      "Iteration 22, loss = 0.22145648\n",
      "Iteration 23, loss = 0.21947461\n",
      "Iteration 24, loss = 0.21805300\n",
      "Iteration 25, loss = 0.21726641\n",
      "Iteration 26, loss = 0.21613507\n",
      "Iteration 27, loss = 0.21492532\n",
      "Iteration 28, loss = 0.21428446\n",
      "Iteration 29, loss = 0.21282041\n",
      "Iteration 30, loss = 0.21242802\n",
      "Iteration 31, loss = 0.21139965\n",
      "Iteration 32, loss = 0.21030130\n",
      "Iteration 33, loss = 0.20957423\n",
      "Iteration 34, loss = 0.20872124\n",
      "Iteration 35, loss = 0.20820916\n",
      "Iteration 36, loss = 0.20770833\n",
      "Iteration 37, loss = 0.20680534\n",
      "Iteration 38, loss = 0.20615974\n",
      "Iteration 39, loss = 0.20589835\n",
      "Iteration 40, loss = 0.20571186\n",
      "Iteration 41, loss = 0.20509408\n",
      "Iteration 42, loss = 0.20440541\n",
      "Iteration 43, loss = 0.20470355\n",
      "Iteration 44, loss = 0.20319072\n",
      "Iteration 45, loss = 0.20293033\n",
      "Iteration 46, loss = 0.20319132\n",
      "Iteration 47, loss = 0.20179230\n",
      "Iteration 48, loss = 0.20133529\n",
      "Iteration 49, loss = 0.20147342\n",
      "Iteration 50, loss = 0.20161515\n",
      "Iteration 51, loss = 0.20102810\n",
      "Iteration 52, loss = 0.20128134\n",
      "Iteration 53, loss = 0.20043442\n",
      "Iteration 54, loss = 0.19931936\n",
      "Iteration 55, loss = 0.19942261\n",
      "Iteration 56, loss = 0.19966390\n",
      "Iteration 57, loss = 0.19843165\n",
      "Iteration 58, loss = 0.19815129\n",
      "Iteration 59, loss = 0.19805797\n",
      "Iteration 60, loss = 0.19730602\n",
      "Iteration 61, loss = 0.19763908\n",
      "Iteration 62, loss = 0.19720231\n",
      "Iteration 63, loss = 0.19696258\n",
      "Iteration 64, loss = 0.19628757\n",
      "Iteration 65, loss = 0.19596634\n",
      "Iteration 66, loss = 0.19536254\n",
      "Iteration 67, loss = 0.19552005\n",
      "Iteration 68, loss = 0.19524633\n",
      "Iteration 69, loss = 0.19508406\n",
      "Iteration 70, loss = 0.19459586\n",
      "Iteration 71, loss = 0.19407170\n",
      "Iteration 72, loss = 0.19349965\n",
      "Iteration 73, loss = 0.19410531\n",
      "Iteration 74, loss = 0.19360261\n",
      "Iteration 75, loss = 0.19393916\n",
      "Iteration 76, loss = 0.19320561\n",
      "Iteration 77, loss = 0.19267345\n",
      "Iteration 78, loss = 0.19234145\n",
      "Iteration 79, loss = 0.19229318\n",
      "Iteration 80, loss = 0.19183554\n",
      "Iteration 81, loss = 0.19144815\n",
      "Iteration 82, loss = 0.19118570\n",
      "Iteration 83, loss = 0.19186641\n",
      "Iteration 84, loss = 0.19081794\n",
      "Iteration 85, loss = 0.19081109\n",
      "Iteration 86, loss = 0.19092946\n",
      "Iteration 87, loss = 0.19033183\n",
      "Iteration 88, loss = 0.19008743\n",
      "Iteration 89, loss = 0.19005506\n",
      "Iteration 90, loss = 0.19066690\n",
      "Iteration 91, loss = 0.18953697\n",
      "Iteration 92, loss = 0.18910409\n",
      "Iteration 93, loss = 0.18861002\n",
      "Iteration 94, loss = 0.18884064\n",
      "Iteration 95, loss = 0.18888235\n",
      "Iteration 96, loss = 0.18849890\n",
      "Iteration 97, loss = 0.18755316\n",
      "Iteration 98, loss = 0.18814868\n",
      "Iteration 99, loss = 0.18771008\n",
      "Iteration 100, loss = 0.18776529\n",
      "Iteration 101, loss = 0.18674658\n",
      "Iteration 102, loss = 0.18755489\n",
      "Iteration 103, loss = 0.18672763\n",
      "Iteration 104, loss = 0.18648693\n",
      "Iteration 105, loss = 0.18708473\n",
      "Iteration 106, loss = 0.18754091\n",
      "Iteration 107, loss = 0.18653648\n",
      "Iteration 108, loss = 0.18540991\n",
      "Iteration 109, loss = 0.18560525\n",
      "Iteration 110, loss = 0.18543616\n",
      "Iteration 111, loss = 0.18531726\n",
      "Iteration 112, loss = 0.18519739\n",
      "Iteration 113, loss = 0.18565270\n",
      "Iteration 114, loss = 0.18513945\n",
      "Iteration 115, loss = 0.18498140\n",
      "Iteration 116, loss = 0.18410834\n",
      "Iteration 117, loss = 0.18421957\n",
      "Iteration 118, loss = 0.18430685\n",
      "Iteration 119, loss = 0.18392705\n",
      "Iteration 120, loss = 0.18369344\n",
      "Iteration 121, loss = 0.18366417\n",
      "Iteration 122, loss = 0.18360544\n",
      "Iteration 123, loss = 0.18361711\n",
      "Iteration 124, loss = 0.18365444\n",
      "Iteration 125, loss = 0.18376333\n",
      "Iteration 126, loss = 0.18278636\n",
      "Iteration 127, loss = 0.18264358\n",
      "Iteration 128, loss = 0.18222174\n",
      "Iteration 129, loss = 0.18274194\n",
      "Iteration 130, loss = 0.18250077\n",
      "Iteration 131, loss = 0.18201503\n",
      "Iteration 132, loss = 0.18190730\n",
      "Iteration 133, loss = 0.18179985\n",
      "Iteration 134, loss = 0.18155828\n",
      "Iteration 135, loss = 0.18136444\n",
      "Iteration 136, loss = 0.18156444\n",
      "Iteration 137, loss = 0.18090758\n",
      "Iteration 138, loss = 0.18151150\n",
      "Iteration 139, loss = 0.18069117\n",
      "Iteration 140, loss = 0.18062704\n",
      "Iteration 141, loss = 0.18038492\n",
      "Iteration 142, loss = 0.18036700\n",
      "Iteration 143, loss = 0.18006341\n",
      "Iteration 144, loss = 0.17974014\n",
      "Iteration 145, loss = 0.17987217\n",
      "Iteration 146, loss = 0.17989148\n",
      "Iteration 147, loss = 0.17947709\n",
      "Iteration 148, loss = 0.17940311\n",
      "Iteration 149, loss = 0.17955049\n",
      "Iteration 150, loss = 0.17984939\n",
      "Iteration 151, loss = 0.17994156\n",
      "Iteration 152, loss = 0.17882487\n",
      "Iteration 153, loss = 0.17901862\n",
      "Iteration 154, loss = 0.17872311\n",
      "Iteration 155, loss = 0.17862198\n",
      "Iteration 156, loss = 0.17825090\n",
      "Iteration 157, loss = 0.17798552\n",
      "Iteration 158, loss = 0.17862268\n",
      "Iteration 159, loss = 0.17811152\n",
      "Iteration 160, loss = 0.17792122\n",
      "Iteration 161, loss = 0.17756347\n",
      "Iteration 162, loss = 0.17692092\n",
      "Iteration 163, loss = 0.17685515\n",
      "Iteration 164, loss = 0.17684100\n",
      "Iteration 165, loss = 0.17670221\n",
      "Iteration 166, loss = 0.17684784\n",
      "Iteration 167, loss = 0.17684288\n",
      "Iteration 168, loss = 0.17684225\n",
      "Iteration 169, loss = 0.17698305\n",
      "Iteration 170, loss = 0.17694049\n",
      "Iteration 171, loss = 0.17660500\n",
      "Iteration 172, loss = 0.17670000\n",
      "Iteration 173, loss = 0.17572052\n",
      "Iteration 174, loss = 0.17639467\n",
      "Iteration 175, loss = 0.17681586\n",
      "Iteration 176, loss = 0.17624728\n",
      "Iteration 177, loss = 0.17634616\n",
      "Iteration 178, loss = 0.17577219\n",
      "Iteration 179, loss = 0.17577458\n",
      "Iteration 180, loss = 0.17528675\n",
      "Iteration 181, loss = 0.17553262\n",
      "Iteration 182, loss = 0.17506253\n",
      "Iteration 183, loss = 0.17548828\n",
      "Iteration 184, loss = 0.17594336\n",
      "Iteration 185, loss = 0.17522699\n",
      "Iteration 186, loss = 0.17461425\n",
      "Iteration 187, loss = 0.17583173\n",
      "Iteration 188, loss = 0.17482810\n",
      "Iteration 189, loss = 0.17499260\n",
      "Iteration 190, loss = 0.17471667\n",
      "Iteration 191, loss = 0.17586224\n",
      "Iteration 192, loss = 0.17451067\n",
      "Iteration 193, loss = 0.17506805\n",
      "Iteration 194, loss = 0.17461275\n",
      "Iteration 195, loss = 0.17353281\n",
      "Iteration 196, loss = 0.17361973\n",
      "Iteration 197, loss = 0.17373923\n",
      "Iteration 198, loss = 0.17346192\n",
      "Iteration 199, loss = 0.17360360\n",
      "Iteration 200, loss = 0.17281277\n"
     ]
    }
   ],
   "source": [
    "# Define columns to impute\n",
    "ordinal_cols = (\n",
    "    [f\"gp{i}\" for i in range(1, 8)] + [f\"gs{i}\" for i in range(1, 8)] +\n",
    "    [f\"ge{i}\" for i in range(1, 7)] + [f\"gf{i}\" for i in range(1, 8)] +\n",
    "    [f\"a_hn{i}\" for i in range(1, 6)] + [\"a_hn7\", \"a_hn10\"] +\n",
    "    [f\"a_e{i}\" for i in range(1, 8)] + [\"a_c6\", \"a_c2\", \"a_act11\"]\n",
    ")\n",
    "columns_to_impute = ordinal_cols#['ge1', 'ge2', 'ge3', 'ge4', 'ge5', 'ge6']\n",
    "\n",
    "\n",
    "# Apply DAE imputation\n",
    "imputed_df_dae, validation_results_dae = apply_sklearn_dae_imputation(df, columns_to_impute)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_results_dae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model - Bayesian PCA "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.decomposition import PCA\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.distributions as dist\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import time\n",
    "\n",
    "\n",
    "class BayesianPCATorch(nn.Module):\n",
    "    \"\"\"\n",
    "    PyTorch implementation of Bayesian PCA model\n",
    "    \"\"\"\n",
    "    def __init__(self, n_features, n_components):\n",
    "        super(BayesianPCATorch, self).__init__()\n",
    "        self.n_features = n_features\n",
    "        self.n_components = n_components\n",
    "        \n",
    "        # Priors for loadings (W)\n",
    "        self.w_mu = nn.Parameter(torch.zeros(n_features, n_components), requires_grad=True)\n",
    "        self.w_log_sigma = nn.Parameter(torch.zeros(n_features, n_components), requires_grad=True)\n",
    "        \n",
    "        # Noise precision (inverse variance)\n",
    "        self.log_tau = nn.Parameter(torch.zeros(1), requires_grad=True)\n",
    "        \n",
    "    def forward(self, z):\n",
    "        # Sample from approximate posterior for W\n",
    "        w_sigma = torch.exp(self.w_log_sigma)\n",
    "        epsilon_w = torch.randn_like(self.w_mu)\n",
    "        w = self.w_mu + w_sigma * epsilon_w\n",
    "        \n",
    "        # Compute reconstruction\n",
    "        tau = torch.exp(self.log_tau)\n",
    "        reconstruction = torch.matmul(z, w.t())\n",
    "        \n",
    "        return reconstruction, w, tau\n",
    "    \n",
    "    def sample_loadings(self, n_samples=1):\n",
    "        \"\"\"Sample loadings (W) from the approximate posterior\"\"\"\n",
    "        w_sigma = torch.exp(self.w_log_sigma)\n",
    "        epsilon_w = torch.randn(n_samples, self.n_features, self.n_components, device=self.w_mu.device)\n",
    "        w_samples = self.w_mu.unsqueeze(0) + w_sigma.unsqueeze(0) * epsilon_w\n",
    "        return w_samples\n",
    "\n",
    "\n",
    "class BayesianPCAImputer(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    A scikit-learn compatible Bayesian PCA imputation model\n",
    "    using PyTorch for GPU acceleration\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self,\n",
    "                 n_components=5,\n",
    "                 n_samples=1000,\n",
    "                 batch_size=64,\n",
    "                 n_epochs=100,\n",
    "                 learning_rate=0.01,\n",
    "                 device=None,\n",
    "                 verbose=True):\n",
    "        \"\"\"\n",
    "        Initialize PyTorch-based Bayesian PCA imputer\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        n_components : int\n",
    "            Number of principal components\n",
    "        n_samples : int\n",
    "            Number of posterior samples to draw\n",
    "        batch_size : int\n",
    "            Batch size for training\n",
    "        n_epochs : int\n",
    "            Number of training epochs\n",
    "        learning_rate : float\n",
    "            Learning rate for optimizer\n",
    "        device : str or torch.device\n",
    "            Device to use ('cuda' or 'cpu'), defaults to CUDA if available\n",
    "        verbose : bool\n",
    "            Whether to print progress\n",
    "        \"\"\"\n",
    "        self.n_components = n_components\n",
    "        self.n_samples = n_samples\n",
    "        self.batch_size = batch_size\n",
    "        self.n_epochs = n_epochs\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "        # Set device\n",
    "        if device is None:\n",
    "            self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        else:\n",
    "            self.device = torch.device(device)\n",
    "            \n",
    "        self.verbose = verbose\n",
    "        \n",
    "        # Initialize model components\n",
    "        self.pca_model = None  # For standard PCA initialization\n",
    "        self.scaler = None     # For data scaling\n",
    "        self.model = None      # The PyTorch model\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        \"\"\"\n",
    "        Fit Bayesian PCA model using PyTorch\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X : pandas.DataFrame\n",
    "            Data with missing values\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        self : object\n",
    "            Returns self\n",
    "        \"\"\"\n",
    "        # Store column names and index\n",
    "        self.columns = X.columns\n",
    "        self.index = X.index\n",
    "        \n",
    "        # Create a copy of data\n",
    "        X_data = X.copy()\n",
    "        \n",
    "        # Convert to numpy array\n",
    "        X_array = X_data.values\n",
    "        \n",
    "        # Initialize scaler\n",
    "        self.scaler = StandardScaler()\n",
    "        \n",
    "        # Simple imputation for initial values (for fitting the scaler)\n",
    "        simple_imputer = SimpleImputer(strategy='mean')\n",
    "        X_imputed_for_scaling = simple_imputer.fit_transform(X_array)\n",
    "        \n",
    "        # Scale data\n",
    "        X_scaled = self.scaler.fit_transform(X_imputed_for_scaling)\n",
    "        \n",
    "        # Create a mask for missing values\n",
    "        self.missing_mask = np.isnan(X_array)\n",
    "        \n",
    "        # Perform standard PCA for initialization\n",
    "        pca = PCA(n_components=self.n_components)\n",
    "        pca.fit(X_scaled)\n",
    "        self.pca_model = pca\n",
    "        \n",
    "        # Initialize PyTorch model\n",
    "        n_samples, n_features = X_scaled.shape\n",
    "        self.model = BayesianPCATorch(n_features, self.n_components).to(self.device)\n",
    "        \n",
    "        # Initialize model parameters using standard PCA\n",
    "        with torch.no_grad():\n",
    "            # Initialize W to PCA loadings\n",
    "            self.model.w_mu.data = torch.tensor(pca.components_.T, dtype=torch.float32).to(self.device)\n",
    "            \n",
    "            # Initialize noise precision (tau) based on explained variance\n",
    "            explained_var = pca.explained_variance_\n",
    "            noise_var = np.mean(np.var(X_scaled, axis=0) - np.sum(explained_var))\n",
    "            noise_var = max(noise_var, 1e-6)  # Ensure positive variance\n",
    "            self.model.log_tau.data = torch.tensor([np.log(1.0 / noise_var)], dtype=torch.float32).to(self.device)\n",
    "        \n",
    "        # Prepare data for PyTorch training\n",
    "        # Fill missing values with zeros (will be handled by the mask)\n",
    "        X_for_torch = X_scaled.copy()\n",
    "        X_for_torch[self.missing_mask] = 0.0\n",
    "        \n",
    "        # Convert to PyTorch tensors\n",
    "        X_tensor = torch.tensor(X_for_torch, dtype=torch.float32).to(self.device)\n",
    "        mask_tensor = torch.tensor(~self.missing_mask, dtype=torch.float32).to(self.device)\n",
    "        \n",
    "        # Initialize latent variables (z) using PCA scores\n",
    "        z_init = pca.transform(X_scaled)\n",
    "        \n",
    "        # Store the latent variables as a model parameter\n",
    "        self.latent_z = nn.Parameter(torch.tensor(z_init, dtype=torch.float32, device=self.device))\n",
    "        \n",
    "        # Train the model\n",
    "        self._train_model(X_tensor, mask_tensor)\n",
    "        \n",
    "        # Store final latent variables for later use\n",
    "        self.z = self.latent_z.detach().cpu().numpy()\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def _train_model(self, X, mask):\n",
    "        \"\"\"\n",
    "        Train the Bayesian PCA model using stochastic variational inference\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X : torch.Tensor\n",
    "            Data tensor\n",
    "        mask : torch.Tensor\n",
    "            Mask tensor (1 for observed, 0 for missing)\n",
    "        \"\"\"\n",
    "        # Setup optimizer - include latent_z as a parameter of self\n",
    "        parameters = list(self.model.parameters()) + [self.latent_z]\n",
    "        optimizer = optim.Adam(parameters, lr=self.learning_rate)\n",
    "        \n",
    "        # Setup scheduler for learning rate decay\n",
    "        scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "            optimizer, mode='min', factor=0.5, patience=5, verbose=self.verbose\n",
    "        )\n",
    "        \n",
    "        # Training loop\n",
    "        if self.verbose:\n",
    "            print(f\"Training Bayesian PCA model on {self.device}...\")\n",
    "            print(f\"Data shape: {X.shape}, Components: {self.n_components}\")\n",
    "            print(f\"Missing values: {torch.sum(mask == 0).item()} out of {X.numel()}\")\n",
    "            \n",
    "        epoch_pbar = range(self.n_epochs)\n",
    "        if self.verbose:\n",
    "            epoch_pbar = tqdm(epoch_pbar, desc=\"Training\")\n",
    "            \n",
    "        # For early stopping\n",
    "        best_loss = float('inf')\n",
    "        patience = 60  # Increased patience to 60\n",
    "        patience_counter = 0\n",
    "        \n",
    "        # Store z for later use\n",
    "        for epoch in epoch_pbar:\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass\n",
    "            x_recon, w_samples, tau = self.model(self.latent_z)\n",
    "            \n",
    "            # Compute loss - only for observed values\n",
    "            # Likelihood term (reconstruction error)\n",
    "            mse_loss = torch.sum(mask * (X - x_recon) ** 2)\n",
    "            \n",
    "            # Prior on z (standard normal)\n",
    "            z_prior_loss = 0.5 * torch.sum(self.latent_z ** 2)\n",
    "            \n",
    "            # Prior on W (standard normal)\n",
    "            w_mu = self.model.w_mu\n",
    "            w_sigma = torch.exp(self.model.w_log_sigma)\n",
    "            w_prior_loss = 0.5 * torch.sum(w_mu ** 2 + w_sigma ** 2 - torch.log(w_sigma ** 2) - 1)\n",
    "            \n",
    "            # Total loss\n",
    "            loss = mse_loss * tau + z_prior_loss + w_prior_loss\n",
    "            \n",
    "            # Backward pass and optimization step\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Get current loss\n",
    "            current_loss = loss.item()\n",
    "            \n",
    "            # Update learning rate\n",
    "            scheduler.step(current_loss)\n",
    "            \n",
    "            # Update progress bar with current loss\n",
    "            if self.verbose:\n",
    "                epoch_pbar.set_postfix({\"Loss\": f\"{current_loss:.4f}\"})\n",
    "            \n",
    "            # Early stopping\n",
    "            if current_loss < best_loss:\n",
    "                best_loss = current_loss\n",
    "                patience_counter = 0\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "                \n",
    "            if patience_counter >= patience:\n",
    "                if self.verbose:\n",
    "                    print(f\"Early stopping at epoch {epoch + 1}\")\n",
    "                break\n",
    "                \n",
    "    def transform(self, X):\n",
    "        \"\"\"\n",
    "        Impute missing values using Bayesian PCA\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X : pandas.DataFrame\n",
    "            Data with missing values\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        imputed_df : pandas.DataFrame\n",
    "            Data with imputed values\n",
    "        \"\"\"\n",
    "        # Create copy to avoid modifying original\n",
    "        imputed_df = X.copy()\n",
    "        \n",
    "        # Check if this is the same data used for fitting\n",
    "        new_data = not np.array_equal(X.index, self.index) or not np.array_equal(X.columns, self.columns)\n",
    "        \n",
    "        if new_data:\n",
    "            # For new data, we need to project onto the learned components\n",
    "            # This is a simplified approach and could be improved\n",
    "            \n",
    "            # Create a copy of data\n",
    "            X_data = X.copy()\n",
    "            \n",
    "            # Convert to numpy array\n",
    "            X_array = X_data.values\n",
    "            \n",
    "            # Simple imputation for initial values\n",
    "            simple_imputer = SimpleImputer(strategy='mean')\n",
    "            X_imputed = simple_imputer.fit_transform(X_array)\n",
    "            \n",
    "            # Scale data\n",
    "            X_scaled = self.scaler.transform(X_imputed)\n",
    "            \n",
    "            # Create a mask for missing values\n",
    "            missing_mask = np.isnan(X_array)\n",
    "            \n",
    "            # Project data onto principal components\n",
    "            z = self.pca_model.transform(X_scaled)\n",
    "            \n",
    "            # Get mean of W from model\n",
    "            w_samples = self.model.sample_loadings(n_samples=self.n_samples)\n",
    "            w_mean = w_samples.mean(dim=0).cpu().detach().numpy()\n",
    "            \n",
    "            # Reconstruct data\n",
    "            X_reconstructed = np.dot(z, w_mean.T)\n",
    "            \n",
    "            # Inverse transform to original scale\n",
    "            X_reconstructed = self.scaler.inverse_transform(X_reconstructed)\n",
    "            \n",
    "            # Only replace missing values\n",
    "            X_array_imputed = X_array.copy()\n",
    "            X_array_imputed[missing_mask] = X_reconstructed[missing_mask]\n",
    "            \n",
    "            # Convert back to DataFrame\n",
    "            imputed_df = pd.DataFrame(X_array_imputed, \n",
    "                                      columns=X.columns, \n",
    "                                      index=X.index)\n",
    "        else:\n",
    "            # For the same data used in fitting, use posterior samples\n",
    "            \n",
    "            # Sample from the model multiple times to get uncertainty estimates\n",
    "            w_samples = self.model.sample_loadings(n_samples=self.n_samples)  # [n_samples, n_features, n_components]\n",
    "            w_mean = w_samples.mean(dim=0).cpu().detach().numpy()  # [n_features, n_components]\n",
    "            \n",
    "            # Reconstruct data from latent variables (z)\n",
    "            X_reconstructed = np.dot(self.z, w_mean.T)  # [n_samples, n_features]\n",
    "            \n",
    "            # Inverse transform to original scale\n",
    "            X_reconstructed = self.scaler.inverse_transform(X_reconstructed)\n",
    "            \n",
    "            # Convert to DataFrame\n",
    "            X_reconstructed_df = pd.DataFrame(X_reconstructed, \n",
    "                                             columns=self.columns, \n",
    "                                             index=self.index)\n",
    "            \n",
    "            # Only replace missing values\n",
    "            for col in imputed_df.columns:\n",
    "                missing_idx = imputed_df[col].isna()\n",
    "                if missing_idx.any():\n",
    "                    imputed_df.loc[missing_idx, col] = X_reconstructed_df.loc[missing_idx, col].values\n",
    "        \n",
    "        return imputed_df\n",
    "    \n",
    "    def fit_transform(self, X, y=None):\n",
    "        \"\"\"\n",
    "        Fit and transform\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X : pandas.DataFrame\n",
    "            Data with missing values\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        imputed_df : pandas.DataFrame\n",
    "            Data with imputed values\n",
    "        \"\"\"\n",
    "        self.fit(X)\n",
    "        return self.transform(X)\n",
    "\n",
    "\n",
    "def apply_bpca_imputation(df, columns_to_impute, validation_df=None, validation_masks=None, original_values=None, display_progress=True, use_gpu=True):\n",
    "    \"\"\"\n",
    "    Apply Bayesian PCA imputation to patient data using PyTorch\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pandas.DataFrame\n",
    "        Patient data with missing values\n",
    "    columns_to_impute : list\n",
    "        List of column names to impute\n",
    "    validation_df : pandas.DataFrame, optional\n",
    "        Validation dataset with artificially missing values\n",
    "    validation_masks : dict, optional\n",
    "        Dictionary of masks for validation data\n",
    "    original_values : dict, optional\n",
    "        Dictionary of original values for validation\n",
    "    display_progress : bool\n",
    "        Whether to display progress\n",
    "    use_gpu : bool\n",
    "        Whether to use GPU acceleration\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    imputed_df : pandas.DataFrame\n",
    "        Data with imputed values\n",
    "    validation_results : dict, optional\n",
    "        Validation results if validation data provided\n",
    "    \"\"\"\n",
    "    # Extract relevant columns including potential predictors\n",
    "    # Use all columns except those with excessive missing values\n",
    "    threshold = 0.5  # Columns with more than 50% missing values are excluded\n",
    "    columns_to_use = [col for col in df.columns \n",
    "                      if df[col].isna().mean() < threshold]\n",
    "    \n",
    "    # Ensure all columns_to_impute are included\n",
    "    for col in columns_to_impute:\n",
    "        if col not in columns_to_use:\n",
    "            columns_to_use.append(col)\n",
    "    \n",
    "    # Extract subset of data\n",
    "    X = df[columns_to_use].copy()\n",
    "    \n",
    "    # Ensure all columns are numeric\n",
    "    for col in X.columns:\n",
    "        X[col] = pd.to_numeric(X[col], errors='coerce')\n",
    "    \n",
    "    # Initialize Bayesian PCA imputer\n",
    "    n_components = min(5, len(columns_to_use) - 1)  # Ensure n_components is valid\n",
    "    \n",
    "    # Set device based on user preference and availability\n",
    "    device = 'cuda' if use_gpu and torch.cuda.is_available() else 'cpu'\n",
    "    \n",
    "    # Determine batch size based on data size\n",
    "    batch_size = min(64, len(X))  # Default 64, but smaller if dataset is tiny\n",
    "    \n",
    "    # Configure epochs based on data size\n",
    "    n_epochs = max(100, min(300, 10000 // len(X) + 30))  # Updated minimum to 100 epochs\n",
    "    \n",
    "    if display_progress:\n",
    "        print(f\"Starting PyTorch Bayesian PCA imputation with {n_components} components\")\n",
    "        print(f\"Using device: {device}\")\n",
    "        print(f\"Training for up to {n_epochs} epochs with batch size {batch_size}\")\n",
    "        \n",
    "        if device == 'cuda':\n",
    "            print(f\"GPU Info: {torch.cuda.get_device_name(0)}\")\n",
    "            print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "        \n",
    "    # Determine reasonable number of posterior samples based on data size\n",
    "    n_samples = 1000  # Default\n",
    "    \n",
    "    imputer = BayesianPCAImputer(\n",
    "        n_components=n_components,\n",
    "        n_samples=n_samples,\n",
    "        batch_size=batch_size,\n",
    "        n_epochs=n_epochs,\n",
    "        learning_rate=0.01,\n",
    "        device=device,\n",
    "        verbose=display_progress\n",
    "    )\n",
    "    \n",
    "    # Fit and transform with progress tracking\n",
    "    start_time = None\n",
    "    if display_progress:\n",
    "        start_time = time.time()\n",
    "        print(\"Training PyTorch Bayesian PCA imputation model...\")\n",
    "    \n",
    "    X_imputed = imputer.fit_transform(X)\n",
    "    \n",
    "    if display_progress and start_time:\n",
    "        elapsed = time.time() - start_time\n",
    "        print(f\"BPCA imputation completed in {elapsed:.2f} seconds ({elapsed/60:.2f} minutes)\")\n",
    "    \n",
    "    # Create imputed dataframe\n",
    "    imputed_df = df.copy()\n",
    "    imputed_df[columns_to_impute] = X_imputed[columns_to_impute]\n",
    "    \n",
    "    # Validate if validation data provided\n",
    "    validation_results = None\n",
    "    if validation_df is not None and validation_masks is not None and original_values is not None:\n",
    "        validation_results = {}\n",
    "        validation_start_time = None\n",
    "        \n",
    "        if display_progress:\n",
    "            validation_start_time = time.time()\n",
    "            print(\"\\nStarting validation on test data...\")\n",
    "        \n",
    "        # Extract validation data\n",
    "        X_val = validation_df[columns_to_use].copy()\n",
    "        \n",
    "        # Ensure all validation columns are numeric\n",
    "        for col in X_val.columns:\n",
    "            X_val[col] = pd.to_numeric(X_val[col], errors='coerce')\n",
    "            \n",
    "        # Impute validation data\n",
    "        X_val_imputed = imputer.transform(X_val)\n",
    "        \n",
    "        # For storing overall metrics\n",
    "        all_real_vals = []\n",
    "        all_imputed_vals = []\n",
    "        column_metrics = []\n",
    "        \n",
    "        # Compare imputed values to real values\n",
    "        with tqdm(columns_to_impute, desc=\"Validating results\") as pbar:\n",
    "            for col in pbar:\n",
    "                pbar.set_description(f\"Validating {col}\")\n",
    "                # Get indices where values were artificially set to NaN\n",
    "                mask = validation_masks[col] & validation_df[col].isna()\n",
    "                \n",
    "                if mask.sum() == 0:\n",
    "                    validation_results[col] = {\n",
    "                        'error': \"No artificially missing values\"\n",
    "                    }\n",
    "                    continue\n",
    "                    \n",
    "                real_vals = original_values[col][mask]\n",
    "                imputed_vals = X_val_imputed[col][mask]\n",
    "                \n",
    "                # Collect all values for overall metrics\n",
    "                all_real_vals.extend(real_vals.values)\n",
    "                all_imputed_vals.extend(imputed_vals.values)\n",
    "                \n",
    "                # Calculate continuous metrics (MAE and RMSE) - NO ROUNDING\n",
    "                mae = mean_absolute_error(real_vals, imputed_vals)\n",
    "                rmse = np.sqrt(mean_squared_error(real_vals, imputed_vals))\n",
    "                \n",
    "                # Calculate classification metrics - WITH ROUNDING\n",
    "                real_vals_class = process_for_classification(real_vals)\n",
    "                imputed_vals_class = process_for_classification(imputed_vals)\n",
    "                \n",
    "                classification_metrics = calculate_classification_metrics(real_vals_class, imputed_vals_class)\n",
    "                \n",
    "                # Store metrics for summary\n",
    "                column_metrics.append({\n",
    "                    'column': col,\n",
    "                    'mae': mae,\n",
    "                    'rmse': rmse,\n",
    "                    'accuracy': classification_metrics['accuracy'],\n",
    "                    'count': len(real_vals)\n",
    "                })\n",
    "                \n",
    "                validation_results[col] = {\n",
    "                    'mae': mae,\n",
    "                    'rmse': rmse,\n",
    "                    'accuracy': classification_metrics['accuracy'],\n",
    "                    'auc_multiclass': classification_metrics['auc_multiclass'],\n",
    "                    'avg_sensitivity': classification_metrics['avg_sensitivity'],\n",
    "                    'avg_specificity': classification_metrics['avg_specificity'],\n",
    "                    'avg_ppv': classification_metrics['avg_ppv'],\n",
    "                    'avg_npv': classification_metrics['avg_npv'],\n",
    "                    'precision_macro': classification_metrics['precision_macro'],\n",
    "                    'recall_macro': classification_metrics['recall_macro'],\n",
    "                    'real_distribution': real_vals.describe(),\n",
    "                    'imputed_distribution': imputed_vals.describe()\n",
    "                }\n",
    "                \n",
    "                # Update progress\n",
    "                pbar.set_postfix({\"MAE\": f\"{mae:.4f}\", \"RMSE\": f\"{rmse:.4f}\", \"Acc\": f\"{classification_metrics['accuracy']:.4f}\"})\n",
    "        \n",
    "        # Calculate overall metrics\n",
    "        if all_real_vals:\n",
    "            overall_mae = mean_absolute_error(all_real_vals, all_imputed_vals)\n",
    "            overall_rmse = np.sqrt(mean_squared_error(all_real_vals, all_imputed_vals))\n",
    "            \n",
    "            validation_results['overall'] = {\n",
    "                'mae': overall_mae,\n",
    "                'rmse': overall_rmse,\n",
    "                'total_values': len(all_real_vals)\n",
    "            }\n",
    "        \n",
    "        # Print detailed summary\n",
    "        if display_progress:\n",
    "            validation_time = time.time() - validation_start_time if validation_start_time else 0\n",
    "            total_time = validation_time + (time.time() - start_time if start_time else 0)\n",
    "            \n",
    "            print(\"\\n\" + \"=\"*80)\n",
    "            print(f\"PYTORCH BAYESIAN PCA IMPUTATION SUMMARY (Device: {device})\")\n",
    "            print(\"=\"*80)\n",
    "            \n",
    "            # Time information\n",
    "            print(f\"\\nTIMING INFORMATION:\")\n",
    "            print(f\"  Training Time: {elapsed:.2f} seconds ({elapsed/60:.2f} minutes)\")\n",
    "            print(f\"  Validation Time: {validation_time:.2f} seconds ({validation_time/60:.2f} minutes)\")\n",
    "            print(f\"  Total Time: {total_time:.2f} seconds ({total_time/60:.2f} minutes)\")\n",
    "            \n",
    "            # Overall metrics\n",
    "            if 'overall' in validation_results:\n",
    "                print(f\"\\nOVERALL METRICS (across {validation_results['overall']['total_values']} values):\")\n",
    "                print(f\"  MAE: {validation_results['overall']['mae']:.4f}\")\n",
    "                print(f\"  RMSE: {validation_results['overall']['rmse']:.4f}\")\n",
    "            \n",
    "            # Per-column metrics\n",
    "            print(\"\\nPER-COLUMN METRICS:\")\n",
    "            print(\"-\"*80)\n",
    "            print(f\"{'Column':<20} {'MAE':<10} {'RMSE':<10} {'Accuracy':<10} {'Count':<10}\")\n",
    "            print(\"-\"*80)\n",
    "            \n",
    "            for metric in sorted(column_metrics, key=lambda x: x['mae']):\n",
    "                print(f\"{metric['column']:<20} {metric['mae']:<10.4f} {metric['rmse']:<10.4f} {metric['accuracy']:<10.4f} {metric['count']:<10}\")\n",
    "            \n",
    "            print(\"=\"*80)\n",
    "    \n",
    "    return imputed_df, validation_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting PyTorch Bayesian PCA imputation with 5 components\n",
      "Using device: cpu\n",
      "Training for up to 100 epochs with batch size 64\n",
      "Training PyTorch Bayesian PCA imputation model...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Bayesian PCA model on cpu...\n",
      "Data shape: torch.Size([5230, 10]), Components: 5\n",
      "Missing values: 36725 out of 52300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   3%|â–Ž         | 3/100 [00:04<02:25,  1.50s/it, Loss=143057371136.0000]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[55], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m columns_to_impute \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mge1\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mge2\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mge3\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mge4\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mge5\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mge6\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Apply Transformer imputation\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m imputed_df_bpca, validation_results_bpca \u001b[38;5;241m=\u001b[39m \u001b[43mapply_bpca_imputation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns_to_impute\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[54], line 460\u001b[0m, in \u001b[0;36mapply_bpca_imputation\u001b[0;34m(df, columns_to_impute, validation_df, validation_masks, original_values, display_progress, use_gpu)\u001b[0m\n\u001b[1;32m    457\u001b[0m     start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m    458\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining PyTorch Bayesian PCA imputation model...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 460\u001b[0m X_imputed \u001b[38;5;241m=\u001b[39m \u001b[43mimputer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    462\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m display_progress \u001b[38;5;129;01mand\u001b[39;00m start_time:\n\u001b[1;32m    463\u001b[0m     elapsed \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start_time\n",
      "File \u001b[0;32m~/jinenv/lib/python3.11/site-packages/sklearn/utils/_set_output.py:319\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[0;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[1;32m    318\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 319\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    320\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m    321\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[1;32m    322\u001b[0m         return_tuple \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    323\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[1;32m    324\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[1;32m    325\u001b[0m         )\n",
      "Cell \u001b[0;32mIn[54], line 370\u001b[0m, in \u001b[0;36mBayesianPCAImputer.fit_transform\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    356\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mfit_transform\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    357\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    358\u001b[0m \u001b[38;5;124;03m    Fit and transform\u001b[39;00m\n\u001b[1;32m    359\u001b[0m \u001b[38;5;124;03m    \u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    368\u001b[0m \u001b[38;5;124;03m        Data with imputed values\u001b[39;00m\n\u001b[1;32m    369\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 370\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    371\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform(X)\n",
      "Cell \u001b[0;32mIn[54], line 179\u001b[0m, in \u001b[0;36mBayesianPCAImputer.fit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    176\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlatent_z \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mParameter(torch\u001b[38;5;241m.\u001b[39mtensor(z_init, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice))\n\u001b[1;32m    178\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[0;32m--> 179\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_train_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask_tensor\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    181\u001b[0m \u001b[38;5;66;03m# Store final latent variables for later use\u001b[39;00m\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mz \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlatent_z\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\n",
      "Cell \u001b[0;32mIn[54], line 244\u001b[0m, in \u001b[0;36mBayesianPCAImputer._train_model\u001b[0;34m(self, X, mask)\u001b[0m\n\u001b[1;32m    241\u001b[0m loss \u001b[38;5;241m=\u001b[39m mse_loss \u001b[38;5;241m*\u001b[39m tau \u001b[38;5;241m+\u001b[39m z_prior_loss \u001b[38;5;241m+\u001b[39m w_prior_loss\n\u001b[1;32m    243\u001b[0m \u001b[38;5;66;03m# Backward pass and optimization step\u001b[39;00m\n\u001b[0;32m--> 244\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    245\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m    247\u001b[0m \u001b[38;5;66;03m# Get current loss\u001b[39;00m\n",
      "File \u001b[0;32m~/jinenv/lib/python3.11/site-packages/torch/_tensor.py:626\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    616\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    617\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    618\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    619\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    624\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    625\u001b[0m     )\n\u001b[0;32m--> 626\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    627\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    628\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/jinenv/lib/python3.11/site-packages/torch/autograd/__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/jinenv/lib/python3.11/site-packages/torch/autograd/graph.py:823\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    821\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    822\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 823\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    824\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    825\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    826\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    827\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Define columns to impute\n",
    "ordinal_cols = (\n",
    "    [f\"gp{i}\" for i in range(1, 8)] + [f\"gs{i}\" for i in range(1, 8)] +\n",
    "    [f\"ge{i}\" for i in range(1, 7)] + [f\"gf{i}\" for i in range(1, 8)] +\n",
    "    [f\"a_hn{i}\" for i in range(1, 6)] + [\"a_hn7\", \"a_hn10\"] +\n",
    "    [f\"a_e{i}\" for i in range(1, 8)] + [\"a_c6\", \"a_c2\", \"a_act11\"]\n",
    ")\n",
    "columns_to_impute = ordinal_cols#['ge1', 'ge2', 'ge3', 'ge4', 'ge5', 'ge6']\n",
    "\n",
    "# Apply Transformer imputation\n",
    "imputed_df_bpca, validation_results_bpca = apply_bpca_imputation(df, columns_to_impute)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        1.286870\n",
       "14       1.268549\n",
       "18       1.276519\n",
       "22       1.286916\n",
       "25       1.000000\n",
       "           ...   \n",
       "18178    1.341442\n",
       "18179    1.000000\n",
       "18180    1.354321\n",
       "18181    1.354068\n",
       "18185    1.341488\n",
       "Name: ge1, Length: 5230, dtype: float64"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imputed_df_bpca['ge1']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model - Da Xu (interview paper) DL method"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "https://www.sciencedirect.com/science/article/pii/S1532046420302045\n",
    "https://ieeexplore.ieee.org/document/9238392"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from tqdm import tqdm\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "class DeepAutoencoderModel(nn.Module):\n",
    "    \"\"\"\n",
    "    Deep Autoencoder model for missing value imputation with patient embedding\n",
    "    \n",
    "    Based on the papers:\n",
    "    - \"A Deep Learningâ€“Based Unsupervised Method to Impute Missing Values in Patient Records\"\n",
    "    - \"A deep learningâ€“based, unsupervised method to impute missing values in electronic health records\"\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim, patient_vocab_size, embedding_dim=16, hidden_dims=(64, 32, 16, 32, 64), \n",
    "                 dropout_rate=0.2, activation='relu'):\n",
    "        \"\"\"\n",
    "        Initialize the model\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        input_dim : int\n",
    "            Dimension of input data\n",
    "        patient_vocab_size : int\n",
    "            Number of unique patients for embedding\n",
    "        embedding_dim : int\n",
    "            Size of patient embedding vectors\n",
    "        hidden_dims : tuple\n",
    "            Sizes of hidden layers\n",
    "        dropout_rate : float\n",
    "            Dropout rate for regularization\n",
    "        activation : str\n",
    "            Activation function ('relu' or 'elu')\n",
    "        \"\"\"\n",
    "        super(DeepAutoencoderModel, self).__init__()\n",
    "        \n",
    "        # Patient embedding layer\n",
    "        self.patient_embedding = nn.Embedding(patient_vocab_size + 1, embedding_dim)\n",
    "        \n",
    "        # Build encoder layers\n",
    "        self.encoder_layers = nn.ModuleList()\n",
    "        prev_dim = input_dim\n",
    "        \n",
    "        # Get the number of layers for encoder (half of hidden_dims rounded up)\n",
    "        encoder_size = (len(hidden_dims) + 1) // 2\n",
    "        \n",
    "        for i in range(encoder_size):\n",
    "            layer = nn.Sequential(\n",
    "                nn.Linear(prev_dim, hidden_dims[i]),\n",
    "                nn.BatchNorm1d(hidden_dims[i]),\n",
    "                nn.ReLU() if activation == 'relu' else nn.ELU(),\n",
    "                nn.Dropout(dropout_rate)\n",
    "            )\n",
    "            self.encoder_layers.append(layer)\n",
    "            prev_dim = hidden_dims[i]\n",
    "        \n",
    "        # Code layer dimension (bottleneck)\n",
    "        self.code_dim = hidden_dims[encoder_size - 1]\n",
    "        \n",
    "        # Build decoder layers\n",
    "        self.decoder_layers = nn.ModuleList()\n",
    "        \n",
    "        # First decoder layer takes concatenated code and patient embedding\n",
    "        prev_dim = self.code_dim + embedding_dim\n",
    "        \n",
    "        for i in range(encoder_size, len(hidden_dims)):\n",
    "            layer = nn.Sequential(\n",
    "                nn.Linear(prev_dim, hidden_dims[i]),\n",
    "                nn.BatchNorm1d(hidden_dims[i]),\n",
    "                nn.ReLU() if activation == 'relu' else nn.ELU(),\n",
    "                nn.Dropout(dropout_rate)\n",
    "            )\n",
    "            self.decoder_layers.append(layer)\n",
    "            prev_dim = hidden_dims[i]\n",
    "        \n",
    "        # Output layer\n",
    "        self.output_layer = nn.Linear(prev_dim, input_dim)\n",
    "        \n",
    "    def encode(self, x):\n",
    "        \"\"\"\n",
    "        Encode input data to latent representation\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        x : torch.Tensor\n",
    "            Input data\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        torch.Tensor\n",
    "            Encoded representation\n",
    "        \"\"\"\n",
    "        h = x\n",
    "        for layer in self.encoder_layers:\n",
    "            h = layer(h)\n",
    "        return h\n",
    "    \n",
    "    def decode(self, code, patient_emb):\n",
    "        \"\"\"\n",
    "        Decode latent representation to reconstructed input\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        code : torch.Tensor\n",
    "            Encoded representation\n",
    "        patient_emb : torch.Tensor\n",
    "            Patient embedding vectors\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        torch.Tensor\n",
    "            Reconstructed input\n",
    "        \"\"\"\n",
    "        # Concatenate code and patient embedding\n",
    "        h = torch.cat([code, patient_emb], dim=1)\n",
    "        \n",
    "        for layer in self.decoder_layers:\n",
    "            h = layer(h)\n",
    "            \n",
    "        return self.output_layer(h)\n",
    "    \n",
    "    def forward(self, x, patient_ids):\n",
    "        \"\"\"\n",
    "        Forward pass through the model\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        x : torch.Tensor\n",
    "            Input data\n",
    "        patient_ids : torch.Tensor\n",
    "            Patient IDs\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        torch.Tensor\n",
    "            Reconstructed input\n",
    "        \"\"\"\n",
    "        # Get patient embeddings\n",
    "        patient_emb = self.patient_embedding(patient_ids).squeeze(1)\n",
    "        \n",
    "        # Encode input\n",
    "        code = self.encode(x)\n",
    "        \n",
    "        # Decode latent representation\n",
    "        output = self.decode(code, patient_emb)\n",
    "        \n",
    "        return output\n",
    "\n",
    "\n",
    "class MaskedMSELoss(nn.Module):\n",
    "    \"\"\"\n",
    "    MSE loss that ignores missing values\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(MaskedMSELoss, self).__init__()\n",
    "        \n",
    "    def forward(self, pred, target, mask):\n",
    "        \"\"\"\n",
    "        Calculate MSE loss only on observed values\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        pred : torch.Tensor\n",
    "            Predicted values\n",
    "        target : torch.Tensor\n",
    "            Target values\n",
    "        mask : torch.Tensor\n",
    "            Binary mask (1 for observed, 0 for missing)\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        torch.Tensor\n",
    "            Masked MSE loss\n",
    "        \"\"\"\n",
    "        # Apply mask to predictions and targets\n",
    "        masked_pred = pred * mask\n",
    "        masked_target = target * mask\n",
    "        \n",
    "        # Calculate squared error\n",
    "        squared_error = (masked_pred - masked_target) ** 2\n",
    "        \n",
    "        # Sum squared error and count observed values\n",
    "        sum_squared_error = torch.sum(squared_error)\n",
    "        count = torch.sum(mask) + 1e-8  # Add small epsilon to avoid division by zero\n",
    "        \n",
    "        # Return MSE\n",
    "        return sum_squared_error / count\n",
    "\n",
    "\n",
    "class TemporalSimilarityLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Loss component for temporal similarity regularization\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(TemporalSimilarityLoss, self).__init__()\n",
    "        \n",
    "    def forward(self, pred, patient_batch_indices):\n",
    "        \"\"\"\n",
    "        Calculate temporal similarity loss based on batch proximity\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        pred : torch.Tensor\n",
    "            Predicted values\n",
    "        patient_batch_indices : torch.Tensor\n",
    "            Indices grouping patients in the batch\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        torch.Tensor\n",
    "            Temporal similarity loss\n",
    "        \"\"\"\n",
    "        batch_size = pred.size(0)\n",
    "        \n",
    "        # If batch size is too small, return zero loss\n",
    "        if batch_size <= 1:\n",
    "            return torch.tensor(0.0, device=pred.device)\n",
    "        \n",
    "        # Create batch position indices\n",
    "        indices = torch.arange(batch_size, device=pred.device).float()\n",
    "        \n",
    "        # Expand dimensions for pairwise operations\n",
    "        pred_expanded_1 = pred.unsqueeze(1)  # [batch, 1, features]\n",
    "        pred_expanded_2 = pred.unsqueeze(0)  # [1, batch, features]\n",
    "        \n",
    "        indices_1 = indices.unsqueeze(1)  # [batch, 1]\n",
    "        indices_2 = indices.unsqueeze(0)  # [1, batch]\n",
    "        \n",
    "        # Calculate pairwise differences between predictions\n",
    "        pred_diff = torch.sum((pred_expanded_1 - pred_expanded_2) ** 2, dim=2)\n",
    "        \n",
    "        # Create patient similarity mask (1 where same patient, 0 otherwise)\n",
    "        patient_expanded_1 = patient_batch_indices.unsqueeze(1)\n",
    "        patient_expanded_2 = patient_batch_indices.unsqueeze(0)\n",
    "        patient_mask = (patient_expanded_1 == patient_expanded_2).float()\n",
    "        \n",
    "        # Create diagonal mask to exclude self-comparisons\n",
    "        diag_mask = 1.0 - torch.eye(batch_size, device=pred.device)\n",
    "        \n",
    "        # Combine masks\n",
    "        combined_mask = patient_mask * diag_mask\n",
    "        \n",
    "        # If no patient pairs exist, return zero loss\n",
    "        if torch.sum(combined_mask) == 0:\n",
    "            return torch.tensor(0.0, device=pred.device)\n",
    "        \n",
    "        # Calculate temporal similarity weights based on proximity in batch\n",
    "        # (Simple approach: closer indices = more similar)\n",
    "        temporal_diff = torch.abs(indices_1 - indices_2)\n",
    "        similarity_weights = 1.0 / (temporal_diff + 1.0)\n",
    "        \n",
    "        # Apply masks and weights\n",
    "        weighted_diff = pred_diff * similarity_weights * combined_mask\n",
    "        \n",
    "        # Normalize and return loss\n",
    "        return torch.sum(weighted_diff) / (torch.sum(combined_mask) + 1e-8)\n",
    "\n",
    "\n",
    "class DeepAutoencoderImputer:\n",
    "    \"\"\"\n",
    "    Deep Learning-Based Unsupervised Method for Missing Value Imputation\n",
    "    \n",
    "    Based on the papers:\n",
    "    - \"A Deep Learningâ€“Based Unsupervised Method to Impute Missing Values in Patient Records\"\n",
    "    - \"A deep learningâ€“based, unsupervised method to impute missing values in electronic health records\"\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 embedding_dim=16,\n",
    "                 hidden_layers=(64, 32, 16, 32, 64),\n",
    "                 activation='relu',\n",
    "                 dropout_rate=0.2,\n",
    "                 learning_rate=0.001,\n",
    "                 weight_decay=0.00001,\n",
    "                 temporal_weight=0.3,\n",
    "                 batch_size=16,\n",
    "                 epochs=100,\n",
    "                 patience=10,\n",
    "                 device=None,\n",
    "                 verbose=1):\n",
    "        \"\"\"\n",
    "        Initialize imputer\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        embedding_dim : int\n",
    "            Size of patient embedding vectors\n",
    "        hidden_layers : tuple\n",
    "            Sizes of hidden layers\n",
    "        activation : str\n",
    "            Activation function ('relu' or 'elu')\n",
    "        dropout_rate : float\n",
    "            Dropout rate for regularization\n",
    "        learning_rate : float\n",
    "            Learning rate for optimizer\n",
    "        weight_decay : float\n",
    "            L2 regularization strength\n",
    "        temporal_weight : float\n",
    "            Weight for temporal similarity regularization\n",
    "        batch_size : int\n",
    "            Batch size for training\n",
    "        epochs : int\n",
    "            Maximum number of training epochs\n",
    "        patience : int\n",
    "            Patience for early stopping\n",
    "        device : str or torch.device\n",
    "            Device to use ('cpu', 'cuda', or None for auto-detection)\n",
    "        verbose : int\n",
    "            Verbosity level (0 or 1)\n",
    "        \"\"\"\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_layers = hidden_layers\n",
    "        self.activation = activation\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.learning_rate = learning_rate\n",
    "        self.weight_decay = weight_decay\n",
    "        self.temporal_weight = temporal_weight\n",
    "        self.batch_size = batch_size\n",
    "        self.epochs = epochs\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        \n",
    "        # Set device\n",
    "        if device is None:\n",
    "            self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        else:\n",
    "            self.device = torch.device(device)\n",
    "            \n",
    "        if self.verbose:\n",
    "            print(f\"Using device: {self.device}\")\n",
    "            \n",
    "            # Print CUDA details if using GPU\n",
    "            if self.device.type == 'cuda':\n",
    "                print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")\n",
    "                print(f\"CUDA memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "                print(f\"CUDA capability: {torch.cuda.get_device_properties(0).major}.{torch.cuda.get_device_properties(0).minor}\")\n",
    "        \n",
    "        # Will be initialized during fitting\n",
    "        self.model = None\n",
    "        self.scaler = None\n",
    "        self.columns = None\n",
    "        self.patient_id_col = None\n",
    "        self.time_col = None\n",
    "        self.numerical_cols = []\n",
    "        self.categorical_cols = []\n",
    "        self.ordinal_cols = []\n",
    "        self.cat_encoders = {}\n",
    "        self.columns_to_impute = None\n",
    "        self.history = {'train_loss': [], 'val_loss': []}\n",
    "        \n",
    "    def fit(self, X, patient_id_col, time_col=None, numerical_cols=None, \n",
    "            categorical_cols=None, ordinal_cols=None):\n",
    "        \"\"\"\n",
    "        Fit the imputer model\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X : DataFrame\n",
    "            Input data with missing values\n",
    "        patient_id_col : str\n",
    "            Name of the column containing patient IDs\n",
    "        time_col : str, optional\n",
    "            Name of the column containing time information\n",
    "        numerical_cols : list, optional\n",
    "            Names of numerical columns. If None, autodetect.\n",
    "        categorical_cols : list, optional\n",
    "            Names of categorical columns. If None, autodetect.\n",
    "        ordinal_cols : list, optional\n",
    "            Names of ordinal columns. If None, autodetect.\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        self : object\n",
    "            Returns self\n",
    "        \"\"\"\n",
    "        self.original_data = X.copy()\n",
    "        self.original_columns = X.columns.tolist()\n",
    "        self.patient_id_col = patient_id_col\n",
    "        self.time_col = time_col\n",
    "        \n",
    "        # Autodetect column types if not provided\n",
    "        if numerical_cols is None:\n",
    "            numerical_cols = X.select_dtypes(include=['float', 'int']).columns.tolist()\n",
    "            # Exclude patient_id_col\n",
    "            if patient_id_col in numerical_cols:\n",
    "                numerical_cols.remove(patient_id_col)\n",
    "        self.numerical_cols = numerical_cols\n",
    "        \n",
    "        if categorical_cols is None:\n",
    "            categorical_cols = X.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "            # Exclude patient_id_col and time_col\n",
    "            if patient_id_col in categorical_cols:\n",
    "                categorical_cols.remove(patient_id_col)\n",
    "            if time_col and time_col in categorical_cols:\n",
    "                categorical_cols.remove(time_col)\n",
    "        self.categorical_cols = categorical_cols\n",
    "        \n",
    "        if ordinal_cols is None:\n",
    "            ordinal_cols = []\n",
    "        self.ordinal_cols = ordinal_cols\n",
    "        \n",
    "        # Preprocess data\n",
    "        X_processed, missing_mask, cat_indices = self._preprocess_data(X, fit=True)\n",
    "        \n",
    "        # Extract patient IDs\n",
    "        patient_ids = X[patient_id_col].values\n",
    "        \n",
    "        # Train model\n",
    "        self._train_model(X_processed, patient_ids, missing_mask)\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def _preprocess_data(self, X, fit=True):\n",
    "        \"\"\"\n",
    "        Preprocess data for the imputation model\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X : DataFrame\n",
    "            Input data\n",
    "        fit : bool\n",
    "            Whether to fit or transform\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        tuple\n",
    "            (processed data, missing mask, categorical indices)\n",
    "        \"\"\"\n",
    "        # Make a copy of the input data\n",
    "        X_processed = X.copy()\n",
    "\n",
    "        # Track the derived date columns to avoid adding them to columns_to_impute\n",
    "        self.derived_date_cols = []\n",
    "\n",
    "        # Handle datetime column if present\n",
    "        if self.time_col and self.time_col in X.columns and pd.api.types.is_datetime64_any_dtype(X[self.time_col]):\n",
    "            # Extract features from datetime\n",
    "            date_year_col = f'{self.time_col}_year'\n",
    "            date_month_col = f'{self.time_col}_month'\n",
    "            date_day_col = f'{self.time_col}_day'\n",
    "            \n",
    "            X_processed[date_year_col] = X[self.time_col].dt.year\n",
    "            X_processed[date_month_col] = X[self.time_col].dt.month\n",
    "            X_processed[date_day_col] = X[self.time_col].dt.day\n",
    "            \n",
    "            # Store derived date columns\n",
    "            self.derived_date_cols = [date_year_col, date_month_col, date_day_col]\n",
    "            \n",
    "            # Add these new columns to numerical cols but NOT to columns_to_impute\n",
    "            if fit:\n",
    "                for col in self.derived_date_cols:\n",
    "                    if col not in self.numerical_cols:\n",
    "                        self.numerical_cols.append(col)\n",
    "                    \n",
    "            # Drop the original datetime column\n",
    "            X_processed = X_processed.drop(columns=[self.time_col])\n",
    "\n",
    "        # Create missing value mask (1 for observed, 0 for missing)\n",
    "        missing_mask = ~X_processed.isna()\n",
    "\n",
    "        # Scale numerical columns\n",
    "        if self.numerical_cols:\n",
    "            # Use only available numeric columns\n",
    "            numeric_cols_present = [col for col in self.numerical_cols if col in X_processed.columns]\n",
    "            \n",
    "            if fit:\n",
    "                self.num_scaler = StandardScaler()\n",
    "                if numeric_cols_present:\n",
    "                    # Temporarily fill missing values for scaling\n",
    "                    temp_imputer = SimpleImputer(strategy='mean')\n",
    "                    X_temp = pd.DataFrame(\n",
    "                        temp_imputer.fit_transform(X_processed[numeric_cols_present]),\n",
    "                        columns=numeric_cols_present,\n",
    "                        index=X_processed.index\n",
    "                    )\n",
    "                    \n",
    "                    # Fit scaler on non-missing data\n",
    "                    X_numerical_scaled = self.num_scaler.fit_transform(X_temp)\n",
    "                    \n",
    "                    # Update processed data with scaled values\n",
    "                    for i, col in enumerate(numeric_cols_present):\n",
    "                        X_processed[col] = X_numerical_scaled[:, i]\n",
    "            else:\n",
    "                if numeric_cols_present:\n",
    "                    # Temporarily fill missing values for scaling\n",
    "                    temp_imputer = SimpleImputer(strategy='mean')\n",
    "                    X_temp = pd.DataFrame(\n",
    "                        temp_imputer.fit_transform(X_processed[numeric_cols_present]),\n",
    "                        columns=numeric_cols_present,\n",
    "                        index=X_processed.index\n",
    "                    )\n",
    "                    \n",
    "                    # Transform with scaler\n",
    "                    X_numerical_scaled = self.num_scaler.transform(X_temp)\n",
    "                    \n",
    "                    # Update processed data with scaled values\n",
    "                    for i, col in enumerate(numeric_cols_present):\n",
    "                        X_processed[col] = X_numerical_scaled[:, i]\n",
    "\n",
    "        # Encode categorical columns\n",
    "        cat_indices = {}\n",
    "        current_idx = len(self.numerical_cols)\n",
    "\n",
    "        for col in self.categorical_cols:\n",
    "            if col in X_processed.columns:\n",
    "                # Handle categorical data safely\n",
    "                if pd.api.types.is_categorical_dtype(X_processed[col]):\n",
    "                    # Convert to string first to avoid category errors\n",
    "                    filled_col = X_processed[col].astype(str).fillna('MISSING')\n",
    "                else:\n",
    "                    # Fill NaN values with a placeholder\n",
    "                    filled_col = X_processed[col].fillna('MISSING')\n",
    "                \n",
    "                if fit:\n",
    "                    # Handle both older and newer scikit-learn versions\n",
    "                    try:\n",
    "                        # For newer scikit-learn versions\n",
    "                        encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "                    except TypeError:\n",
    "                        # For older scikit-learn versions\n",
    "                        encoder = OneHotEncoder(sparse=False, handle_unknown='ignore')\n",
    "                        \n",
    "                    encoded = encoder.fit_transform(filled_col.values.reshape(-1, 1))\n",
    "                    self.cat_encoders[col] = encoder\n",
    "                else:\n",
    "                    encoder = self.cat_encoders[col]\n",
    "                    encoded = encoder.transform(filled_col.values.reshape(-1, 1))\n",
    "                \n",
    "                # Add encoded columns\n",
    "                encoded_cols = [f\"{col}_{cat}\" for cat in encoder.categories_[0]]\n",
    "                cat_indices[col] = (current_idx, current_idx + len(encoded_cols))\n",
    "                current_idx += len(encoded_cols)\n",
    "                \n",
    "                for i, encoded_col in enumerate(encoded_cols):\n",
    "                    X_processed[encoded_col] = encoded[:, i]\n",
    "                \n",
    "                # Drop original categorical column\n",
    "                X_processed = X_processed.drop(columns=[col])\n",
    "                \n",
    "                # Update missing mask for one-hot encoded columns\n",
    "                for encoded_col in encoded_cols:\n",
    "                    missing_mask[encoded_col] = missing_mask[col]\n",
    "\n",
    "        # Drop non-numeric columns and ensure all data is numeric\n",
    "        X_processed = X_processed.select_dtypes(include=[np.number])\n",
    "        missing_mask = missing_mask[X_processed.columns]\n",
    "\n",
    "        if fit:\n",
    "            self.processed_columns = X_processed.columns.tolist()\n",
    "            \n",
    "        return X_processed, missing_mask, cat_indices\n",
    "    \n",
    "    def _train_model(self, X_processed, patient_ids, missing_mask):\n",
    "        \"\"\"\n",
    "        Train the imputation model\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X_processed : DataFrame\n",
    "            Preprocessed data\n",
    "        patient_ids : ndarray\n",
    "            Patient IDs for each record\n",
    "        missing_mask : DataFrame\n",
    "            Binary mask for observed values\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        None\n",
    "        \"\"\"\n",
    "        # Convert to numpy arrays\n",
    "        X_np = X_processed.values\n",
    "        mask_np = missing_mask.values\n",
    "        \n",
    "        # Create patient ID mapping\n",
    "        unique_patient_ids = np.unique(patient_ids)\n",
    "        patient_id_to_idx = {id_: i for i, id_ in enumerate(unique_patient_ids)}\n",
    "        \n",
    "        # Convert patient IDs to numeric indices\n",
    "        patient_indices = np.array([patient_id_to_idx.get(id_, len(patient_id_to_idx)) for id_ in patient_ids])\n",
    "        \n",
    "        # Also create a mapping from record index to patient index for temporal loss\n",
    "        record_to_patient_idx = patient_indices.copy()\n",
    "        \n",
    "        # Convert to PyTorch tensors\n",
    "        X_tensor = torch.FloatTensor(X_np)\n",
    "        mask_tensor = torch.FloatTensor(mask_np)\n",
    "        patient_tensor = torch.LongTensor(patient_indices)\n",
    "        patient_batch_tensor = torch.LongTensor(record_to_patient_idx)\n",
    "        \n",
    "        # Create dataset and dataloader\n",
    "        dataset = TensorDataset(X_tensor, mask_tensor, patient_tensor, patient_batch_tensor)\n",
    "        \n",
    "        # Split into train and validation sets\n",
    "        train_size = int(0.8 * len(dataset))\n",
    "        val_size = len(dataset) - train_size\n",
    "        train_dataset, val_dataset = torch.utils.data.random_split(\n",
    "            dataset, [train_size, val_size]\n",
    "        )\n",
    "        \n",
    "        train_loader = DataLoader(\n",
    "            train_dataset, \n",
    "            batch_size=self.batch_size, \n",
    "            shuffle=True,\n",
    "            num_workers=0 if self.device.type == 'cuda' else 2  # Use multiple workers on CPU\n",
    "        )\n",
    "        \n",
    "        val_loader = DataLoader(\n",
    "            val_dataset, \n",
    "            batch_size=self.batch_size, \n",
    "            shuffle=False,\n",
    "            num_workers=0 if self.device.type == 'cuda' else 2\n",
    "        )\n",
    "        \n",
    "        # Initialize model\n",
    "        input_dim = X_np.shape[1]\n",
    "        patient_vocab_size = len(unique_patient_ids)\n",
    "        \n",
    "        if self.verbose:\n",
    "            print(f\"Building model with input dimension {input_dim} and {patient_vocab_size} unique patients\")\n",
    "            \n",
    "        self.model = DeepAutoencoderModel(\n",
    "            input_dim=input_dim,\n",
    "            patient_vocab_size=patient_vocab_size,\n",
    "            embedding_dim=self.embedding_dim,\n",
    "            hidden_dims=self.hidden_layers,\n",
    "            dropout_rate=self.dropout_rate,\n",
    "            activation=self.activation\n",
    "        ).to(self.device)\n",
    "        \n",
    "        # Initialize loss functions\n",
    "        masked_loss_fn = MaskedMSELoss().to(self.device)\n",
    "        temporal_loss_fn = TemporalSimilarityLoss().to(self.device)\n",
    "        \n",
    "        # Initialize optimizer\n",
    "        optimizer = optim.Adam(\n",
    "            self.model.parameters(), \n",
    "            lr=self.learning_rate, \n",
    "            weight_decay=self.weight_decay\n",
    "        )\n",
    "        \n",
    "        # Initialize learning rate scheduler\n",
    "        scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "            optimizer, \n",
    "            mode='min', \n",
    "            factor=0.5, \n",
    "            patience=self.patience // 2, \n",
    "            verbose=self.verbose\n",
    "        )\n",
    "        \n",
    "        # Initialize early stopping variables\n",
    "        best_val_loss = float('inf')\n",
    "        best_model_state = None\n",
    "        patience_counter = 0\n",
    "        \n",
    "        # Training loop\n",
    "        if self.verbose:\n",
    "            print(f\"Starting training for {self.epochs} epochs\")\n",
    "            \n",
    "        # Use tqdm for progress tracking\n",
    "        pbar = tqdm(range(self.epochs), desc=\"Training\", disable=not self.verbose)\n",
    "        \n",
    "        for epoch in pbar:\n",
    "            # Training phase\n",
    "            self.model.train()\n",
    "            train_loss = 0.0\n",
    "            train_recon_loss = 0.0\n",
    "            train_temporal_loss = 0.0\n",
    "            batch_count = 0\n",
    "            \n",
    "            for X_batch, mask_batch, patient_batch, patient_idx_batch in train_loader:\n",
    "                # Move tensors to device\n",
    "                X_batch = X_batch.to(self.device)\n",
    "                mask_batch = mask_batch.to(self.device)\n",
    "                patient_batch = patient_batch.to(self.device)\n",
    "                patient_idx_batch = patient_idx_batch.to(self.device)\n",
    "                \n",
    "                # Forward pass\n",
    "                output = self.model(X_batch, patient_batch)\n",
    "                \n",
    "                # Calculate loss\n",
    "                recon_loss = masked_loss_fn(output, X_batch, mask_batch)\n",
    "                \n",
    "                if self.temporal_weight > 0:\n",
    "                    temporal_loss = temporal_loss_fn(output, patient_idx_batch)\n",
    "                    loss = recon_loss + self.temporal_weight * temporal_loss\n",
    "                else:\n",
    "                    temporal_loss = torch.tensor(0.0, device=self.device)\n",
    "                    loss = recon_loss\n",
    "                \n",
    "                # Backward pass and optimize\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "                # Accumulate losses\n",
    "                train_loss += loss.item()\n",
    "                train_recon_loss += recon_loss.item()\n",
    "                train_temporal_loss += temporal_loss.item()\n",
    "                batch_count += 1\n",
    "            \n",
    "            # Calculate average training losses\n",
    "            avg_train_loss = train_loss / batch_count\n",
    "            avg_train_recon_loss = train_recon_loss / batch_count\n",
    "            avg_train_temporal_loss = train_temporal_loss / batch_count\n",
    "            \n",
    "            # Validation phase\n",
    "            self.model.eval()\n",
    "            val_loss = 0.0\n",
    "            val_batch_count = 0\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                for X_batch, mask_batch, patient_batch, patient_idx_batch in val_loader:\n",
    "                    # Move tensors to device\n",
    "                    X_batch = X_batch.to(self.device)\n",
    "                    mask_batch = mask_batch.to(self.device)\n",
    "                    patient_batch = patient_batch.to(self.device)\n",
    "                    patient_idx_batch = patient_idx_batch.to(self.device)\n",
    "                    \n",
    "                    # Forward pass\n",
    "                    output = self.model(X_batch, patient_batch)\n",
    "                    \n",
    "                    # Calculate loss\n",
    "                    recon_loss = masked_loss_fn(output, X_batch, mask_batch)\n",
    "                    \n",
    "                    if self.temporal_weight > 0:\n",
    "                        temporal_loss = temporal_loss_fn(output, patient_idx_batch)\n",
    "                        loss = recon_loss + self.temporal_weight * temporal_loss\n",
    "                    else:\n",
    "                        loss = recon_loss\n",
    "                    \n",
    "                    # Accumulate loss\n",
    "                    val_loss += loss.item()\n",
    "                    val_batch_count += 1\n",
    "            \n",
    "            # Calculate average validation loss\n",
    "            avg_val_loss = val_loss / val_batch_count\n",
    "            \n",
    "            # Update learning rate scheduler\n",
    "            scheduler.step(avg_val_loss)\n",
    "            \n",
    "            # Update history\n",
    "            self.history['train_loss'].append(avg_train_loss)\n",
    "            self.history['val_loss'].append(avg_val_loss)\n",
    "            \n",
    "            # Update progress bar\n",
    "            if self.verbose:\n",
    "                pbar.set_postfix({\n",
    "                    \"train_loss\": f\"{avg_train_loss:.4f}\",\n",
    "                    \"val_loss\": f\"{avg_val_loss:.4f}\",\n",
    "                    \"recon\": f\"{avg_train_recon_loss:.4f}\",\n",
    "                    \"temporal\": f\"{avg_train_temporal_loss:.4f}\"\n",
    "                })\n",
    "            \n",
    "            # Check for early stopping\n",
    "            if avg_val_loss < best_val_loss:\n",
    "                best_val_loss = avg_val_loss\n",
    "                best_model_state = self.model.state_dict()\n",
    "                patience_counter = 0\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "                if patience_counter >= self.patience:\n",
    "                    if self.verbose:\n",
    "                        print(f\"Early stopping at epoch {epoch+1}\")\n",
    "                    break\n",
    "        \n",
    "        # Load best model\n",
    "        if best_model_state is not None:\n",
    "            self.model.load_state_dict(best_model_state)\n",
    "            \n",
    "        # Return model to evaluation mode\n",
    "        self.model.eval()\n",
    "    \n",
    "    def transform(self, X):\n",
    "        \"\"\"\n",
    "        Impute missing values using trained model\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X : DataFrame\n",
    "            Input data with missing values\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        DataFrame\n",
    "            Data with imputed values\n",
    "        \"\"\"\n",
    "        # Check if model is trained\n",
    "        if self.model is None:\n",
    "            raise ValueError(\"Model not trained. Call fit() first.\")\n",
    "        \n",
    "        # Make a copy of the input data\n",
    "        X_imputed = X.copy()\n",
    "        \n",
    "        # Preprocess data\n",
    "        X_processed, missing_mask, cat_indices = self._preprocess_data(X, fit=False)\n",
    "        \n",
    "        # Get patient IDs\n",
    "        patient_ids = X[self.patient_id_col].values\n",
    "        \n",
    "        # Create patient ID mapping\n",
    "        unique_patient_ids = np.unique(self.original_data[self.patient_id_col])\n",
    "        patient_id_to_idx = {id_: i for i, id_ in enumerate(unique_patient_ids)}\n",
    "        \n",
    "        # Convert patient IDs to numeric indices (with fallback for unseen patients)\n",
    "        patient_indices = np.array([patient_id_to_idx.get(id_, len(patient_id_to_idx)) for id_ in patient_ids])\n",
    "        \n",
    "        # Convert to PyTorch tensors\n",
    "        X_tensor = torch.FloatTensor(X_processed.values).to(self.device)\n",
    "        patient_tensor = torch.LongTensor(patient_indices).to(self.device)\n",
    "        \n",
    "        # Perform imputation in batches to avoid memory issues\n",
    "        batch_size = self.batch_size * 2  # Use larger batch size for inference\n",
    "        n_samples = X_tensor.shape[0]\n",
    "        imputed_data = np.zeros_like(X_tensor.cpu().numpy())\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for i in range(0, n_samples, batch_size):\n",
    "                end_idx = min(i + batch_size, n_samples)\n",
    "                batch_X = X_tensor[i:end_idx]\n",
    "                batch_patient = patient_tensor[i:end_idx]\n",
    "                \n",
    "                # Get model predictions\n",
    "                batch_output = self.model(batch_X, batch_patient)\n",
    "                \n",
    "                # Store predictions\n",
    "                imputed_data[i:end_idx] = batch_output.cpu().numpy()\n",
    "        \n",
    "        # Convert back to DataFrame\n",
    "        imputed_df = pd.DataFrame(imputed_data, columns=X_processed.columns, index=X_processed.index)\n",
    "        \n",
    "        # Handle numerical columns\n",
    "        for col in self.numerical_cols:\n",
    "            if col in X.columns and not col.startswith(f'{self.time_col}_'):\n",
    "                # Get mask for missing values in original data\n",
    "                missing_indices = X[col].isna()\n",
    "                \n",
    "                if missing_indices.any():\n",
    "                    # Only update missing values\n",
    "                    # If column is in columns_to_impute, round to nearest integer\n",
    "                    if self.columns_to_impute and col in self.columns_to_impute:\n",
    "                        X_imputed.loc[missing_indices, col] = np.round(\n",
    "                            imputed_df.loc[missing_indices, col].values\n",
    "                        )\n",
    "                    else:\n",
    "                        X_imputed.loc[missing_indices, col] = imputed_df.loc[missing_indices, col].values\n",
    "        \n",
    "        # Handle categorical columns (need to convert one-hot back to original categories)\n",
    "        for col in self.categorical_cols:\n",
    "            if col in X.columns:\n",
    "                missing_indices = X[col].isna()\n",
    "                \n",
    "                if missing_indices.any() and col in cat_indices:\n",
    "                    # Get the one-hot encoded columns for this categorical variable\n",
    "                    start_idx, end_idx = cat_indices[col]\n",
    "                    encoded_cols = self.processed_columns[start_idx:end_idx]\n",
    "                    \n",
    "                    # Get indices where values are missing\n",
    "                    missing_idx = missing_indices[missing_indices].index\n",
    "                    \n",
    "                    # Extract one-hot encoded values for the missing entries\n",
    "                    one_hot_values = imputed_df.loc[missing_idx, encoded_cols].values\n",
    "                    \n",
    "                    # Get the index of the max value along axis 1\n",
    "                    max_indices = np.argmax(one_hot_values, axis=1)\n",
    "                    \n",
    "                    # Map back to original categories\n",
    "                    categories = self.cat_encoders[col].categories_[0]\n",
    "                    imputed_categories = [categories[idx] for idx in max_indices]\n",
    "                    \n",
    "                    # Handle categorical data types safely\n",
    "                    if pd.api.types.is_categorical_dtype(X[col]):\n",
    "                        # Get the existing categories\n",
    "                        existing_cats = X[col].cat.categories\n",
    "                        \n",
    "                        # Filter out categories that aren't in the existing ones\n",
    "                        valid_categories = []\n",
    "                        for cat in imputed_categories:\n",
    "                            if cat in existing_cats:\n",
    "                                valid_categories.append(cat)\n",
    "                            else:\n",
    "                                # Use the most common category as fallback\n",
    "                                most_common = X[col].value_counts().index[0]\n",
    "                                valid_categories.append(most_common)\n",
    "                        \n",
    "                        # Update missing values\n",
    "                        for idx, cat in zip(missing_idx, valid_categories):\n",
    "                            X_imputed.loc[idx, col] = cat\n",
    "                    else:\n",
    "                        # Update missing values in the original DataFrame\n",
    "                        X_imputed.loc[missing_idx, col] = imputed_categories\n",
    "        \n",
    "        return X_imputed\n",
    "    \n",
    "    def fit_transform(self, X, patient_id_col, time_col=None, numerical_cols=None, \n",
    "                     categorical_cols=None, ordinal_cols=None):\n",
    "        \"\"\"\n",
    "        Fit the model and impute missing values\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X : DataFrame\n",
    "            Input data with missing values\n",
    "        patient_id_col : str\n",
    "            Name of the column containing patient IDs\n",
    "        time_col : str, optional\n",
    "            Name of the column containing time information\n",
    "        numerical_cols : list, optional\n",
    "            Names of numerical columns. If None, autodetect.\n",
    "        categorical_cols : list, optional\n",
    "            Names of categorical columns. If None, autodetect.\n",
    "        ordinal_cols : list, optional\n",
    "            Names of ordinal columns. If None, autodetect.\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        DataFrame\n",
    "            Data with imputed values\n",
    "        \"\"\"\n",
    "        self.fit(X, patient_id_col, time_col, numerical_cols, categorical_cols, ordinal_cols)\n",
    "        return self.transform(X)\n",
    "    \n",
    "    def plot_loss(self):\n",
    "        \"\"\"\n",
    "        Plot the training and validation loss curves\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        matplotlib figure\n",
    "            Loss curve plot\n",
    "        \"\"\"\n",
    "        if not self.history['train_loss']:\n",
    "            raise ValueError(\"Model has not been trained yet. Call fit() first.\")\n",
    "        \n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.plot(self.history['train_loss'], label='Training Loss')\n",
    "        plt.plot(self.history['val_loss'], label='Validation Loss')\n",
    "        plt.title('Deep Autoencoder Imputation Loss')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        return plt\n",
    "\n",
    "\n",
    "def apply_deep_autoencoder_imputation(df, columns_to_impute, validation_df=None, validation_masks=None, original_values=None, \n",
    "                                     hidden_layers=(64, 32, 16, 32, 64), latent_dim=8, batch_size=16, epochs=100, learning_rate=0.001):\n",
    "    \"\"\"\n",
    "    Apply deep autoencoder imputation to data with an interface matching your VAE implementation\n",
    "    \n",
    "        Parameters:\n",
    "    -----------\n",
    "    df : pandas.DataFrame\n",
    "        Data with missing values\n",
    "    columns_to_impute : list\n",
    "        List of column names to impute\n",
    "    validation_df : pandas.DataFrame, optional\n",
    "        Validation dataset with artificially missing values\n",
    "    validation_masks : dict, optional\n",
    "        Dictionary of masks for validation data\n",
    "    original_values : dict, optional\n",
    "        Dictionary of original values for validation\n",
    "    hidden_layers : tuple\n",
    "        Sizes of hidden layers\n",
    "    latent_dim : int\n",
    "        Dimension of the patient embedding (equivalent to latent_dim in VAE)\n",
    "    batch_size : int\n",
    "        Batch size for training\n",
    "    epochs : int\n",
    "        Number of training epochs\n",
    "    learning_rate : float\n",
    "        Learning rate for optimizer\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    imputed_df : pandas.DataFrame\n",
    "        Data with imputed values\n",
    "    validation_results : dict, optional\n",
    "        Validation results if validation data provided\n",
    "    \"\"\"\n",
    "    # Print column information\n",
    "    print(f\"Data has {len(df)} rows and {len(df.columns)} columns\")\n",
    "    print(f\"Columns to impute: {columns_to_impute}\")\n",
    "\n",
    "    # IMPORTANT: Make a copy of columns_to_impute to avoid modifying the original list\n",
    "    local_columns_to_impute = [col for col in columns_to_impute if col in df.columns]\n",
    "\n",
    "    # Remove any date-derived columns that might have been added from a previous run\n",
    "    date_derived_cols = [col for col in columns_to_impute if col.startswith('qol_date_')]\n",
    "    if date_derived_cols:\n",
    "        print(f\"Warning: Date-derived columns found in columns_to_impute: {date_derived_cols}\")\n",
    "        print(f\"These will not be used for imputation.\")\n",
    "        for col in date_derived_cols:\n",
    "            if col in local_columns_to_impute:\n",
    "                local_columns_to_impute.remove(col)\n",
    "\n",
    "    # Print missingness information\n",
    "    print(\"\\nMissingness rates in columns to impute:\")\n",
    "    for col in local_columns_to_impute:\n",
    "        if col in df.columns:\n",
    "            miss_rate = df[col].isna().mean() * 100\n",
    "            print(f\"{col}: {miss_rate:.2f}%\")\n",
    "        else:\n",
    "            print(f\"{col}: column not found in data\")\n",
    "\n",
    "    # Extract relevant columns including potential predictors\n",
    "    # Use all columns except those with excessive missing values\n",
    "    threshold = 0.5  # Columns with more than 50% missing values are excluded\n",
    "    columns_to_use = [col for col in df.columns \n",
    "                    if df[col].isna().mean() < threshold]\n",
    "\n",
    "    # Ensure all columns_to_impute are included\n",
    "    for col in local_columns_to_impute:\n",
    "        if col not in columns_to_use and col in df.columns:\n",
    "            columns_to_use.append(col)\n",
    "            \n",
    "    # Always include id and qol_date if available\n",
    "    id_col = 'id'\n",
    "    if id_col not in columns_to_use and id_col in df.columns:\n",
    "        columns_to_use.append(id_col)\n",
    "        \n",
    "    time_col = 'qol_date'\n",
    "    if time_col not in columns_to_use and time_col in df.columns:\n",
    "        columns_to_use.append(time_col)\n",
    "\n",
    "    print(f\"\\nUsing {len(columns_to_use)} columns for imputation model\")\n",
    "\n",
    "    # Extract subset of data\n",
    "    X = df[columns_to_use].copy()\n",
    "\n",
    "    # Ensure all columns to impute are numeric\n",
    "    print(\"\\nConverting columns to numeric...\")\n",
    "    with tqdm(local_columns_to_impute) as pbar:\n",
    "        for col in pbar:\n",
    "            pbar.set_description(f\"Processing {col}\")\n",
    "            if col in X.columns:\n",
    "                # Convert to numeric if possible\n",
    "                X[col] = pd.to_numeric(X[col], errors='coerce')\n",
    "\n",
    "    # Check CUDA availability\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    # Initialize deep autoencoder imputer\n",
    "    print(\"\\nInitializing deep autoencoder imputer...\")\n",
    "    imputer = DeepAutoencoderImputer(\n",
    "        embedding_dim=latent_dim,\n",
    "        hidden_layers=hidden_layers,\n",
    "        activation='elu',\n",
    "        dropout_rate=0.3,\n",
    "        learning_rate=learning_rate,\n",
    "        weight_decay=0.0001,\n",
    "        temporal_weight=0.4,\n",
    "        batch_size=batch_size,\n",
    "        epochs=epochs,\n",
    "        patience=20,\n",
    "        device=device,\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    # Store the columns to impute in the imputer object\n",
    "    imputer.columns_to_impute = local_columns_to_impute\n",
    "\n",
    "    # Fit and transform\n",
    "    print(\"\\nTraining deep autoencoder imputation model...\")\n",
    "    X_imputed = imputer.fit_transform(\n",
    "        X, \n",
    "        patient_id_col=id_col,  # Use 'id' as the patient ID column\n",
    "        time_col=\"qol_date\",  # Use qol_date for temporal information\n",
    "        numerical_cols=local_columns_to_impute  # Only specify columns to impute as numerical\n",
    "    )\n",
    "\n",
    "    # Create imputed dataframe\n",
    "    print(\"\\nCreating final imputed dataframe...\")\n",
    "    imputed_df = df.copy()\n",
    "\n",
    "    # Copy imputed values to the output dataframe\n",
    "    for col in local_columns_to_impute:\n",
    "        if col in X_imputed.columns and col in imputed_df.columns:\n",
    "            # Get mask for missing values in the original data\n",
    "            missing_mask = df[col].isna()\n",
    "            # Only update missing values\n",
    "            imputed_df.loc[missing_mask, col] = X_imputed.loc[missing_mask, col]\n",
    "\n",
    "    # Print post-imputation statistics\n",
    "    print(\"\\nPost-imputation statistics:\")\n",
    "    for col in local_columns_to_impute:\n",
    "        if col in imputed_df.columns:\n",
    "            # Count originally missing values that have been imputed\n",
    "            missing_before = df[col].isna().sum()\n",
    "            missing_after = imputed_df[col].isna().sum()\n",
    "            num_imputed = missing_before - missing_after\n",
    "            print(f\"{col}: {num_imputed} values imputed\")\n",
    "\n",
    "    # Validate if validation data provided\n",
    "    validation_results = None\n",
    "    if validation_df is not None and validation_masks is not None and original_values is not None:\n",
    "        validation_results = {}\n",
    "        \n",
    "        print(\"\\nPerforming validation on held-out data...\")\n",
    "        \n",
    "        # Extract validation data\n",
    "        X_val = validation_df[columns_to_use].copy()\n",
    "        \n",
    "        # Ensure all validation columns are numeric\n",
    "        for col in X_val.columns:\n",
    "            if col in local_columns_to_impute:\n",
    "                X_val[col] = pd.to_numeric(X_val[col], errors='coerce')\n",
    "            \n",
    "        # Impute validation data\n",
    "        print(\"Imputing validation data...\")\n",
    "        X_val_imputed = imputer.transform(X_val)\n",
    "        \n",
    "        # Compare imputed values to real values\n",
    "        print(\"Calculating validation metrics...\")\n",
    "        with tqdm(local_columns_to_impute, desc=\"Validating results\") as pbar:\n",
    "            for col in pbar:\n",
    "                pbar.set_description(f\"Validating {col}\")\n",
    "                # Get indices where values were artificially set to NaN\n",
    "                if col not in validation_masks:\n",
    "                    validation_results[col] = {\n",
    "                        'error': f\"Column {col} not found in validation masks\"\n",
    "                    }\n",
    "                    continue\n",
    "                    \n",
    "                mask = validation_masks[col] & validation_df[col].isna()\n",
    "                \n",
    "                if mask.sum() == 0:\n",
    "                    validation_results[col] = {\n",
    "                        'error': \"No artificially missing values\"\n",
    "                    }\n",
    "                    continue\n",
    "                \n",
    "                if col not in original_values:\n",
    "                    validation_results[col] = {\n",
    "                        'error': f\"Column {col} not found in original values\"\n",
    "                    }\n",
    "                    continue\n",
    "                    \n",
    "                real_vals = original_values[col][mask]\n",
    "                \n",
    "                if col not in X_val_imputed.columns:\n",
    "                    validation_results[col] = {\n",
    "                        'error': f\"Column {col} not found in imputed values\"\n",
    "                    }\n",
    "                    continue\n",
    "                    \n",
    "                imputed_vals = X_val_imputed.loc[mask, col]\n",
    "                \n",
    "                # Calculate continuous metrics (MAE and RMSE) - NO ROUNDING\n",
    "                mae = mean_absolute_error(real_vals, imputed_vals)\n",
    "                rmse = np.sqrt(mean_squared_error(real_vals, imputed_vals))\n",
    "                \n",
    "                # Calculate classification metrics - WITH ROUNDING\n",
    "                real_vals_class = process_for_classification(real_vals)\n",
    "                imputed_vals_class = process_for_classification(imputed_vals)\n",
    "                \n",
    "                classification_metrics = calculate_classification_metrics(real_vals_class, imputed_vals_class)\n",
    "                \n",
    "                validation_results[col] = {\n",
    "                    'mae': mae,\n",
    "                    'rmse': rmse,\n",
    "                    'accuracy': classification_metrics['accuracy'],\n",
    "                    'auc_multiclass': classification_metrics['auc_multiclass'],\n",
    "                    'avg_sensitivity': classification_metrics['avg_sensitivity'],\n",
    "                    'avg_specificity': classification_metrics['avg_specificity'],\n",
    "                    'avg_ppv': classification_metrics['avg_ppv'],\n",
    "                    'avg_npv': classification_metrics['avg_npv'],\n",
    "                    'precision_macro': classification_metrics['precision_macro'],\n",
    "                    'recall_macro': classification_metrics['recall_macro'],\n",
    "                    'real_distribution': real_vals.describe(),\n",
    "                    'imputed_distribution': pd.Series(imputed_vals).describe()\n",
    "                }\n",
    "                \n",
    "                # Update progress\n",
    "                pbar.set_postfix({\"MAE\": f\"{mae:.4f}\", \"RMSE\": f\"{rmse:.4f}\", \"Acc\": f\"{classification_metrics['accuracy']:.4f}\"})\n",
    "\n",
    "    print(f\"\\nTraining loss plot saved to deep_autoencoder_loss.png\")\n",
    "    print(\"\\nImputation complete!\")\n",
    "\n",
    "    return imputed_df, validation_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data has 5230 rows and 62 columns\n",
      "Columns to impute: ['ge1', 'ge2', 'ge3', 'ge4', 'ge5', 'ge6']\n",
      "\n",
      "Missingness rates in columns to impute:\n",
      "ge1: 79.89%\n",
      "ge2: 80.52%\n",
      "ge3: 81.24%\n",
      "ge4: 80.15%\n",
      "ge5: 80.21%\n",
      "ge6: 80.11%\n",
      "\n",
      "Using 11 columns for imputation model\n",
      "\n",
      "Converting columns to numeric...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing ge6: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:00<00:00, 2011.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Initializing deep autoencoder imputer..."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Using device: cpu\n",
      "\n",
      "Training deep autoencoder imputation model...\n",
      "Building model with input dimension 34 and 1754 unique patients\n",
      "Starting training for 100 epochs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/100 [00:52<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m columns_to_impute \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mge1\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mge2\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mge3\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mge4\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mge5\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mge6\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# Apply deep autoencoder imputation with explicit column specifications\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m imputed_df, validation_results \u001b[38;5;241m=\u001b[39m \u001b[43mapply_deep_autoencoder_imputation\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcolumns_to_impute\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolumns_to_impute\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[11], line 1073\u001b[0m, in \u001b[0;36mapply_deep_autoencoder_imputation\u001b[0;34m(df, columns_to_impute, validation_df, validation_masks, original_values, hidden_layers, latent_dim, batch_size, epochs, learning_rate)\u001b[0m\n\u001b[1;32m   1071\u001b[0m \u001b[38;5;66;03m# Fit and transform\u001b[39;00m\n\u001b[1;32m   1072\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mTraining deep autoencoder imputation model...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 1073\u001b[0m X_imputed \u001b[38;5;241m=\u001b[39m \u001b[43mimputer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1074\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m   1075\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpatient_id_col\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mid_col\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Use 'id' as the patient ID column\u001b[39;49;00m\n\u001b[1;32m   1076\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtime_col\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mqol_date\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Use qol_date for temporal information\u001b[39;49;00m\n\u001b[1;32m   1077\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnumerical_cols\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_columns_to_impute\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Only specify columns to impute as numerical\u001b[39;49;00m\n\u001b[1;32m   1078\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1080\u001b[0m \u001b[38;5;66;03m# Create imputed dataframe\u001b[39;00m\n\u001b[1;32m   1081\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mCreating final imputed dataframe...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[11], line 928\u001b[0m, in \u001b[0;36mDeepAutoencoderImputer.fit_transform\u001b[0;34m(self, X, patient_id_col, time_col, numerical_cols, categorical_cols, ordinal_cols)\u001b[0m\n\u001b[1;32m    903\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mfit_transform\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, patient_id_col, time_col\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, numerical_cols\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \n\u001b[1;32m    904\u001b[0m                  categorical_cols\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, ordinal_cols\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    905\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    906\u001b[0m \u001b[38;5;124;03m    Fit the model and impute missing values\u001b[39;00m\n\u001b[1;32m    907\u001b[0m \u001b[38;5;124;03m    \u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    926\u001b[0m \u001b[38;5;124;03m        Data with imputed values\u001b[39;00m\n\u001b[1;32m    927\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 928\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpatient_id_col\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtime_col\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnumerical_cols\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcategorical_cols\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mordinal_cols\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    929\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform(X)\n",
      "Cell \u001b[0;32mIn[11], line 418\u001b[0m, in \u001b[0;36mDeepAutoencoderImputer.fit\u001b[0;34m(self, X, patient_id_col, time_col, numerical_cols, categorical_cols, ordinal_cols)\u001b[0m\n\u001b[1;32m    415\u001b[0m patient_ids \u001b[38;5;241m=\u001b[39m X[patient_id_col]\u001b[38;5;241m.\u001b[39mvalues\n\u001b[1;32m    417\u001b[0m \u001b[38;5;66;03m# Train model\u001b[39;00m\n\u001b[0;32m--> 418\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_train_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_processed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpatient_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmissing_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    420\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "Cell \u001b[0;32mIn[11], line 687\u001b[0m, in \u001b[0;36mDeepAutoencoderImputer._train_model\u001b[0;34m(self, X_processed, patient_ids, missing_mask)\u001b[0m\n\u001b[1;32m    684\u001b[0m patient_idx_batch \u001b[38;5;241m=\u001b[39m patient_idx_batch\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m    686\u001b[0m \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[0;32m--> 687\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpatient_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    689\u001b[0m \u001b[38;5;66;03m# Calculate loss\u001b[39;00m\n\u001b[1;32m    690\u001b[0m recon_loss \u001b[38;5;241m=\u001b[39m masked_loss_fn(output, X_batch, mask_batch)\n",
      "File \u001b[0;32m~/jinenv/lib/python3.11/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/jinenv/lib/python3.11/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[11], line 151\u001b[0m, in \u001b[0;36mDeepAutoencoderModel.forward\u001b[0;34m(self, x, patient_ids)\u001b[0m\n\u001b[1;32m    148\u001b[0m patient_emb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpatient_embedding(patient_ids)\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    150\u001b[0m \u001b[38;5;66;03m# Encode input\u001b[39;00m\n\u001b[0;32m--> 151\u001b[0m code \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    153\u001b[0m \u001b[38;5;66;03m# Decode latent representation\u001b[39;00m\n\u001b[1;32m    154\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecode(code, patient_emb)\n",
      "Cell \u001b[0;32mIn[11], line 104\u001b[0m, in \u001b[0;36mDeepAutoencoderModel.encode\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    102\u001b[0m h \u001b[38;5;241m=\u001b[39m x\n\u001b[1;32m    103\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder_layers:\n\u001b[0;32m--> 104\u001b[0m     h \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mh\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    105\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m h\n",
      "File \u001b[0;32m~/jinenv/lib/python3.11/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/jinenv/lib/python3.11/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/jinenv/lib/python3.11/site-packages/torch/nn/modules/container.py:250\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    248\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    249\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 250\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    251\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/jinenv/lib/python3.11/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/jinenv/lib/python3.11/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/jinenv/lib/python3.11/site-packages/torch/nn/modules/batchnorm.py:193\u001b[0m, in \u001b[0;36m_BatchNorm.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    186\u001b[0m     bn_training \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrunning_mean \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrunning_var \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    188\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    189\u001b[0m \u001b[38;5;124;03mBuffers are only updated if they are to be tracked and we are in training mode. Thus they only need to be\u001b[39;00m\n\u001b[1;32m    190\u001b[0m \u001b[38;5;124;03mpassed when the update should occur (i.e. in training mode when they are tracked), or when buffer stats are\u001b[39;00m\n\u001b[1;32m    191\u001b[0m \u001b[38;5;124;03mused for normalization (i.e. in eval mode when buffers are not None).\u001b[39;00m\n\u001b[1;32m    192\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m--> 193\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_norm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    194\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    195\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# If buffers are not to be tracked, ensure that they won't be updated\u001b[39;49;00m\n\u001b[1;32m    196\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrunning_mean\u001b[49m\n\u001b[1;32m    197\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrack_running_stats\u001b[49m\n\u001b[1;32m    198\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    199\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrunning_var\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrack_running_stats\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    200\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    201\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbn_training\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    203\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexponential_average_factor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    204\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    205\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/jinenv/lib/python3.11/site-packages/torch/nn/functional.py:2822\u001b[0m, in \u001b[0;36mbatch_norm\u001b[0;34m(input, running_mean, running_var, weight, bias, training, momentum, eps)\u001b[0m\n\u001b[1;32m   2819\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m training:\n\u001b[1;32m   2820\u001b[0m     _verify_batch_size(\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize())\n\u001b[0;32m-> 2822\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_norm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2823\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2824\u001b[0m \u001b[43m    \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2825\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2826\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrunning_mean\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2827\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrunning_var\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2828\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2829\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmomentum\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2830\u001b[0m \u001b[43m    \u001b[49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2831\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackends\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcudnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menabled\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2832\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "ordinal_cols = (\n",
    "    [f\"gp{i}\" for i in range(1, 8)] + [f\"gs{i}\" for i in range(1, 8)] +\n",
    "    [f\"ge{i}\" for i in range(1, 7)] + [f\"gf{i}\" for i in range(1, 8)] +\n",
    "    [f\"a_hn{i}\" for i in range(1, 6)] + [\"a_hn7\", \"a_hn10\"] +\n",
    "    [f\"a_e{i}\" for i in range(1, 8)] + [\"a_c6\", \"a_c2\", \"a_act11\"]\n",
    ")\n",
    "columns_to_impute = ordinal_cols#['ge1', 'ge2', 'ge3', 'ge4', 'ge5', 'ge6']\n",
    "# Apply deep autoencoder imputation with explicit column specifications\n",
    "imputed_df, validation_results = apply_deep_autoencoder_imputation(\n",
    "    df=df, \n",
    "    columns_to_impute=columns_to_impute\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model - Softimpute"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "https://github.com/travisbrady/py-soft-impute/tree/master"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add Travis Brady's SoftImpute implementation to your code\n",
    "def frob(Uold, Dsqold, Vold, U, Dsq, V):\n",
    "    denom = (Dsqold ** 2).sum()\n",
    "    utu = Dsq * (U.T.dot(Uold))\n",
    "    vtv = Dsqold * (Vold.T.dot(V))\n",
    "    uvprod = utu.dot(vtv).diagonal().sum()\n",
    "    num = denom + (Dsqold ** 2).sum() - 2*uvprod\n",
    "    return num / max(denom, 1e-9)\n",
    "\n",
    "\n",
    "class SoftImpute:\n",
    "    def __init__(self, J=2, thresh=1e-05, lambda_=0, maxit=100, random_state=None, verbose=False):\n",
    "        self.J = J\n",
    "        self.thresh = thresh\n",
    "        self.lambda_ = lambda_\n",
    "        self.maxit = maxit\n",
    "        self.rs = np.random.RandomState(random_state)\n",
    "        self.verbose = verbose\n",
    "        self.u = None\n",
    "        self.d = None\n",
    "        self.v = None\n",
    "\n",
    "    def fit(self, X):\n",
    "        n, m = X.shape\n",
    "        xnas = np.isnan(X)\n",
    "        nz = m*n - xnas.sum()\n",
    "        xfill = X.copy()\n",
    "        V = np.zeros((m, self.J))\n",
    "        U = self.rs.normal(0.0, 1.0, (n, self.J))\n",
    "        U, _, _ = np.linalg.svd(U, full_matrices=False)\n",
    "        Dsq = np.ones((self.J, 1))\n",
    "        #xfill[xnas] = 0.0\n",
    "        col_means = np.nanmean(xfill, axis=0)\n",
    "        np.copyto(xfill, col_means, where=np.isnan(xfill))\n",
    "        ratio = 1.0\n",
    "        iters = 0\n",
    "        while ratio > self.thresh and iters < self.maxit:\n",
    "            iters += 1\n",
    "            U_old = U\n",
    "            V_old = V\n",
    "            Dsq_old = Dsq\n",
    "            B = U.T.dot(xfill)\n",
    "            if self.lambda_ > 0:\n",
    "                tmp = (Dsq / (Dsq + self.lambda_))\n",
    "                B = B * tmp\n",
    "            Bsvd = np.linalg.svd(B.T, full_matrices=False)\n",
    "            V = Bsvd[0]\n",
    "            Dsq = Bsvd[1][:, np.newaxis]\n",
    "            U = U.dot(Bsvd[2])\n",
    "            tmp = Dsq * V.T\n",
    "            xhat = U.dot(tmp)\n",
    "            xfill[xnas] = xhat[xnas]\n",
    "            A = xfill.dot(V).T\n",
    "            Asvd = np.linalg.svd(A.T, full_matrices=False)\n",
    "            U = Asvd[0]\n",
    "            Dsq = Asvd[1][:, np.newaxis]\n",
    "            V = V.dot(Asvd[2])\n",
    "            tmp = Dsq * V.T\n",
    "            xhat = U.dot(tmp)\n",
    "            xfill[xnas] = xhat[xnas]\n",
    "            ratio = frob(U_old, Dsq_old, V_old, U, Dsq, V)\n",
    "            if self.verbose:\n",
    "                print('iter: %4d ratio = %.5f' % (iters, ratio))\n",
    "        self.u = U[:, :self.J]\n",
    "        self.d = Dsq[:self.J]\n",
    "        self.v = V[:, :self.J]\n",
    "        return self\n",
    "\n",
    "    def suv(self, vd):\n",
    "        res = self.u.dot(vd.T)\n",
    "        return res\n",
    "\n",
    "    def predict(self, X, copyto=False):\n",
    "        vd = self.v * np.outer(np.ones(self.v.shape[0]), self.d)\n",
    "        X_imp = self.suv(vd)\n",
    "        if copyto:\n",
    "            np.copyto(X, X_imp, where=np.isnan(X))\n",
    "        else:\n",
    "            return X_imp\n",
    "\n",
    "\n",
    "def apply_softimpute_imputation(df, columns_to_impute, validation_df=None, validation_masks=None, original_values=None,\n",
    "                               J=None, thresh=1e-05, lambda_=0, maxit=100, random_state=42):\n",
    "    \"\"\"\n",
    "    Apply SoftImpute imputation using Travis Brady's implementation\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pandas.DataFrame\n",
    "        Data with missing values\n",
    "    columns_to_impute : list\n",
    "        List of column names to impute\n",
    "    validation_df : pandas.DataFrame, optional\n",
    "        Validation dataset with artificially missing values\n",
    "    validation_masks : dict, optional\n",
    "        Dictionary of masks for validation data\n",
    "    original_values : dict, optional\n",
    "        Dictionary of original values for validation\n",
    "    J : int, optional\n",
    "        Number of factors/rank. If None, will be estimated\n",
    "    thresh : float\n",
    "        Convergence threshold\n",
    "    lambda_ : float\n",
    "        Regularization parameter\n",
    "    maxit : int\n",
    "        Maximum number of iterations\n",
    "    random_state : int\n",
    "        Random seed for reproducibility\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    imputed_df : pandas.DataFrame\n",
    "        Data with imputed values\n",
    "    validation_results : dict, optional\n",
    "        Validation results if validation data provided\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Extract relevant columns including potential predictors\n",
    "        # Use all columns except those with excessive missing values\n",
    "        threshold = 0.5  # Columns with more than 50% missing values are excluded\n",
    "        columns_to_use = [col for col in df.columns \n",
    "                            if df[col].isna().mean() < threshold]\n",
    "        \n",
    "        # Ensure all columns_to_impute are included\n",
    "        for col in columns_to_impute:\n",
    "            if col not in columns_to_use:\n",
    "                columns_to_use.append(col)\n",
    "        \n",
    "        # Extract subset of data\n",
    "        X = df[columns_to_use].copy()\n",
    "        \n",
    "        # Ensure all columns are numeric\n",
    "        for col in X.columns:\n",
    "            X[col] = pd.to_numeric(X[col], errors='coerce')\n",
    "        \n",
    "        print(f\"SoftImpute (Travis Brady): Using {len(columns_to_use)} columns, imputing {len(columns_to_impute)} columns\")\n",
    "        \n",
    "        # Set J (rank) if not provided\n",
    "        if J is None:\n",
    "            J = min(X.shape[0], X.shape[1]) // 4\n",
    "            J = max(2, J)  # Ensure at least rank 2\n",
    "        \n",
    "        print(f\"SoftImpute parameters: J={J}, thresh={thresh}, lambda_={lambda_}, maxit={maxit}\")\n",
    "        \n",
    "        # Scale the data for better numerical stability\n",
    "        scaler = StandardScaler()\n",
    "        \n",
    "        # Initial imputation with column means for scaling\n",
    "        X_for_scaling = X.copy()\n",
    "        for col in X_for_scaling.columns:\n",
    "            X_for_scaling[col] = X_for_scaling[col].fillna(X_for_scaling[col].mean())\n",
    "        \n",
    "        # Fit scaler and transform\n",
    "        X_scaled = pd.DataFrame(\n",
    "            scaler.fit_transform(X_for_scaling),\n",
    "            columns=X.columns,\n",
    "            index=X.index\n",
    "        )\n",
    "        \n",
    "        # Restore NaN values in scaled data\n",
    "        X_scaled[X.isna()] = np.nan\n",
    "        \n",
    "        # Initialize SoftImpute\n",
    "        imputer = SoftImpute(\n",
    "            J=J,\n",
    "            thresh=thresh,\n",
    "            lambda_=lambda_,\n",
    "            maxit=maxit,\n",
    "            random_state=random_state,\n",
    "            verbose=True\n",
    "        )\n",
    "        \n",
    "        # Fit and predict\n",
    "        print(\"Training SoftImpute imputation model...\")\n",
    "        imputer.fit(X_scaled.values)\n",
    "        X_imputed_scaled = imputer.predict(X_scaled.values)\n",
    "        \n",
    "        # Inverse transform to original scale\n",
    "        X_imputed = pd.DataFrame(\n",
    "            scaler.inverse_transform(X_imputed_scaled),\n",
    "            columns=X.columns,\n",
    "            index=X.index\n",
    "        )\n",
    "        \n",
    "        # Create imputed dataframe - only replace missing values\n",
    "        imputed_df = df.copy()\n",
    "        for col in columns_to_impute:\n",
    "            if col in X_imputed.columns:\n",
    "                missing_mask = df[col].isna()\n",
    "                imputed_df.loc[missing_mask, col] = X_imputed.loc[missing_mask, col]\n",
    "        \n",
    "        # Validate if validation data provided\n",
    "        validation_results = None\n",
    "        if validation_df is not None and validation_masks is not None and original_values is not None:\n",
    "            validation_results = {}\n",
    "            \n",
    "            # Extract validation data\n",
    "            X_val = validation_df[columns_to_use].copy()\n",
    "            \n",
    "            # Ensure all validation columns are numeric\n",
    "            for col in X_val.columns:\n",
    "                X_val[col] = pd.to_numeric(X_val[col], errors='coerce')\n",
    "                \n",
    "            # Scale validation data\n",
    "            X_val_for_scaling = X_val.copy()\n",
    "            for col in X_val_for_scaling.columns:\n",
    "                X_val_for_scaling[col] = X_val_for_scaling[col].fillna(X_val_for_scaling[col].mean())\n",
    "            \n",
    "            X_val_scaled = pd.DataFrame(\n",
    "                scaler.transform(X_val_for_scaling),\n",
    "                columns=X_val.columns,\n",
    "                index=X_val.index\n",
    "            )\n",
    "            \n",
    "            # Restore NaN values\n",
    "            X_val_scaled[X_val.isna()] = np.nan\n",
    "            \n",
    "            # Create a new imputer for validation data\n",
    "            val_imputer = SoftImpute(\n",
    "                J=J,\n",
    "                thresh=thresh,\n",
    "                lambda_=lambda_,\n",
    "                maxit=maxit,\n",
    "                random_state=random_state,\n",
    "                verbose=False  # Less verbose for validation\n",
    "            )\n",
    "            \n",
    "            # Impute validation data\n",
    "            print(\"Imputing validation data...\")\n",
    "            val_imputer.fit(X_val_scaled.values)\n",
    "            X_val_imputed_scaled = val_imputer.predict(X_val_scaled.values)\n",
    "            \n",
    "            # Inverse transform\n",
    "            X_val_imputed = pd.DataFrame(\n",
    "                scaler.inverse_transform(X_val_imputed_scaled),\n",
    "                columns=X_val.columns,\n",
    "                index=X_val.index\n",
    "            )\n",
    "            \n",
    "            # Compare imputed values to real values\n",
    "            with tqdm(columns_to_impute, desc=\"Validating results\") as pbar:\n",
    "                for col in pbar:\n",
    "                    pbar.set_description(f\"Validating {col}\")\n",
    "                    # Get indices where values were artificially set to NaN\n",
    "                    mask = validation_masks[col] & validation_df[col].isna()\n",
    "                    \n",
    "                    if mask.sum() == 0:\n",
    "                        validation_results[col] = {\n",
    "                            'error': \"No artificially missing values\"\n",
    "                        }\n",
    "                        continue\n",
    "                        \n",
    "                    real_vals = original_values[col][mask]\n",
    "                    imputed_vals = X_val_imputed.loc[mask, col]\n",
    "                    \n",
    "                    # Calculate continuous metrics (MAE and RMSE) - NO ROUNDING\n",
    "                    mae = mean_absolute_error(real_vals, imputed_vals)\n",
    "                    rmse = np.sqrt(mean_squared_error(real_vals, imputed_vals))\n",
    "                    \n",
    "                    # Calculate classification metrics - WITH ROUNDING\n",
    "                    real_vals_class = process_for_classification(real_vals)\n",
    "                    imputed_vals_class = process_for_classification(imputed_vals)\n",
    "                    \n",
    "                    classification_metrics = calculate_classification_metrics(real_vals_class, imputed_vals_class)\n",
    "                    \n",
    "                    validation_results[col] = {\n",
    "                        'mae': mae,\n",
    "                        'rmse': rmse,\n",
    "                        'accuracy': classification_metrics['accuracy'],\n",
    "                        'auc_multiclass': classification_metrics['auc_multiclass'],\n",
    "                        'avg_sensitivity': classification_metrics['avg_sensitivity'],\n",
    "                        'avg_specificity': classification_metrics['avg_specificity'],\n",
    "                        'avg_ppv': classification_metrics['avg_ppv'],\n",
    "                        'avg_npv': classification_metrics['avg_npv'],\n",
    "                        'precision_macro': classification_metrics['precision_macro'],\n",
    "                        'recall_macro': classification_metrics['recall_macro'],\n",
    "                        'real_distribution': real_vals.describe(),\n",
    "                        'imputed_distribution': pd.Series(imputed_vals).describe()\n",
    "                    }\n",
    "                    \n",
    "                    # Update progress\n",
    "                    pbar.set_postfix({\"MAE\": f\"{mae:.4f}\", \"RMSE\": f\"{rmse:.4f}\", \"Acc\": f\"{classification_metrics['accuracy']:.4f}\"})\n",
    "        \n",
    "        return imputed_df, validation_results\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error in SoftImpute imputation: {str(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        \n",
    "        # Fallback to simple mean imputation\n",
    "        print(\"Falling back to simple mean imputation\")\n",
    "        result_df = df.copy()\n",
    "        for col in columns_to_impute:\n",
    "            if col in result_df.columns:\n",
    "                result_df[col] = pd.to_numeric(result_df[col], errors='coerce')\n",
    "                result_df[col] = result_df[col].fillna(result_df[col].mean())\n",
    "        return result_df, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SoftImpute (Travis Brady): Using 48 columns, imputing 44 columns\n",
      "SoftImpute parameters: J=12, thresh=1e-05, lambda_=0, maxit=100\n",
      "Training SoftImpute imputation model...\n",
      "iter:    1 ratio = 2.00000\n",
      "iter:    2 ratio = 1.17932\n",
      "iter:    3 ratio = 1.15715\n",
      "iter:    4 ratio = 1.03011\n",
      "iter:    5 ratio = 0.48539\n",
      "iter:    6 ratio = 0.30640\n",
      "iter:    7 ratio = 0.22492\n",
      "iter:    8 ratio = 0.47394\n",
      "iter:    9 ratio = 0.48334\n",
      "iter:   10 ratio = 0.35093\n",
      "iter:   11 ratio = 0.39443\n",
      "iter:   12 ratio = 0.31349\n",
      "iter:   13 ratio = 0.27191\n",
      "iter:   14 ratio = 0.04629\n",
      "iter:   15 ratio = 0.00582\n",
      "iter:   16 ratio = 0.02698\n",
      "iter:   17 ratio = 0.15518\n",
      "iter:   18 ratio = 0.44253\n",
      "iter:   19 ratio = 0.31798\n",
      "iter:   20 ratio = 0.20299\n",
      "iter:   21 ratio = 0.17513\n",
      "iter:   22 ratio = 0.12604\n",
      "iter:   23 ratio = 0.07183\n",
      "iter:   24 ratio = 0.13206\n",
      "iter:   25 ratio = 0.06586\n",
      "iter:   26 ratio = 0.05695\n",
      "iter:   27 ratio = 0.03083\n",
      "iter:   28 ratio = 0.08093\n",
      "iter:   29 ratio = 0.10034\n",
      "iter:   30 ratio = 0.06728\n",
      "iter:   31 ratio = 0.03964\n",
      "iter:   32 ratio = 0.06495\n",
      "iter:   33 ratio = 0.05839\n",
      "iter:   34 ratio = 0.00530\n",
      "iter:   35 ratio = -0.00213\n"
     ]
    }
   ],
   "source": [
    "ordinal_cols = (\n",
    "    [f\"gp{i}\" for i in range(1, 8)] + [f\"gs{i}\" for i in range(1, 8)] +\n",
    "    [f\"ge{i}\" for i in range(1, 7)] + [f\"gf{i}\" for i in range(1, 8)] +\n",
    "    [f\"a_hn{i}\" for i in range(1, 6)] + [\"a_hn7\", \"a_hn10\"] +\n",
    "    [f\"a_e{i}\" for i in range(1, 8)] + [\"a_c6\", \"a_c2\", \"a_act11\"]\n",
    ")\n",
    "columns_to_impute = ordinal_cols#['ge1', 'ge2', 'ge3', 'ge4', 'ge5', 'ge6']\n",
    "# Apply deep autoencoder imputation with explicit column specifications\n",
    "imputed_df, validation_results = apply_softimpute_imputation(\n",
    "    df=df, \n",
    "    columns_to_impute=columns_to_impute\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model - KNN imputing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add this import at the top of your file\n",
    "from sklearn.impute import KNNImputer\n",
    "\n",
    "\n",
    "def apply_knn_imputation(df, columns_to_impute, validation_df=None, validation_masks=None, original_values=None,\n",
    "                        n_neighbors=5, weights='uniform', metric='nan_euclidean'):\n",
    "    \"\"\"\n",
    "    Apply KNN imputation using scikit-learn's KNNImputer\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pandas.DataFrame\n",
    "        Data with missing values\n",
    "    columns_to_impute : list\n",
    "        List of column names to impute\n",
    "    validation_df : pandas.DataFrame, optional\n",
    "        Validation dataset with artificially missing values\n",
    "    validation_masks : dict, optional\n",
    "        Dictionary of masks for validation data\n",
    "    original_values : dict, optional\n",
    "        Dictionary of original values for validation\n",
    "    n_neighbors : int\n",
    "        Number of neighboring samples to use for imputation\n",
    "    weights : str or callable\n",
    "        Weight function used in prediction. Possible values:\n",
    "        - 'uniform': uniform weights (default)\n",
    "        - 'distance': weight points by the inverse of their distance\n",
    "    metric : str\n",
    "        Distance metric for searching neighbors. Possible values:\n",
    "        - 'nan_euclidean': euclidean distance ignoring NaNs (default)\n",
    "        - 'euclidean': standard euclidean distance\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    imputed_df : pandas.DataFrame\n",
    "        Data with imputed values\n",
    "    validation_results : dict, optional\n",
    "        Validation results if validation data provided\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Extract relevant columns including potential predictors\n",
    "        # Use all columns except those with excessive missing values\n",
    "        threshold = 0.5  # Columns with more than 50% missing values are excluded\n",
    "        columns_to_use = [col for col in df.columns \n",
    "                            if df[col].isna().mean() < threshold]\n",
    "        \n",
    "        # Ensure all columns_to_impute are included\n",
    "        for col in columns_to_impute:\n",
    "            if col not in columns_to_use:\n",
    "                columns_to_use.append(col)\n",
    "        \n",
    "        # Extract subset of data\n",
    "        X = df[columns_to_use].copy()\n",
    "        \n",
    "        # Ensure all columns are numeric\n",
    "        for col in X.columns:\n",
    "            X[col] = pd.to_numeric(X[col], errors='coerce')\n",
    "        \n",
    "        print(f\"KNN Imputation: Using {len(columns_to_use)} columns, imputing {len(columns_to_impute)} columns\")\n",
    "        \n",
    "        # Adjust n_neighbors based on available data\n",
    "        # For each column to impute, check how many complete cases we have\n",
    "        min_complete_cases = float('inf')\n",
    "        for col in columns_to_impute:\n",
    "            if col in X.columns:\n",
    "                # Count rows where this column has data and at least one other column has data\n",
    "                col_observed = X[col].notna()\n",
    "                other_cols = [c for c in X.columns if c != col]\n",
    "                if other_cols:\n",
    "                    other_observed = X[other_cols].notna().any(axis=1)\n",
    "                    complete_cases = (col_observed & other_observed).sum()\n",
    "                else:\n",
    "                    complete_cases = col_observed.sum()\n",
    "                min_complete_cases = min(min_complete_cases, complete_cases)\n",
    "        \n",
    "        # Adjust n_neighbors to be at most the number of complete cases - 1\n",
    "        if min_complete_cases != float('inf') and min_complete_cases > 0:\n",
    "            n_neighbors = min(n_neighbors, max(1, min_complete_cases - 1))\n",
    "        \n",
    "        print(f\"KNN parameters: n_neighbors={n_neighbors}, weights='{weights}', metric='{metric}'\")\n",
    "        \n",
    "        # Initialize KNNImputer\n",
    "        imputer = KNNImputer(\n",
    "            n_neighbors=n_neighbors,\n",
    "            weights=weights,\n",
    "            metric=metric,\n",
    "            keep_empty_features=True  # Keep features that are all NaN\n",
    "        )\n",
    "        \n",
    "        # Fit and transform\n",
    "        print(\"Training KNN imputation model...\")\n",
    "        X_imputed_array = imputer.fit_transform(X.values)\n",
    "        \n",
    "        # Convert back to DataFrame\n",
    "        X_imputed = pd.DataFrame(X_imputed_array, columns=X.columns, index=X.index)\n",
    "        \n",
    "        # Create imputed dataframe - only replace missing values\n",
    "        imputed_df = df.copy()\n",
    "        for col in columns_to_impute:\n",
    "            if col in X_imputed.columns:\n",
    "                missing_mask = df[col].isna()\n",
    "                imputed_df.loc[missing_mask, col] = X_imputed.loc[missing_mask, col]\n",
    "        \n",
    "        # Print imputation statistics\n",
    "        print(\"KNN Imputation completed successfully!\")\n",
    "        for col in columns_to_impute:\n",
    "            if col in df.columns:\n",
    "                original_missing = df[col].isna().sum()\n",
    "                final_missing = imputed_df[col].isna().sum()\n",
    "                imputed_count = original_missing - final_missing\n",
    "                print(f\"  {col}: {imputed_count} values imputed\")\n",
    "        \n",
    "        # Validate if validation data provided\n",
    "        validation_results = None\n",
    "        if validation_df is not None and validation_masks is not None and original_values is not None:\n",
    "            validation_results = {}\n",
    "            \n",
    "            # Extract validation data\n",
    "            X_val = validation_df[columns_to_use].copy()\n",
    "            \n",
    "            # Ensure all validation columns are numeric\n",
    "            for col in X_val.columns:\n",
    "                X_val[col] = pd.to_numeric(X_val[col], errors='coerce')\n",
    "            \n",
    "            # Check validation data for n_neighbors adjustment\n",
    "            val_min_complete_cases = float('inf')\n",
    "            for col in columns_to_impute:\n",
    "                if col in X_val.columns:\n",
    "                    col_observed = X_val[col].notna()\n",
    "                    other_cols = [c for c in X_val.columns if c != col]\n",
    "                    if other_cols:\n",
    "                        other_observed = X_val[other_cols].notna().any(axis=1)\n",
    "                        complete_cases = (col_observed & other_observed).sum()\n",
    "                    else:\n",
    "                        complete_cases = col_observed.sum()\n",
    "                    val_min_complete_cases = min(val_min_complete_cases, complete_cases)\n",
    "            \n",
    "            # Adjust n_neighbors for validation\n",
    "            val_n_neighbors = n_neighbors\n",
    "            if val_min_complete_cases != float('inf') and val_min_complete_cases > 0:\n",
    "                val_n_neighbors = min(n_neighbors, max(1, val_min_complete_cases - 1))\n",
    "            \n",
    "            # Create a new imputer for validation data\n",
    "            val_imputer = KNNImputer(\n",
    "                n_neighbors=val_n_neighbors,\n",
    "                weights=weights,\n",
    "                metric=metric,\n",
    "                keep_empty_features=True\n",
    "            )\n",
    "            \n",
    "            # Impute validation data\n",
    "            print(\"Imputing validation data...\")\n",
    "            X_val_imputed_array = val_imputer.fit_transform(X_val.values)\n",
    "            X_val_imputed = pd.DataFrame(X_val_imputed_array, columns=X_val.columns, index=X_val.index)\n",
    "            \n",
    "            # Compare imputed values to real values\n",
    "            with tqdm(columns_to_impute, desc=\"Validating results\") as pbar:\n",
    "                for col in pbar:\n",
    "                    pbar.set_description(f\"Validating {col}\")\n",
    "                    # Get indices where values were artificially set to NaN\n",
    "                    mask = validation_masks[col] & validation_df[col].isna()\n",
    "                    \n",
    "                    if mask.sum() == 0:\n",
    "                        validation_results[col] = {\n",
    "                            'error': \"No artificially missing values\"\n",
    "                        }\n",
    "                        continue\n",
    "                        \n",
    "                    real_vals = original_values[col][mask]\n",
    "                    imputed_vals = X_val_imputed.loc[mask, col]\n",
    "                    \n",
    "                    # Calculate MAE and RMSE\n",
    "                    mae = mean_absolute_error(real_vals, imputed_vals)\n",
    "                    rmse = np.sqrt(mean_squared_error(real_vals, imputed_vals))\n",
    "                    \n",
    "                    # Calculate classification metrics - WITH ROUNDING\n",
    "                    real_vals_class = process_for_classification(real_vals)\n",
    "                    imputed_vals_class = process_for_classification(imputed_vals)\n",
    "                    \n",
    "                    classification_metrics = calculate_classification_metrics(real_vals_class, imputed_vals_class)\n",
    "                    \n",
    "                    validation_results[col] = {\n",
    "                        'mae': mae,\n",
    "                        'rmse': rmse,\n",
    "                        'accuracy': classification_metrics['accuracy'],\n",
    "                        'auc_multiclass': classification_metrics['auc_multiclass'],\n",
    "                        'avg_sensitivity': classification_metrics['avg_sensitivity'],\n",
    "                        'avg_specificity': classification_metrics['avg_specificity'],\n",
    "                        'avg_ppv': classification_metrics['avg_ppv'],\n",
    "                        'avg_npv': classification_metrics['avg_npv'],\n",
    "                        'precision_macro': classification_metrics['precision_macro'],\n",
    "                        'recall_macro': classification_metrics['recall_macro'],\n",
    "                        'real_distribution': real_vals.describe(),\n",
    "                        'imputed_distribution': pd.Series(imputed_vals).describe()\n",
    "                    }\n",
    "                    \n",
    "                    # Update progress\n",
    "                    pbar.set_postfix({\"MAE\": f\"{mae:.4f}\", \"RMSE\": f\"{rmse:.4f}\", \"Acc\": f\"{classification_metrics['accuracy']:.4f}\"})\n",
    "        \n",
    "        return imputed_df, validation_results\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error in KNN imputation: {str(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        \n",
    "        # Fallback to simple mean imputation\n",
    "        print(\"Falling back to simple mean imputation\")\n",
    "        result_df = df.copy()\n",
    "        for col in columns_to_impute:\n",
    "            if col in result_df.columns:\n",
    "                result_df[col] = pd.to_numeric(result_df[col], errors='coerce')\n",
    "                result_df[col] = result_df[col].fillna(result_df[col].mean())\n",
    "        return result_df, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN Imputation: Using 48 columns, imputing 44 columns\n",
      "KNN parameters: n_neighbors=5, weights='uniform', metric='nan_euclidean'\n",
      "Training KNN imputation model...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[38], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m columns_to_impute \u001b[38;5;241m=\u001b[39m ordinal_cols\u001b[38;5;66;03m#['ge1', 'ge2', 'ge3', 'ge4', 'ge5', 'ge6']\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Apply deep autoencoder imputation with explicit column specifications\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m imputed_df, validation_results \u001b[38;5;241m=\u001b[39m \u001b[43mapply_knn_imputation\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcolumns_to_impute\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolumns_to_impute\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[37], line 92\u001b[0m, in \u001b[0;36mapply_knn_imputation\u001b[0;34m(df, columns_to_impute, validation_df, validation_masks, original_values, n_neighbors, weights, metric)\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;66;03m# Fit and transform\u001b[39;00m\n\u001b[1;32m     91\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining KNN imputation model...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 92\u001b[0m X_imputed_array \u001b[38;5;241m=\u001b[39m \u001b[43mimputer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     94\u001b[0m \u001b[38;5;66;03m# Convert back to DataFrame\u001b[39;00m\n\u001b[1;32m     95\u001b[0m X_imputed \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(X_imputed_array, columns\u001b[38;5;241m=\u001b[39mX\u001b[38;5;241m.\u001b[39mcolumns, index\u001b[38;5;241m=\u001b[39mX\u001b[38;5;241m.\u001b[39mindex)\n",
      "File \u001b[0;32m~/jinenv/lib/python3.11/site-packages/sklearn/utils/_set_output.py:319\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[0;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[1;32m    318\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 319\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    320\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m    321\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[1;32m    322\u001b[0m         return_tuple \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    323\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[1;32m    324\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[1;32m    325\u001b[0m         )\n",
      "File \u001b[0;32m~/jinenv/lib/python3.11/site-packages/sklearn/base.py:918\u001b[0m, in \u001b[0;36mTransformerMixin.fit_transform\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    903\u001b[0m         warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    904\u001b[0m             (\n\u001b[1;32m    905\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis object (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) has a `transform`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    913\u001b[0m             \u001b[38;5;167;01mUserWarning\u001b[39;00m,\n\u001b[1;32m    914\u001b[0m         )\n\u001b[1;32m    916\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    917\u001b[0m     \u001b[38;5;66;03m# fit method of arity 1 (unsupervised transformation)\u001b[39;00m\n\u001b[0;32m--> 918\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_params\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    919\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    920\u001b[0m     \u001b[38;5;66;03m# fit method of arity 2 (supervised transformation)\u001b[39;00m\n\u001b[1;32m    921\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfit(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\u001b[38;5;241m.\u001b[39mtransform(X)\n",
      "File \u001b[0;32m~/jinenv/lib/python3.11/site-packages/sklearn/utils/_set_output.py:319\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[0;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[1;32m    318\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 319\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    320\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m    321\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[1;32m    322\u001b[0m         return_tuple \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    323\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[1;32m    324\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[1;32m    325\u001b[0m         )\n",
      "File \u001b[0;32m~/jinenv/lib/python3.11/site-packages/sklearn/impute/_knn.py:376\u001b[0m, in \u001b[0;36mKNNImputer.transform\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    367\u001b[0m \u001b[38;5;66;03m# process in fixed-memory chunks\u001b[39;00m\n\u001b[1;32m    368\u001b[0m gen \u001b[38;5;241m=\u001b[39m pairwise_distances_chunked(\n\u001b[1;32m    369\u001b[0m     X[row_missing_idx, :],\n\u001b[1;32m    370\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit_X,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    374\u001b[0m     reduce_func\u001b[38;5;241m=\u001b[39mprocess_chunk,\n\u001b[1;32m    375\u001b[0m )\n\u001b[0;32m--> 376\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mgen\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m    377\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# process_chunk modifies X in place. No return value.\u001b[39;49;00m\n\u001b[1;32m    378\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mpass\u001b[39;49;00m\n\u001b[1;32m    380\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkeep_empty_features:\n",
      "File \u001b[0;32m~/jinenv/lib/python3.11/site-packages/sklearn/metrics/pairwise.py:2261\u001b[0m, in \u001b[0;36mpairwise_distances_chunked\u001b[0;34m(X, Y, reduce_func, metric, n_jobs, working_memory, **kwds)\u001b[0m\n\u001b[1;32m   2259\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m reduce_func \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   2260\u001b[0m     chunk_size \u001b[38;5;241m=\u001b[39m D_chunk\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m-> 2261\u001b[0m     D_chunk \u001b[38;5;241m=\u001b[39m \u001b[43mreduce_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43mD_chunk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msl\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2262\u001b[0m     _check_chunk_size(D_chunk, chunk_size)\n\u001b[1;32m   2263\u001b[0m \u001b[38;5;28;01myield\u001b[39;00m D_chunk\n",
      "File \u001b[0;32m~/jinenv/lib/python3.11/site-packages/sklearn/impute/_knn.py:339\u001b[0m, in \u001b[0;36mKNNImputer.transform.<locals>.process_chunk\u001b[0;34m(dist_chunk, start)\u001b[0m\n\u001b[1;32m    334\u001b[0m dist_subset \u001b[38;5;241m=\u001b[39m dist_chunk[dist_idx_map[receivers_idx] \u001b[38;5;241m-\u001b[39m start][\n\u001b[1;32m    335\u001b[0m     :, potential_donors_idx\n\u001b[1;32m    336\u001b[0m ]\n\u001b[1;32m    338\u001b[0m \u001b[38;5;66;03m# receivers with all nan distances impute with mean\u001b[39;00m\n\u001b[0;32m--> 339\u001b[0m all_nan_dist_mask \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43misnan\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdist_subset\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mall(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    340\u001b[0m all_nan_receivers_idx \u001b[38;5;241m=\u001b[39m receivers_idx[all_nan_dist_mask]\n\u001b[1;32m    342\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m all_nan_receivers_idx\u001b[38;5;241m.\u001b[39msize:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "ordinal_cols = (\n",
    "    [f\"gp{i}\" for i in range(1, 8)] + [f\"gs{i}\" for i in range(1, 8)] +\n",
    "    [f\"ge{i}\" for i in range(1, 7)] + [f\"gf{i}\" for i in range(1, 8)] +\n",
    "    [f\"a_hn{i}\" for i in range(1, 6)] + [\"a_hn7\", \"a_hn10\"] +\n",
    "    [f\"a_e{i}\" for i in range(1, 8)] + [\"a_c6\", \"a_c2\", \"a_act11\"]\n",
    ")\n",
    "columns_to_impute = ordinal_cols#['ge1', 'ge2', 'ge3', 'ge4', 'ge5', 'ge6']\n",
    "# Apply deep autoencoder imputation with explicit column specifications\n",
    "imputed_df, validation_results = apply_knn_imputation(\n",
    "    df=df, \n",
    "    columns_to_impute=columns_to_impute\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Real data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== Analyzing Dataset =====\n",
      "Dataset shape: (18187, 65)\n",
      "\n",
      "Missing value counts:\n",
      "ge1: 15454 missing values (84.97%)\n",
      "ge2: 15497 missing values (85.21%)\n",
      "ge3: 15543 missing values (85.46%)\n",
      "ge4: 15472 missing values (85.07%)\n",
      "ge5: 15482 missing values (85.13%)\n",
      "ge6: 15469 missing values (85.06%)\n",
      "\n",
      "Observed values per column:\n",
      "ge1: 2733 observed values (15.03%)\n",
      "  Range: 0.0 to 4.0\n",
      "  Mean: 1.0893, Std: 1.1332\n",
      "  Unique values: 5 (0.2% unique)\n",
      "ge2: 2690 observed values (14.79%)\n",
      "  Range: 0.0 to 4.0\n",
      "  Mean: 2.4647, Std: 1.1927\n",
      "  Unique values: 5 (0.2% unique)\n",
      "ge3: 2644 observed values (14.54%)\n",
      "  Range: 0.0 to 4.0\n",
      "  Mean: 0.5843, Std: 1.0138\n",
      "  Unique values: 5 (0.2% unique)\n",
      "ge4: 2715 observed values (14.93%)\n",
      "  Range: 0.0 to 4.0\n",
      "  Mean: 1.2891, Std: 1.2363\n",
      "  Unique values: 5 (0.2% unique)\n",
      "ge5: 2705 observed values (14.87%)\n",
      "  Range: 0.0 to 4.0\n",
      "  Mean: 0.9567, Std: 1.1935\n",
      "  Unique values: 5 (0.2% unique)\n",
      "ge6: 2718 observed values (14.94%)\n",
      "  Range: 0.0 to 4.0\n",
      "  Mean: 1.5839, Std: 1.2910\n",
      "  Unique values: 5 (0.2% unique)\n",
      "\n",
      "===== Running imputation on real data =====\n",
      "Missingness statistics before imputation:\n",
      "ge1: 15454 missing values (84.97%)\n",
      "ge2: 15497 missing values (85.21%)\n",
      "ge3: 15543 missing values (85.46%)\n",
      "ge4: 15472 missing values (85.07%)\n",
      "ge5: 15482 missing values (85.13%)\n",
      "ge6: 15469 missing values (85.06%)\n",
      "\n",
      "Running MICE imputation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MICE Imputation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:42<00:00,  8.51s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MICE completed in 42.63 seconds\n",
      "MICE - ge1 distribution similarity: KS=0.4182, p=0.0000\n",
      "MICE - ge2 distribution similarity: KS=0.5261, p=0.0000\n",
      "MICE - ge3 distribution similarity: KS=0.6244, p=0.0000\n",
      "MICE - ge4 distribution similarity: KS=0.5207, p=0.0000\n",
      "MICE - ge5 distribution similarity: KS=0.6050, p=0.0000\n",
      "MICE - ge6 distribution similarity: KS=0.4982, p=0.0000\n",
      "\n",
      "Running Autoencoder imputation...\n",
      "Training autoencoder imputation models...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training model for ge5:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 4/6 [02:25<01:14, 37.03s/it]"
     ]
    }
   ],
   "source": [
    "# Add these imports at the top of your file\n",
    "# from TransformerImputer import apply_transformer_imputation\n",
    "# from CUDATransformerImputer import apply_cudatransformer_imputation\n",
    "\n",
    "def evaluate_real_data_imputation(df, columns_to_impute):\n",
    "    \"\"\"\n",
    "    Evaluate imputation methods on real data with existing missing values\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pandas.DataFrame\n",
    "        Your real dataset with missing values\n",
    "    columns_to_impute : list\n",
    "        List of column names to impute\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    all_imputed_dfs : dict\n",
    "        Dictionary with imputed DataFrames from each method\n",
    "    original_df_with_missing : pandas.DataFrame\n",
    "        Original dataframe with missing values\n",
    "    execution_times : dict\n",
    "        Dictionary with execution times for each method\n",
    "    \"\"\"\n",
    "    # Define the methods to compare\n",
    "    methods = {\n",
    "        'MICE': apply_mice_imputation,\n",
    "        'VAE': apply_vae_imputation,\n",
    "        'DAE': apply_sklearn_dae_imputation,\n",
    "        'Bayesian PCA': apply_bpca_imputation,\n",
    "        'Da XU DL': apply_deep_autoencoder_imputation,\n",
    "        'SoftImpute': apply_softimpute_imputation,\n",
    "        'KNN': apply_knn_imputation\n",
    "    }\n",
    "    \n",
    "    # Initialize dictionaries to store results\n",
    "    all_imputed_dfs = {}\n",
    "    execution_times = {}\n",
    "    distribution_similarity = {method: {} for method in methods}\n",
    "    \n",
    "    # Calculate missingness statistics before imputation\n",
    "    print(\"Missingness statistics before imputation:\")\n",
    "    for col in columns_to_impute:\n",
    "        missing_count = df[col].isna().sum()\n",
    "        missing_percent = (missing_count / len(df)) * 100\n",
    "        print(f\"{col}: {missing_count} missing values ({missing_percent:.2f}%)\")\n",
    "    \n",
    "    # Save a copy of the original data with missing values\n",
    "    original_df_with_missing = df.copy()\n",
    "    \n",
    "    # Run each imputation method\n",
    "    for method_name, method_func in methods.items():\n",
    "        print(f\"\\nRunning {method_name} imputation...\")\n",
    "        \n",
    "        try:\n",
    "            # Time the imputation\n",
    "            start_time = time.time()\n",
    "            \n",
    "            # Create a copy of the dataframe for this method\n",
    "            method_df = df.copy()\n",
    "            \n",
    "            # For methods other than 'Da XU DL', drop the qol_date column to avoid issues\n",
    "            if method_name != 'Da XU DL' and 'qol_date' in method_df.columns:\n",
    "                print(f\"Dropping qol_date column for {method_name} method\")\n",
    "                method_df = method_df.drop(columns=['qol_date'])\n",
    "            \n",
    "            # Apply the imputation method\n",
    "            imputed_df, _ = method_func(\n",
    "                method_df, \n",
    "                columns_to_impute\n",
    "            )\n",
    "            \n",
    "            end_time = time.time()\n",
    "            execution_time = end_time - start_time\n",
    "            execution_times[method_name] = execution_time\n",
    "            \n",
    "            # Store the imputed dataframe\n",
    "            all_imputed_dfs[method_name] = imputed_df\n",
    "            \n",
    "            # Print execution time\n",
    "            print(f\"{method_name} completed in {execution_time:.2f} seconds\")\n",
    "            \n",
    "            # Check if imputation was successful (no missing values in imputed columns)\n",
    "            for col in columns_to_impute:\n",
    "                if col not in imputed_df.columns:\n",
    "                    print(f\"Warning: Column {col} missing in {method_name} results\")\n",
    "                    continue\n",
    "                    \n",
    "                remaining_missing = imputed_df[col].isna().sum()\n",
    "                if remaining_missing > 0:\n",
    "                    print(f\"Warning: {method_name} left {remaining_missing} missing values in {col}\")\n",
    "                \n",
    "                # Measure distribution similarity between observed and imputed values\n",
    "                observed_values = original_df_with_missing.loc[original_df_with_missing[col].notna(), col]\n",
    "                imputed_indices = original_df_with_missing[col].isna() & ~imputed_df[col].isna()\n",
    "                \n",
    "                if imputed_indices.any():\n",
    "                    imputed_values = imputed_df.loc[imputed_indices, col]\n",
    "                    \n",
    "                    if len(observed_values) > 0 and len(imputed_values) > 0:\n",
    "                        # Kolmogorov-Smirnov test for distribution similarity\n",
    "                        ks_stat, ks_pval = ks_2samp(observed_values, imputed_values)\n",
    "                        distribution_similarity[method_name][col] = {\n",
    "                            'ks_stat': ks_stat,\n",
    "                            'ks_pval': ks_pval\n",
    "                        }\n",
    "                        print(f\"{method_name} - {col} distribution similarity: KS={ks_stat:.4f}, p={ks_pval:.4f}\")\n",
    "                        #Lower KS statistic and higher p-values indicate better distribution preservation\n",
    "                    \n",
    "        except Exception as e:\n",
    "            print(f\"Error running {method_name}: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()  # Print the full error trace for better diagnosis\n",
    "    \n",
    "    return all_imputed_dfs, original_df_with_missing, execution_times, distribution_similarity\n",
    "\n",
    "\n",
    "def evaluate_with_sparse_validation(df, columns_to_impute, n_folds=3):\n",
    "    \"\"\"\n",
    "    Evaluate imputation methods using cross-validation optimized for sparse data\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pandas.DataFrame\n",
    "        Your real dataset with missing values\n",
    "    columns_to_impute : list\n",
    "        List of column names to impute\n",
    "    n_folds : int\n",
    "        Number of validation folds\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    results : dict\n",
    "        Dictionary with validation results for each method\n",
    "    \"\"\"\n",
    "    # Define the methods to compare\n",
    "    methods = {\n",
    "        'MICE': apply_mice_imputation,\n",
    "        'VAE': apply_vae_imputation,\n",
    "        'DAE': apply_sklearn_dae_imputation,\n",
    "        'Bayesian PCA': apply_bpca_imputation,\n",
    "        'Da XU DL': apply_deep_autoencoder_imputation,\n",
    "        'SoftImpute': apply_softimpute_imputation,\n",
    "        'KNN': apply_knn_imputation\n",
    "    }\n",
    "\n",
    "    # Initialize results dictionary - now including classification metrics\n",
    "    results = {method: {\n",
    "        'mae': [], 'rmse': [], 'accuracy': [], 'auc_multiclass': [],\n",
    "        'avg_sensitivity': [], 'avg_specificity': [], 'avg_ppv': [], 'avg_npv': [],\n",
    "        'precision_macro': [], 'recall_macro': [], 'time': []\n",
    "    } for method in methods}\n",
    "\n",
    "    # IMPORTANT: Filter out columns that don't exist in the dataframe and any derived date columns\n",
    "    valid_columns = []\n",
    "    for col in columns_to_impute:\n",
    "        if col in df.columns and not col.startswith('qol_date_'):\n",
    "            valid_columns.append(col)\n",
    "\n",
    "    # Print warning if columns were filtered\n",
    "    if len(valid_columns) != len(columns_to_impute):\n",
    "        excluded_cols = set(columns_to_impute) - set(valid_columns)\n",
    "        print(f\"Warning: Excluding {len(excluded_cols)} columns that are not in the dataframe or are derived date columns:\")\n",
    "        print(f\"  {sorted(excluded_cols)}\")\n",
    "        print(f\"Valid columns for validation: {sorted(valid_columns)}\")\n",
    "\n",
    "    # For each column, perform validation on non-missing values\n",
    "    for col in valid_columns:\n",
    "        print(f\"\\n===== Sparse validation for column: {col} =====\")\n",
    "        \n",
    "        # Get indices of non-missing values for this column\n",
    "        observed_indices = df.index[df[col].notna()].tolist()\n",
    "        n_observed = len(observed_indices)\n",
    "        \n",
    "        print(f\"Column {col}: {n_observed} observed values ({n_observed/len(df)*100:.2f}%)\")\n",
    "        \n",
    "        # Skip if too few observations\n",
    "        if n_observed < max(5, n_folds * 2):\n",
    "            print(f\"Too few observed values for validation. Skipping.\")\n",
    "            continue\n",
    "        \n",
    "        # Instead of standard KFold, use a more conservative approach for sparse data\n",
    "        fold_size = max(5, n_observed // n_folds)  # Ensure at least 5 samples per fold\n",
    "        \n",
    "        # Shuffle observed indices\n",
    "        np.random.seed(random.randint(0, 10000))  # For reproducibility\n",
    "        shuffled_indices = np.random.permutation(observed_indices)\n",
    "        \n",
    "        for fold in range(min(n_folds, n_observed // fold_size)):\n",
    "            # Select test indices for this fold\n",
    "            start_idx = fold * fold_size\n",
    "            end_idx = min(start_idx + fold_size, n_observed)\n",
    "            test_indices = shuffled_indices[start_idx:end_idx]\n",
    "            \n",
    "            print(f\"\\nFold {fold+1}/{min(n_folds, n_observed // fold_size)} - Testing on {len(test_indices)} samples\")\n",
    "            \n",
    "            # Create a copy of the original dataframe\n",
    "            df_fold = df.copy()\n",
    "            \n",
    "            # Store original values from test set\n",
    "            original_values = df_fold.loc[test_indices, col].copy()\n",
    "            \n",
    "            # Set test values to NaN (simulating missingness)\n",
    "            df_fold.loc[test_indices, col] = np.nan\n",
    "            \n",
    "            # For each imputation method\n",
    "            for method_name, method_func in methods.items():\n",
    "                print(f\"Running {method_name}...\")\n",
    "                \n",
    "                try:\n",
    "                    # Create a method-specific copy of the dataframe\n",
    "                    method_df = df_fold.copy()\n",
    "                    \n",
    "                    # For methods other than 'Da XU DL', drop the qol_date column\n",
    "                    if method_name != 'Da XU DL' and 'qol_date' in method_df.columns:\n",
    "                        method_df = method_df.drop(columns=['qol_date'])\n",
    "                    \n",
    "                    # Time the imputation\n",
    "                    start_time = time.time()\n",
    "                    \n",
    "                    # Apply imputation - use only valid_columns for imputation\n",
    "                    imputed_df, _ = method_func(method_df.copy(), valid_columns)\n",
    "                    \n",
    "                    end_time = time.time()\n",
    "                    execution_time = end_time - start_time\n",
    "                    \n",
    "                    # Get imputed values for the test indices\n",
    "                    if col not in imputed_df.columns:\n",
    "                        print(f\"Warning: Column {col} not found in {method_name} results. Skipping.\")\n",
    "                        continue\n",
    "                        \n",
    "                    imputed_values = imputed_df.loc[test_indices, col]\n",
    "                    \n",
    "                    # Check for any still-missing values\n",
    "                    still_missing = imputed_values.isna().sum()\n",
    "                    if still_missing > 0:\n",
    "                        print(f\"Warning: {method_name} failed to impute {still_missing}/{len(test_indices)} values\")\n",
    "                        # Use only successfully imputed values for evaluation\n",
    "                        valid_indices = [idx for idx in test_indices if not pd.isna(imputed_df.loc[idx, col])]\n",
    "                        \n",
    "                        if not valid_indices:\n",
    "                            print(f\"No valid imputations to evaluate for {method_name}\")\n",
    "                            continue\n",
    "                            \n",
    "                        original_values_filtered = df.loc[valid_indices, col]\n",
    "                        imputed_values_filtered = imputed_df.loc[valid_indices, col]\n",
    "                    else:\n",
    "                        original_values_filtered = original_values\n",
    "                        imputed_values_filtered = imputed_values\n",
    "                    \n",
    "                    # Calculate continuous metrics (NO ROUNDING)\n",
    "                    mae = mean_absolute_error(original_values_filtered, imputed_values_filtered)\n",
    "                    rmse = np.sqrt(mean_squared_error(original_values_filtered, imputed_values_filtered))\n",
    "                    \n",
    "                    # Calculate classification metrics (WITH ROUNDING)\n",
    "                    real_vals_class = process_for_classification(original_values_filtered)\n",
    "                    imputed_vals_class = process_for_classification(imputed_values_filtered)\n",
    "                    \n",
    "                    classification_metrics = calculate_classification_metrics(real_vals_class, imputed_vals_class)\n",
    "                    \n",
    "                    # Store results\n",
    "                    results[method_name]['mae'].append(mae)\n",
    "                    results[method_name]['rmse'].append(rmse)\n",
    "                    results[method_name]['accuracy'].append(classification_metrics['accuracy'])\n",
    "                    results[method_name]['auc_multiclass'].append(classification_metrics['auc_multiclass'] if not np.isnan(classification_metrics['auc_multiclass']) else 0)\n",
    "                    results[method_name]['avg_sensitivity'].append(classification_metrics['avg_sensitivity'])\n",
    "                    results[method_name]['avg_specificity'].append(classification_metrics['avg_specificity'])\n",
    "                    results[method_name]['avg_ppv'].append(classification_metrics['avg_ppv'])\n",
    "                    results[method_name]['avg_npv'].append(classification_metrics['avg_npv'])\n",
    "                    results[method_name]['precision_macro'].append(classification_metrics['precision_macro'])\n",
    "                    results[method_name]['recall_macro'].append(classification_metrics['recall_macro'])\n",
    "                    results[method_name]['time'].append(execution_time)\n",
    "                    \n",
    "                    print(f\"{method_name} - MAE: {mae:.4f}, RMSE: {rmse:.4f}, Acc: {classification_metrics['accuracy']:.4f}, AUC: {classification_metrics['auc_multiclass']:.4f}, Time: {execution_time:.2f}s\")\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"Error with {method_name}: {e}\")\n",
    "                    import traceback\n",
    "                    traceback.print_exc()  # Print full traceback for better debugging\n",
    "\n",
    "    # Calculate average results across all columns and folds\n",
    "    metric_names = ['mae', 'rmse', 'accuracy', 'auc_multiclass', 'avg_sensitivity', 'avg_specificity', \n",
    "                    'avg_ppv', 'avg_npv', 'precision_macro', 'recall_macro', 'time']\n",
    "    \n",
    "    for method in methods:\n",
    "        if results[method]['mae']:  # Check if we have any results\n",
    "            for metric in metric_names:\n",
    "                results[method][f'avg_{metric}'] = np.mean(results[method][metric])\n",
    "                results[method][f'std_{metric}'] = np.std(results[method][metric])\n",
    "        else:\n",
    "            print(f\"No valid results for {method}\")\n",
    "            for metric in metric_names:\n",
    "                results[method][f'avg_{metric}'] = np.nan\n",
    "                results[method][f'std_{metric}'] = np.nan\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "def plot_execution_times(execution_times):\n",
    "    \"\"\"\n",
    "    Plot execution times for each imputation method\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    execution_times : dict\n",
    "        Dictionary with execution times for each method\n",
    "    \"\"\"\n",
    "    methods = list(execution_times.keys())\n",
    "    times = list(execution_times.values())\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    bars = plt.bar(methods, times, alpha=0.7, color='green')\n",
    "    \n",
    "    plt.title('Execution Time by Imputation Method', fontsize=14)\n",
    "    plt.ylabel('Time (seconds)', fontsize=12)\n",
    "    plt.xlabel('Method', fontsize=12)\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    \n",
    "    # Add values on top of bars\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        plt.text(bar.get_x() + bar.get_width()/2., height + 0.02 * max(times),\n",
    "                f'{height:.2f}s', ha='center', va='bottom', fontsize=10)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('imputation_execution_times.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "def plot_distribution_similarity(distribution_similarity):\n",
    "    \"\"\"\n",
    "    Plot KS statistics for distribution similarity\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    distribution_similarity : dict\n",
    "        Dictionary with KS test results\n",
    "    \"\"\"\n",
    "    # Prepare data for plotting\n",
    "    methods = list(distribution_similarity.keys())\n",
    "    columns = list(distribution_similarity[methods[0]].keys()) if methods else []\n",
    "    \n",
    "    if not methods or not columns:\n",
    "        print(\"No distribution similarity data to plot\")\n",
    "        return\n",
    "    \n",
    "    # Create a DataFrame for easier plotting\n",
    "    data = []\n",
    "    for method in methods:\n",
    "        for col in columns:\n",
    "            if col in distribution_similarity[method]:\n",
    "                data.append({\n",
    "                    'Method': method,\n",
    "                    'Column': col,\n",
    "                    'KS Statistic': distribution_similarity[method][col]['ks_stat'],\n",
    "                    'p-value': distribution_similarity[method][col]['ks_pval']\n",
    "                })\n",
    "    \n",
    "    if not data:\n",
    "        print(\"No valid distribution similarity data to plot\")\n",
    "        return\n",
    "    \n",
    "    df_plot = pd.DataFrame(data)\n",
    "    \n",
    "    # Plot KS statistics (lower is better - more similar distributions)\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    chart = sns.barplot(x='Method', y='KS Statistic', hue='Column', data=df_plot)\n",
    "    \n",
    "    plt.title('Distribution Similarity by Method (Lower KS = More Similar)', fontsize=14)\n",
    "    plt.ylabel('Kolmogorov-Smirnov Statistic', fontsize=12)\n",
    "    plt.xlabel('Method', fontsize=12)\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    plt.legend(title='Column', bbox_to_anchor=(1.05, 1), loc='upper left', ncol=3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('distribution_similarity.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    # Plot p-values (higher is better - can't reject null hypothesis of same distribution)\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    chart = sns.barplot(x='Method', y='p-value', hue='Column', data=df_plot)\n",
    "    \n",
    "    plt.title('Distribution Similarity p-values by Method (Higher = More Similar)', fontsize=14)\n",
    "    plt.ylabel('p-value', fontsize=12)\n",
    "    plt.xlabel('Method', fontsize=12)\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    plt.legend(title='Column', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('distribution_pvalues.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "def plot_imputation_histograms(original_df, imputed_dfs, columns_to_impute):\n",
    "    \"\"\"\n",
    "    Plot histograms of imputed values compared to original observed values\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    original_df : pandas.DataFrame\n",
    "        Original data with missing values\n",
    "    imputed_dfs : dict\n",
    "        Dictionary of imputed DataFrames from different methods\n",
    "    columns_to_impute : list\n",
    "        List of column names to impute\n",
    "    \"\"\"\n",
    "    # Filter to only include columns that exist in the original dataframe\n",
    "    valid_columns = [col for col in columns_to_impute if col in original_df.columns]\n",
    "    \n",
    "    if not valid_columns:\n",
    "        print(\"No valid columns found for plotting histograms\")\n",
    "        return\n",
    "        \n",
    "    print(f\"Plotting histograms for columns: {valid_columns}\")\n",
    "    \n",
    "    # Number of columns and methods\n",
    "    n_cols = len(valid_columns)\n",
    "    n_methods = len(imputed_dfs)\n",
    "    \n",
    "    # Create figure with subplots\n",
    "    fig, axes = plt.subplots(n_cols, 1, figsize=(12, 4 * n_cols))\n",
    "    if n_cols == 1:\n",
    "        axes = [axes]\n",
    "    \n",
    "    # Plot for each column\n",
    "    for i, col in enumerate(valid_columns):\n",
    "        ax = axes[i]\n",
    "        \n",
    "        # Plot original observed distribution\n",
    "        observed_values = original_df[col].dropna()\n",
    "        n_observed = len(observed_values)\n",
    "        \n",
    "        sns.histplot(observed_values, ax=ax, \n",
    "                    label=f'Original (observed, n={n_observed})', \n",
    "                    alpha=0.5, color='black', kde=True)\n",
    "        \n",
    "        # Plot imputed distributions (only for previously missing values)\n",
    "        colors = plt.cm.tab10.colors\n",
    "        for j, (method_name, imputed_df) in enumerate(imputed_dfs.items()):\n",
    "            # Make sure the column exists in the imputed dataframe\n",
    "            if col not in imputed_df.columns:\n",
    "                print(f\"Warning: Column {col} not found in {method_name} results\")\n",
    "                continue\n",
    "                \n",
    "            # Get just the imputed values (where original was missing)\n",
    "            missing_mask = original_df[col].isna()\n",
    "            imputed_values = imputed_df.loc[missing_mask, col].dropna()\n",
    "            n_imputed = len(imputed_values)\n",
    "            \n",
    "            if not imputed_values.empty:\n",
    "                color = colors[j % len(colors)]\n",
    "                sns.histplot(imputed_values, ax=ax, \n",
    "                            label=f'{method_name} (imputed, n={n_imputed})', \n",
    "                            alpha=0.5, color=color, kde=True)\n",
    "        \n",
    "        # Add distribution statistics\n",
    "        if not observed_values.empty:\n",
    "            obs_mean = observed_values.mean()\n",
    "            obs_std = observed_values.std()\n",
    "            ax.axvline(obs_mean, color='black', linestyle='--', alpha=0.7)\n",
    "            textstr = f'Observed: Î¼={obs_mean:.2f}, Ïƒ={obs_std:.2f}'\n",
    "            props = dict(boxstyle='round', facecolor='white', alpha=0.5)\n",
    "            ax.text(0.05, 0.95, textstr, transform=ax.transAxes, fontsize=10,\n",
    "                    verticalalignment='top', bbox=props)\n",
    "        \n",
    "        ax.set_title(f'Distribution for {col}', fontsize=14)\n",
    "        ax.legend()\n",
    "        ax.grid(alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('imputation_histograms.png', dpi=300, bbox_inches='tight')\n",
    "    plt.close()  # Close the plot to free memory\n",
    "    print(\"Histogram plot saved to 'imputation_histograms.png'\")\n",
    "\n",
    "def plot_correlation_preservation(original_df, imputed_dfs, columns_to_impute):\n",
    "    \"\"\"\n",
    "    Plot heatmaps showing how well each method preserves correlations\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    original_df : pandas.DataFrame\n",
    "        Original data with missing values\n",
    "    imputed_dfs : dict\n",
    "        Dictionary of imputed DataFrames from different methods\n",
    "    columns_to_impute : list\n",
    "        List of column names to impute\n",
    "    \"\"\"\n",
    "    # Filter to only include columns that exist in all dataframes\n",
    "    valid_columns = []\n",
    "    for col in columns_to_impute:\n",
    "        if col in original_df.columns:\n",
    "            all_valid = True\n",
    "            for method_name, imputed_df in imputed_dfs.items():\n",
    "                if col not in imputed_df.columns:\n",
    "                    all_valid = False\n",
    "                    break\n",
    "            if all_valid:\n",
    "                valid_columns.append(col)\n",
    "    \n",
    "    if not valid_columns:\n",
    "        print(\"No valid columns found for correlation analysis\")\n",
    "        return\n",
    "        \n",
    "    print(f\"Analyzing correlations for columns: {valid_columns}\")\n",
    "    \n",
    "    # Calculate original correlations (using only complete cases)\n",
    "    # This is important for sparse data - we need a baseline for comparison\n",
    "    complete_cases = original_df[valid_columns].dropna()\n",
    "    \n",
    "    if len(complete_cases) < 2:\n",
    "        print(\"Not enough complete cases to calculate original correlations\")\n",
    "        # Use pairwise correlations instead\n",
    "        original_corr = original_df[valid_columns].corr(method='pearson')\n",
    "    else:\n",
    "        original_corr = complete_cases.corr()\n",
    "    \n",
    "    # Calculate number of methods first\n",
    "    n_methods = len(imputed_dfs)\n",
    "    # Set up the figure - 2 rows, 4 columns\n",
    "    n_rows = 2\n",
    "    n_cols = 4\n",
    "    total_plots = n_methods + 1  # +1 for original correlations\n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(20, 8))  # 4 columns * 5 width, 2 rows * 4 height\n",
    "    axes = axes.flatten()  # Flatten to make indexing easier\n",
    "\n",
    "    if n_methods == 0:\n",
    "        print(\"No imputed dataframes to plot correlations\")\n",
    "        return\n",
    "\n",
    "    # Hide unused subplots\n",
    "    for idx in range(total_plots, n_rows * n_cols):\n",
    "        axes[idx].set_visible(False)\n",
    "    \n",
    "    # Plot original correlation\n",
    "    sns.heatmap(original_corr, annot=False, cmap='coolwarm', vmin=-1, vmax=1, ax=axes[0])\n",
    "    axes[0].set_title('Original Correlations', fontsize=14)\n",
    "    axes[0].set_xticklabels([])\n",
    "    axes[0].set_yticklabels([])\n",
    "    axes[0].set_xlabel('')\n",
    "    axes[0].set_ylabel('')\n",
    "    \n",
    "    # Plot correlations for each imputation method\n",
    "    for i, (method_name, imputed_df) in enumerate(imputed_dfs.items()):\n",
    "        imputed_corr = imputed_df[valid_columns].corr()\n",
    "        sns.heatmap(imputed_corr, annot=False, cmap='coolwarm', vmin=-1, vmax=1, ax=axes[i+1])\n",
    "        axes[i+1].set_title(f'{method_name} Correlations', fontsize=14)\n",
    "        axes[i+1].set_xticklabels([])\n",
    "        axes[i+1].set_yticklabels([])\n",
    "        axes[i+1].set_xlabel('')\n",
    "        axes[i+1].set_ylabel('')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('correlation_preservation.png', dpi=300, bbox_inches='tight')\n",
    "    plt.close()  # Close the plot to free memory\n",
    "    print(\"Correlation plot saved to 'correlation_preservation.png'\")\n",
    "    \n",
    "    # Calculate and plot correlation differences\n",
    "    print(\"Calculating correlation differences...\")\n",
    "    correlation_diffs = {}\n",
    "    \n",
    "    for method_name, imputed_df in imputed_dfs.items():\n",
    "        imputed_corr = imputed_df[valid_columns].corr()\n",
    "        # Calculate absolute differences between original and imputed correlations\n",
    "        diff_matrix = np.abs(original_corr - imputed_corr)\n",
    "        correlation_diffs[method_name] = diff_matrix\n",
    "    \n",
    "    # Plot correlation differences (lower is better - less difference from original)\n",
    "    # Calculate grid dimensions: 2 rows, 4 columns\n",
    "    n_rows = 2\n",
    "    n_cols = 4\n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(20, 8))  # 4 columns * 5 width, 2 rows * 4 height\n",
    "    axes = axes.flatten()  # Flatten to make indexing easier\n",
    "\n",
    "    # Hide unused subplots if n_methods < 8\n",
    "    for idx in range(n_methods, n_rows * n_cols):\n",
    "        axes[idx].set_visible(False)\n",
    "    \n",
    "    for i, (method_name, diff_matrix) in enumerate(correlation_diffs.items()):\n",
    "        sns.heatmap(diff_matrix, annot=False, cmap='YlOrRd', vmin=0, vmax=1, ax=axes[i])\n",
    "        axes[i].set_title(f'{method_name} Correlation Differences', fontsize=12)\n",
    "        \n",
    "        # Hide x and y tick labels to reduce clutter\n",
    "        axes[i].set_xticklabels([])\n",
    "        axes[i].set_yticklabels([])\n",
    "        axes[i].set_xlabel('')\n",
    "        axes[i].set_ylabel('')\n",
    "        \n",
    "        # Calculate and display average difference\n",
    "        avg_diff = np.mean(diff_matrix.values)\n",
    "        axes[i].text(0.5, -0.05, f'Avg Diff: {avg_diff:.4f}', \n",
    "                    horizontalalignment='center', transform=axes[i].transAxes, fontsize=10)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('correlation_differences.png', dpi=300, bbox_inches='tight')\n",
    "    plt.close()  # Close the plot to free memory\n",
    "    print(\"Correlation difference plot saved to 'correlation_differences.png'\")\n",
    "\n",
    "\n",
    "\n",
    "def plot_validation_results(validation_results):\n",
    "    \"\"\"\n",
    "    Plot validation results including classification metrics\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    validation_results : dict\n",
    "        Dictionary with validation results\n",
    "    \"\"\"\n",
    "    # Prepare data for plotting\n",
    "    methods = [m for m in validation_results.keys() \n",
    "              if 'avg_mae' in validation_results[m] and not np.isnan(validation_results[m]['avg_mae'])]\n",
    "    \n",
    "    if not methods:\n",
    "        print(\"No valid validation results to plot\")\n",
    "        return\n",
    "    \n",
    "    # Extract metrics\n",
    "    mae_means = [validation_results[m]['avg_mae'] for m in methods]\n",
    "    mae_stds = [validation_results[m]['std_mae'] for m in methods]\n",
    "    rmse_means = [validation_results[m]['avg_rmse'] for m in methods]\n",
    "    rmse_stds = [validation_results[m]['std_rmse'] for m in methods]\n",
    "    acc_means = [validation_results[m].get('avg_accuracy', 0) for m in methods]\n",
    "    acc_stds = [validation_results[m].get('std_accuracy', 0) for m in methods]\n",
    "    auc_means = [validation_results[m].get('avg_auc_multiclass', 0) for m in methods]\n",
    "    auc_stds = [validation_results[m].get('std_auc_multiclass', 0) for m in methods]\n",
    "    sens_means = [validation_results[m].get('avg_avg_sensitivity', 0) for m in methods]\n",
    "    sens_stds = [validation_results[m].get('std_avg_sensitivity', 0) for m in methods]\n",
    "    spec_means = [validation_results[m].get('avg_avg_specificity', 0) for m in methods]\n",
    "    spec_stds = [validation_results[m].get('std_avg_specificity', 0) for m in methods]\n",
    "    ppv_means = [validation_results[m].get('avg_avg_ppv', 0) for m in methods]\n",
    "    ppv_stds = [validation_results[m].get('std_avg_ppv', 0) for m in methods]\n",
    "    npv_means = [validation_results[m].get('avg_avg_npv', 0) for m in methods]\n",
    "    npv_stds = [validation_results[m].get('std_avg_npv', 0) for m in methods]\n",
    "    time_means = [validation_results[m]['avg_time'] for m in methods]\n",
    "    time_stds = [validation_results[m]['std_time'] for m in methods]\n",
    "    \n",
    "    # Create figure with multiple subplots\n",
    "    fig, axes = plt.subplots(3, 3, figsize=(18, 15))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    metrics_data = [\n",
    "        (mae_means, mae_stds, 'MAE (Lower = Better)', 'blue'),\n",
    "        (rmse_means, rmse_stds, 'RMSE (Lower = Better)', 'orange'),\n",
    "        (acc_means, acc_stds, 'Accuracy (Higher = Better)', 'green'),\n",
    "        (auc_means, auc_stds, 'AUC Multi-class (Higher = Better)', 'red'),\n",
    "        (sens_means, sens_stds, 'Average Sensitivity (Higher = Better)', 'purple'),\n",
    "        (spec_means, spec_stds, 'Average Specificity (Higher = Better)', 'brown'),\n",
    "        (ppv_means, ppv_stds, 'Average PPV (Higher = Better)', 'pink'),\n",
    "        (npv_means, npv_stds, 'Average NPV (Higher = Better)', 'gray'),\n",
    "        (time_means, time_stds, 'Execution Time (Lower = Better)', 'cyan')\n",
    "    ]\n",
    "    \n",
    "    for i, (means, stds, title, color) in enumerate(metrics_data):\n",
    "        bars = axes[i].bar(methods, means, yerr=stds, capsize=5, alpha=0.7, color=color)\n",
    "        axes[i].set_title(title, fontsize=12)\n",
    "        axes[i].set_ylabel(title.split('(')[0].strip(), fontsize=10)\n",
    "        axes[i].grid(axis='y', linestyle='--', alpha=0.7)\n",
    "        axes[i].tick_params(axis='x', rotation=45)\n",
    "        \n",
    "        # Add values on top of bars (accounting for error bars)\n",
    "        for j, bar in enumerate(bars):\n",
    "            height = bar.get_height()\n",
    "            if not np.isnan(height):\n",
    "                # Add the standard deviation to position text above the error bar\n",
    "                text_height = height + stds[j] + 0.005 * max(means)\n",
    "                axes[i].text(bar.get_x() + bar.get_width()/2., text_height,\n",
    "                        f'{height:.3f}', ha='center', va='bottom', fontsize=8)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('validation_results_with_classification.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def create_summary_dataframe(imputed_dfs, validation_results, distribution_similarity, execution_times):\n",
    "    \"\"\"\n",
    "    Create a summary dataframe of all results including classification metrics\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    imputed_dfs : dict\n",
    "        Dictionary of imputed DataFrames\n",
    "    validation_results : dict\n",
    "        Dictionary with validation results\n",
    "    distribution_similarity : dict\n",
    "        Dictionary with distribution similarity results\n",
    "    execution_times : dict\n",
    "        Dictionary with execution times\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    summary_df : pandas.DataFrame\n",
    "        DataFrame with summary of all results\n",
    "    \"\"\"\n",
    "    # Get list of methods\n",
    "    methods = list(imputed_dfs.keys())\n",
    "    \n",
    "    # Initialize summary data\n",
    "    summary_data = []\n",
    "    \n",
    "    for method in methods:\n",
    "        method_summary = {'Method': method}\n",
    "        \n",
    "        # Add validation metrics if available\n",
    "        if method in validation_results and 'avg_mae' in validation_results[method]:\n",
    "            method_summary['MAE'] = validation_results[method]['avg_mae']\n",
    "            method_summary['RMSE'] = validation_results[method]['avg_rmse']\n",
    "            method_summary['Accuracy'] = validation_results[method].get('avg_accuracy', np.nan)\n",
    "            method_summary['AUC'] = validation_results[method].get('avg_auc_multiclass', np.nan)\n",
    "            method_summary['Sensitivity'] = validation_results[method].get('avg_avg_sensitivity', np.nan)\n",
    "            method_summary['Specificity'] = validation_results[method].get('avg_avg_specificity', np.nan)\n",
    "            method_summary['PPV'] = validation_results[method].get('avg_avg_ppv', np.nan)\n",
    "            method_summary['NPV'] = validation_results[method].get('avg_avg_npv', np.nan)\n",
    "        else:\n",
    "            method_summary['MAE'] = np.nan\n",
    "            method_summary['RMSE'] = np.nan\n",
    "            method_summary['Accuracy'] = np.nan\n",
    "            method_summary['AUC'] = np.nan\n",
    "            method_summary['Sensitivity'] = np.nan\n",
    "            method_summary['Specificity'] = np.nan\n",
    "            method_summary['PPV'] = np.nan\n",
    "            method_summary['NPV'] = np.nan\n",
    "        \n",
    "        # Add execution time\n",
    "        if method in execution_times:\n",
    "            method_summary['Time (s)'] = execution_times[method]\n",
    "        else:\n",
    "            method_summary['Time (s)'] = np.nan\n",
    "        \n",
    "        # Add average distribution similarity metrics if available\n",
    "        if method in distribution_similarity:\n",
    "            ks_stats = []\n",
    "            ks_pvals = []\n",
    "            \n",
    "            for col, results in distribution_similarity[method].items():\n",
    "                if 'ks_stat' in results:\n",
    "                    ks_stats.append(results['ks_stat'])\n",
    "                if 'ks_pval' in results:\n",
    "                    ks_pvals.append(results['ks_pval'])\n",
    "            \n",
    "            if ks_stats:\n",
    "                method_summary['Avg KS Stat'] = np.mean(ks_stats)\n",
    "            else:\n",
    "                method_summary['Avg KS Stat'] = np.nan\n",
    "                \n",
    "            if ks_pvals:\n",
    "                method_summary['Avg KS p-value'] = np.mean(ks_pvals)\n",
    "            else:\n",
    "                method_summary['Avg KS p-value'] = np.nan\n",
    "        else:\n",
    "            method_summary['Avg KS Stat'] = np.nan\n",
    "            method_summary['Avg KS p-value'] = np.nan\n",
    "        \n",
    "        summary_data.append(method_summary)\n",
    "    \n",
    "    # Create DataFrame\n",
    "    summary_df = pd.DataFrame(summary_data)\n",
    "    \n",
    "    # Add ranks for each metric (1 is best)\n",
    "    # Lower is better for these metrics\n",
    "    for metric in ['MAE', 'RMSE', 'Time (s)', 'Avg KS Stat']:\n",
    "        if metric in summary_df.columns:\n",
    "            summary_df[f'{metric} Rank'] = summary_df[metric].rank()\n",
    "    \n",
    "    # Higher is better for these metrics\n",
    "    for metric in ['Accuracy', 'AUC', 'Sensitivity', 'Specificity', 'PPV', 'NPV', 'Avg KS p-value']:\n",
    "        if metric in summary_df.columns:\n",
    "            summary_df[f'{metric} Rank'] = summary_df[metric].rank(ascending=False)\n",
    "    \n",
    "    # Add average rank\n",
    "    rank_columns = [col for col in summary_df.columns if 'Rank' in col]\n",
    "    if rank_columns:\n",
    "        summary_df['Average Rank'] = summary_df[rank_columns].mean(axis=1)\n",
    "        summary_df = summary_df.sort_values('Average Rank')\n",
    "    \n",
    "    return summary_df\n",
    "\n",
    "\n",
    "\n",
    "def main_real_data(df, columns_to_impute):\n",
    "    \"\"\"\n",
    "    Main function to run imputation comparison on real data with high missingness\n",
    "    \n",
    "        Parameters:\n",
    "    -----------\n",
    "    df : pandas.DataFrame\n",
    "        Your real dataset with missing values\n",
    "    columns_to_impute : list\n",
    "        List of column names to impute (default: ge1-ge6)\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    imputed_dfs : dict\n",
    "        Dictionary of imputed DataFrames from each method\n",
    "    summary_df : pandas.DataFrame\n",
    "        DataFrame with summary of all results\n",
    "    \"\"\"\n",
    "    print(\"===== Analyzing Dataset =====\")\n",
    "    print(f\"Dataset shape: {df.shape}\")\n",
    "\n",
    "    # Filter out any derived date columns from columns_to_impute\n",
    "    valid_columns = [col for col in columns_to_impute if col in df.columns and not col.startswith('qol_date_')]\n",
    "    if len(valid_columns) != len(columns_to_impute):\n",
    "        excluded_cols = set(columns_to_impute) - set(valid_columns)\n",
    "        print(f\"Warning: Excluding {len(excluded_cols)} columns that are not in the dataframe or are derived date columns:\")\n",
    "        print(f\"  {sorted(excluded_cols)}\")\n",
    "        print(f\"Using valid columns: {sorted(valid_columns)}\")\n",
    "        columns_to_impute = valid_columns\n",
    "\n",
    "    # Show basic statistics of the dataset\n",
    "    print(\"\\nMissing value counts:\")\n",
    "    missing_counts = df[columns_to_impute].isna().sum()\n",
    "    missing_percents = (missing_counts / len(df)) * 100\n",
    "    for col, count, percent in zip(columns_to_impute, missing_counts, missing_percents):\n",
    "        print(f\"{col}: {count} missing values ({percent:.2f}%)\")\n",
    "\n",
    "    # Print observed values statistics\n",
    "    print(\"\\nObserved values per column:\")\n",
    "    for i, col in enumerate(columns_to_impute):\n",
    "        observed = df[col].dropna()\n",
    "        n_observed = len(observed)\n",
    "        if n_observed > 0:\n",
    "            print(f\"{col}: {n_observed} observed values ({100-missing_percents[i]:.2f}%)\")\n",
    "            print(f\"  Range: {observed.min()} to {observed.max()}\")\n",
    "            print(f\"  Mean: {observed.mean():.4f}, Std: {observed.std():.4f}\")\n",
    "            print(f\"  Unique values: {observed.nunique()} ({observed.nunique()/n_observed*100:.1f}% unique)\")\n",
    "        else:\n",
    "            print(f\"{col}: No observed values\")\n",
    "\n",
    "    print(\"\\n===== Running imputation on real data =====\")\n",
    "    imputed_dfs, original_df, execution_times, distribution_similarity = evaluate_real_data_imputation(\n",
    "        df, columns_to_impute\n",
    "    )\n",
    "\n",
    "    # Create a version of original_df without qol_date for plotting\n",
    "    plotting_df = original_df.copy()\n",
    "    if 'qol_date' in plotting_df.columns:\n",
    "        print(\"\\nRemoving qol_date for plotting functions\")\n",
    "        plotting_df = plotting_df.drop(columns=['qol_date'])\n",
    "\n",
    "    # Also remove any derived date columns if they exist\n",
    "    date_derived_cols = [col for col in plotting_df.columns if col.startswith('qol_date_')]\n",
    "    if date_derived_cols:\n",
    "        print(f\"Removing derived date columns: {date_derived_cols}\")\n",
    "        plotting_df = plotting_df.drop(columns=date_derived_cols)\n",
    "\n",
    "    # Plot execution times\n",
    "    print(\"\\nPlotting execution times...\")\n",
    "    plot_execution_times(execution_times)\n",
    "\n",
    "    # Plot distribution similarity\n",
    "    print(\"\\nPlotting distribution similarity...\")\n",
    "    plot_distribution_similarity(distribution_similarity)\n",
    "\n",
    "    # Plot histograms of imputed values\n",
    "    print(\"\\nPlotting imputation histograms...\")\n",
    "    plot_imputation_histograms(plotting_df, imputed_dfs, columns_to_impute)\n",
    "\n",
    "    # Plot correlation preservation\n",
    "    print(\"\\nPlotting correlation preservation...\")\n",
    "    plot_correlation_preservation(plotting_df, imputed_dfs, columns_to_impute)\n",
    "\n",
    "    print(\"\\n===== Running sparse validation =====\")\n",
    "    validation_results = evaluate_with_sparse_validation(df, columns_to_impute, n_folds=3)\n",
    "\n",
    "    # Plot validation results\n",
    "    print(\"\\nPlotting validation results...\")\n",
    "    plot_validation_results(validation_results)\n",
    "\n",
    "    # Create summary dataframe\n",
    "    print(\"\\nCreating summary of results...\")\n",
    "    summary_df = create_summary_dataframe(\n",
    "        imputed_dfs, \n",
    "        validation_results, \n",
    "        distribution_similarity, \n",
    "        execution_times\n",
    "    )\n",
    "\n",
    "    # Print summary of results\n",
    "    print(\"\\n===== Summary of Results =====\")\n",
    "    print(summary_df.to_string())\n",
    "\n",
    "    # Save summary to CSV\n",
    "    summary_df.to_csv('imputation_summary.csv', index=False)\n",
    "\n",
    "    # Print recommended method based on average rank\n",
    "    if 'Average Rank' in summary_df.columns and not summary_df.empty:\n",
    "        best_method = summary_df.iloc[0]['Method']\n",
    "        print(f\"\\nRecommended imputation method: {best_method}\")\n",
    "\n",
    "    return imputed_dfs, summary_df\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Load your real dataset with missing values here\n",
    "    # df = pd.read_csv('your_dataset.csv')\n",
    "    \n",
    "    # Define columns to impute\n",
    "    \n",
    "    ordinal_cols = (\n",
    "    [f\"gp{i}\" for i in range(1, 8)] + [f\"gs{i}\" for i in range(1, 8)] +\n",
    "    [f\"ge{i}\" for i in range(1, 7)] + [f\"gf{i}\" for i in range(1, 8)] +\n",
    "    [f\"a_hn{i}\" for i in range(1, 6)] + [\"a_hn7\", \"a_hn10\"] +\n",
    "    [f\"a_e{i}\" for i in range(1, 8)] + [\"a_c6\", \"a_c2\", \"a_act11\"]\n",
    ")\n",
    "    columns_to_impute = ordinal_cols#['ge1', 'ge2', 'ge3', 'ge4', 'ge5', 'ge6']\n",
    "    \n",
    "    # Run the analysis\n",
    "    imputed_dfs, summary_df = main_real_data(df, columns_to_impute)\n",
    "    \n",
    "    # Example of how to save the best imputed dataset\n",
    "    if not summary_df.empty:\n",
    "        worst_method = summary_df.iloc[0]['Method']\n",
    "        print(f\"\\nSaving imputed dataset from {worst_method}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jinenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
