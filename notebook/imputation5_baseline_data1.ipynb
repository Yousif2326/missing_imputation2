{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-18 03:36:40.866113: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-04-18 03:36:41.044119: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-04-18 03:36:41.103207: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-04-18 03:36:41.552217: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-04-18 03:36:44.288115: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# import scipy.stats as stats\n",
    "# import statsmodels.api as sm\n",
    "import miceforest as mf\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, Input\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "\n",
    "\n",
    "from tensorflow.keras.layers import Lambda, Dropout, Concatenate\n",
    "from tensorflow.keras import losses\n",
    "from tensorflow.keras import backend as K\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error as sk_mse\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.impute import SimpleImputer\n",
    "import warnings\n",
    "import time\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "import random\n",
    "from sklearn.model_selection import KFold\n",
    "from scipy.stats import ks_2samp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Import + only Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>operation_date</th>\n",
       "      <th>redcap_event_name</th>\n",
       "      <th>ComplicationDate</th>\n",
       "      <th>dob</th>\n",
       "      <th>qol_date</th>\n",
       "      <th>age_diagnosis</th>\n",
       "      <th>gender</th>\n",
       "      <th>overall_primary_tumour</th>\n",
       "      <th>overall_regional_ln</th>\n",
       "      <th>...</th>\n",
       "      <th>a_e5</th>\n",
       "      <th>a_e6</th>\n",
       "      <th>a_e7</th>\n",
       "      <th>a_c6</th>\n",
       "      <th>a_c2</th>\n",
       "      <th>a_act11</th>\n",
       "      <th>readmission_30d</th>\n",
       "      <th>postop_comp</th>\n",
       "      <th>los</th>\n",
       "      <th>DischargeDate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaT</td>\n",
       "      <td>baseline_arm_1</td>\n",
       "      <td>NaT</td>\n",
       "      <td>2049-08-04</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2012-04-27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2011-08-26</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2011-08-26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>2010-02-20</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>16.0</td>\n",
       "      <td>2010-03-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>2009-02-25</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8.0</td>\n",
       "      <td>2009-03-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>2008-02-22</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13.0</td>\n",
       "      <td>2008-03-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18182</th>\n",
       "      <td>1770</td>\n",
       "      <td>NaT</td>\n",
       "      <td>preoperative_arm_1</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18183</th>\n",
       "      <td>1770</td>\n",
       "      <td>NaT</td>\n",
       "      <td>preoperative_arm_1</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18184</th>\n",
       "      <td>1770</td>\n",
       "      <td>NaT</td>\n",
       "      <td>preoperative_arm_1</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>2025-02-24</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18185</th>\n",
       "      <td>1770</td>\n",
       "      <td>NaT</td>\n",
       "      <td>baseline_arm_1</td>\n",
       "      <td>NaT</td>\n",
       "      <td>1982-10-12</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18186</th>\n",
       "      <td>1770</td>\n",
       "      <td>2025-03-14</td>\n",
       "      <td>surgery_arm_1</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>18187 rows Ã— 70 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         id operation_date   redcap_event_name ComplicationDate        dob  \\\n",
       "0         1            NaT      baseline_arm_1              NaT 2049-08-04   \n",
       "1         1     2011-08-26                 NaN              NaT        NaT   \n",
       "2         1     2010-02-20                 NaN              NaT        NaT   \n",
       "3         1     2009-02-25                 NaN              NaT        NaT   \n",
       "4         1     2008-02-22                 NaN              NaT        NaT   \n",
       "...     ...            ...                 ...              ...        ...   \n",
       "18182  1770            NaT  preoperative_arm_1              NaT        NaT   \n",
       "18183  1770            NaT  preoperative_arm_1              NaT        NaT   \n",
       "18184  1770            NaT  preoperative_arm_1              NaT        NaT   \n",
       "18185  1770            NaT      baseline_arm_1              NaT 1982-10-12   \n",
       "18186  1770     2025-03-14       surgery_arm_1              NaT        NaT   \n",
       "\n",
       "        qol_date  age_diagnosis  gender overall_primary_tumour  \\\n",
       "0            NaT            NaN     1.0                    NaN   \n",
       "1            NaT            NaN     NaN                    NaN   \n",
       "2            NaT            NaN     NaN                    NaN   \n",
       "3            NaT            NaN     NaN                    NaN   \n",
       "4            NaT            NaN     NaN                    NaN   \n",
       "...          ...            ...     ...                    ...   \n",
       "18182        NaT            NaN     NaN                    NaN   \n",
       "18183        NaT            NaN     NaN                    NaN   \n",
       "18184 2025-02-24            NaN     NaN                    NaN   \n",
       "18185        NaT            NaN     1.0                    NaN   \n",
       "18186        NaT            NaN     NaN                    NaN   \n",
       "\n",
       "      overall_regional_ln  ...  a_e5  a_e6  a_e7  a_c6  a_c2  a_act11  \\\n",
       "0                     NaN  ...   NaN   NaN   NaN   NaN   NaN      NaN   \n",
       "1                     NaN  ...   NaN   NaN   NaN   NaN   NaN      NaN   \n",
       "2                     NaN  ...   NaN   NaN   NaN   NaN   NaN      NaN   \n",
       "3                     NaN  ...   NaN   NaN   NaN   NaN   NaN      NaN   \n",
       "4                     NaN  ...   NaN   NaN   NaN   NaN   NaN      NaN   \n",
       "...                   ...  ...   ...   ...   ...   ...   ...      ...   \n",
       "18182                 NaN  ...   NaN   NaN   NaN   NaN   NaN      NaN   \n",
       "18183                 NaN  ...   NaN   NaN   NaN   NaN   NaN      NaN   \n",
       "18184                 NaN  ...   0.0   0.0   0.0   1.0   3.0      3.0   \n",
       "18185                 NaN  ...   NaN   NaN   NaN   NaN   NaN      NaN   \n",
       "18186                 NaN  ...   NaN   NaN   NaN   NaN   NaN      NaN   \n",
       "\n",
       "       readmission_30d  postop_comp   los DischargeDate  \n",
       "0                  NaN          0.0   NaN    2012-04-27  \n",
       "1                  NaN          NaN   NaN    2011-08-26  \n",
       "2                  NaN          NaN  16.0    2010-03-08  \n",
       "3                  NaN          NaN   8.0    2009-03-05  \n",
       "4                  NaN          NaN  13.0    2008-03-06  \n",
       "...                ...          ...   ...           ...  \n",
       "18182              NaN          NaN   NaN           NaT  \n",
       "18183              NaN          NaN   NaN           NaT  \n",
       "18184              NaN          NaN   NaN           NaT  \n",
       "18185              NaN          NaN   NaN           NaT  \n",
       "18186              NaN          NaN   NaN           NaT  \n",
       "\n",
       "[18187 rows x 70 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read the CSV file - linked file \n",
    "file_path = \"Merged_TSQIC_REDCap_ACCESS.xlsx\" \n",
    "df = pd.read_excel(file_path)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                              ge1         ge2         ge3         ge4  \\\n",
      "1_month_postop_arm_1    73.961219   73.891967   74.445983   74.099723   \n",
      "1_year_postop_arm_1     66.197183   66.197183   66.549296   66.021127   \n",
      "2_years_postop_arm_1    64.417178   64.723926   64.417178   64.417178   \n",
      "3_months_postop_arm_1   65.807963   66.510539   66.276347   66.042155   \n",
      "3_years_postop_arm_1    65.853659   66.666667   66.260163   65.853659   \n",
      "4_years_postop_arm_1    66.438356   67.123288   66.438356   66.438356   \n",
      "5_years_postop_arm_1    94.219011   94.274597   94.219011   94.219011   \n",
      "6_months_postop_arm_1   66.666667   66.847826   66.847826   67.028986   \n",
      "baseline_arm_1          79.885277   80.516252   81.242830   80.152964   \n",
      "preoperative_arm_1      82.427462   82.509195   82.631794   82.427462   \n",
      "surgery_arm_1          100.000000  100.000000  100.000000  100.000000   \n",
      "\n",
      "                              ge5         ge6  \n",
      "1_month_postop_arm_1    74.030471   73.961219  \n",
      "1_year_postop_arm_1     66.197183   66.197183  \n",
      "2_years_postop_arm_1    64.723926   64.417178  \n",
      "3_months_postop_arm_1   66.276347   65.807963  \n",
      "3_years_postop_arm_1    66.260163   65.853659  \n",
      "4_years_postop_arm_1    67.123288   66.438356  \n",
      "5_years_postop_arm_1    94.219011   94.219011  \n",
      "6_months_postop_arm_1   67.210145   66.847826  \n",
      "baseline_arm_1          80.210325   80.114723  \n",
      "preoperative_arm_1      82.509195   82.509195  \n",
      "surgery_arm_1          100.000000  100.000000  \n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x800 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# First, let's create a function to calculate missing value percentages\n",
    "def calculate_missing_percentages(df, columns, group_col):\n",
    "    # Group by the specified column\n",
    "    grouped = df.groupby(group_col)\n",
    "    \n",
    "    # Initialize a dictionary to store results\n",
    "    missing_percentages = {}\n",
    "    \n",
    "    # Calculate missing percentages for each group\n",
    "    for name, group in grouped:\n",
    "        # Calculate percentage of missing values for each column\n",
    "        missing_pct = group[columns].isna().mean() * 100\n",
    "        missing_percentages[name] = missing_pct\n",
    "    \n",
    "    # Convert to DataFrame for better visualization\n",
    "    result_df = pd.DataFrame(missing_percentages)\n",
    "    \n",
    "    return result_df.T  # Transpose for better readability\n",
    "\n",
    "# Columns of interest\n",
    "ge_columns = ['ge1', 'ge2', 'ge3', 'ge4', 'ge5', 'ge6']\n",
    "\n",
    "# Calculate missing percentages for each redcap_event_name\n",
    "missing_pct_by_event = calculate_missing_percentages(df, ge_columns, 'redcap_event_name')\n",
    "\n",
    "# Display the results\n",
    "print(missing_pct_by_event)\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(missing_pct_by_event, annot=True, cmap='YlGnBu', fmt='.1f')\n",
    "plt.title('Missing Values Percentage by Event and Column')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>operation_date</th>\n",
       "      <th>redcap_event_name</th>\n",
       "      <th>ComplicationDate</th>\n",
       "      <th>dob</th>\n",
       "      <th>qol_date</th>\n",
       "      <th>age_diagnosis</th>\n",
       "      <th>gender</th>\n",
       "      <th>overall_primary_tumour</th>\n",
       "      <th>overall_regional_ln</th>\n",
       "      <th>...</th>\n",
       "      <th>a_e5</th>\n",
       "      <th>a_e6</th>\n",
       "      <th>a_e7</th>\n",
       "      <th>a_c6</th>\n",
       "      <th>a_c2</th>\n",
       "      <th>a_act11</th>\n",
       "      <th>readmission_30d</th>\n",
       "      <th>postop_comp</th>\n",
       "      <th>los</th>\n",
       "      <th>DischargeDate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaT</td>\n",
       "      <td>baseline_arm_1</td>\n",
       "      <td>NaT</td>\n",
       "      <td>2049-08-04</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2012-04-27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1</td>\n",
       "      <td>NaT</td>\n",
       "      <td>baseline_arm_1</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2</td>\n",
       "      <td>NaT</td>\n",
       "      <td>baseline_arm_1</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>2</td>\n",
       "      <td>NaT</td>\n",
       "      <td>baseline_arm_1</td>\n",
       "      <td>NaT</td>\n",
       "      <td>2041-02-25</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>2</td>\n",
       "      <td>NaT</td>\n",
       "      <td>baseline_arm_1</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>2007-01-12</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18178</th>\n",
       "      <td>1769</td>\n",
       "      <td>NaT</td>\n",
       "      <td>baseline_arm_1</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18179</th>\n",
       "      <td>1769</td>\n",
       "      <td>NaT</td>\n",
       "      <td>baseline_arm_1</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>2025-03-10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18180</th>\n",
       "      <td>1769</td>\n",
       "      <td>NaT</td>\n",
       "      <td>baseline_arm_1</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18181</th>\n",
       "      <td>1770</td>\n",
       "      <td>NaT</td>\n",
       "      <td>baseline_arm_1</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>42.054252</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18185</th>\n",
       "      <td>1770</td>\n",
       "      <td>NaT</td>\n",
       "      <td>baseline_arm_1</td>\n",
       "      <td>NaT</td>\n",
       "      <td>1982-10-12</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5230 rows Ã— 70 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         id operation_date redcap_event_name ComplicationDate        dob  \\\n",
       "0         1            NaT    baseline_arm_1              NaT 2049-08-04   \n",
       "14        1            NaT    baseline_arm_1              NaT        NaT   \n",
       "18        2            NaT    baseline_arm_1              NaT        NaT   \n",
       "22        2            NaT    baseline_arm_1              NaT 2041-02-25   \n",
       "25        2            NaT    baseline_arm_1              NaT        NaT   \n",
       "...     ...            ...               ...              ...        ...   \n",
       "18178  1769            NaT    baseline_arm_1              NaT        NaT   \n",
       "18179  1769            NaT    baseline_arm_1              NaT        NaT   \n",
       "18180  1769            NaT    baseline_arm_1              NaT        NaT   \n",
       "18181  1770            NaT    baseline_arm_1              NaT        NaT   \n",
       "18185  1770            NaT    baseline_arm_1              NaT 1982-10-12   \n",
       "\n",
       "        qol_date  age_diagnosis  gender overall_primary_tumour  \\\n",
       "0            NaT            NaN     1.0                    NaN   \n",
       "14           NaT            NaN     NaN                      2   \n",
       "18           NaT            NaN     NaN                      3   \n",
       "22           NaT            NaN     1.0                    NaN   \n",
       "25    2007-01-12            NaN     NaN                    NaN   \n",
       "...          ...            ...     ...                    ...   \n",
       "18178        NaT            NaN     NaN                    NaN   \n",
       "18179 2025-03-10            NaN     NaN                    NaN   \n",
       "18180        NaT            NaN     NaN                      3   \n",
       "18181        NaT      42.054252     NaN                      3   \n",
       "18185        NaT            NaN     1.0                    NaN   \n",
       "\n",
       "      overall_regional_ln  ...  a_e5  a_e6  a_e7  a_c6  a_c2  a_act11  \\\n",
       "0                     NaN  ...   NaN   NaN   NaN   NaN   NaN      NaN   \n",
       "14                      0  ...   NaN   NaN   NaN   NaN   NaN      NaN   \n",
       "18                      1  ...   NaN   NaN   NaN   NaN   NaN      NaN   \n",
       "22                    NaN  ...   NaN   NaN   NaN   NaN   NaN      NaN   \n",
       "25                    NaN  ...   0.0   4.0   0.0   4.0   0.0      1.0   \n",
       "...                   ...  ...   ...   ...   ...   ...   ...      ...   \n",
       "18178                 NaN  ...   NaN   NaN   NaN   NaN   NaN      NaN   \n",
       "18179                 NaN  ...   0.0   0.0   0.0   0.0   1.0      1.0   \n",
       "18180                   1  ...   NaN   NaN   NaN   NaN   NaN      NaN   \n",
       "18181                   0  ...   NaN   NaN   NaN   NaN   NaN      NaN   \n",
       "18185                 NaN  ...   NaN   NaN   NaN   NaN   NaN      NaN   \n",
       "\n",
       "       readmission_30d  postop_comp  los DischargeDate  \n",
       "0                  NaN          0.0  NaN    2012-04-27  \n",
       "14                 NaN          NaN  NaN           NaT  \n",
       "18                 NaN          NaN  NaN           NaT  \n",
       "22                 NaN          NaN  NaN           NaT  \n",
       "25                 NaN          NaN  NaN           NaT  \n",
       "...                ...          ...  ...           ...  \n",
       "18178              NaN          NaN  NaN           NaT  \n",
       "18179              NaN          NaN  NaN           NaT  \n",
       "18180              NaN          NaN  NaN           NaT  \n",
       "18181              NaN          NaN  NaN           NaT  \n",
       "18185              NaN          NaN  NaN           NaT  \n",
       "\n",
       "[5230 rows x 70 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # Leave only baseline\n",
    "df = df[df['redcap_event_name'] == 'baseline_arm_1']\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'operation_date', 'redcap_event_name', 'ComplicationDate', 'dob',\n",
       "       'qol_date', 'age_diagnosis', 'gender', 'overall_primary_tumour',\n",
       "       'overall_regional_ln', 'overall_distant_metastasis', 'neotx___notx',\n",
       "       'neotx___chemo', 'neotx___rads', 'neotx___chemorads', 'neotx___immuno',\n",
       "       'neotx___other', 'procedure123456', 'expectation_treatment',\n",
       "       'path_esoph_primtumour', 'path_esoph_regionalln',\n",
       "       'path_esoph_distantmetast', 'gp1', 'gp2', 'gp3', 'gp4', 'gp5', 'gp6',\n",
       "       'gp7', 'gs1', 'gs2', 'gs3', 'gs4', 'gs5', 'gs6', 'gs7', 'ge1', 'ge2',\n",
       "       'ge3', 'ge4', 'ge5', 'ge6', 'gf1', 'gf2', 'gf3', 'gf4', 'gf5', 'gf6',\n",
       "       'gf7', 'a_hn1', 'a_hn2', 'a_hn3', 'a_hn4', 'a_hn5', 'a_hn7', 'a_hn10',\n",
       "       'a_e1', 'a_e2', 'a_e3', 'a_e4', 'a_e5', 'a_e6', 'a_e7', 'a_c6', 'a_c2',\n",
       "       'a_act11', 'readmission_30d', 'postop_comp', 'los', 'DischargeDate'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'redcap_event_name', 'qol_date', 'age_diagnosis', 'gender',\n",
       "       'overall_primary_tumour', 'overall_regional_ln',\n",
       "       'overall_distant_metastasis', 'neotx___notx', 'neotx___chemo',\n",
       "       'neotx___rads', 'neotx___chemorads', 'neotx___immuno', 'neotx___other',\n",
       "       'procedure123456', 'expectation_treatment', 'path_esoph_primtumour',\n",
       "       'path_esoph_regionalln', 'path_esoph_distantmetast', 'gp1', 'gp2',\n",
       "       'gp3', 'gp4', 'gp5', 'gp6', 'gp7', 'gs1', 'gs2', 'gs3', 'gs4', 'gs5',\n",
       "       'gs6', 'gs7', 'ge1', 'ge2', 'ge3', 'ge4', 'ge5', 'ge6', 'gf1', 'gf2',\n",
       "       'gf3', 'gf4', 'gf5', 'gf6', 'gf7', 'a_hn1', 'a_hn2', 'a_hn3', 'a_hn4',\n",
       "       'a_hn5', 'a_hn7', 'a_hn10', 'a_e1', 'a_e2', 'a_e3', 'a_e4', 'a_e5',\n",
       "       'a_e6', 'a_e7', 'a_c6', 'a_c2', 'a_act11', 'readmission_30d',\n",
       "       'postop_comp', 'los'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#drop dates\n",
    "df = df.drop(columns=[\"ComplicationDate\", \"dob\", \"operation_date\", \"DischargeDate\"]) #qol_date\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>redcap_event_name</th>\n",
       "      <th>qol_date</th>\n",
       "      <th>age_diagnosis</th>\n",
       "      <th>gender</th>\n",
       "      <th>overall_primary_tumour</th>\n",
       "      <th>overall_regional_ln</th>\n",
       "      <th>overall_distant_metastasis</th>\n",
       "      <th>neotx___notx</th>\n",
       "      <th>neotx___chemo</th>\n",
       "      <th>...</th>\n",
       "      <th>a_e4</th>\n",
       "      <th>a_e5</th>\n",
       "      <th>a_e6</th>\n",
       "      <th>a_e7</th>\n",
       "      <th>a_c6</th>\n",
       "      <th>a_c2</th>\n",
       "      <th>a_act11</th>\n",
       "      <th>readmission_30d</th>\n",
       "      <th>postop_comp</th>\n",
       "      <th>los</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2007-01-12</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18178</th>\n",
       "      <td>1769</td>\n",
       "      <td>0</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18179</th>\n",
       "      <td>1769</td>\n",
       "      <td>0</td>\n",
       "      <td>2025-03-10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18180</th>\n",
       "      <td>1769</td>\n",
       "      <td>0</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18181</th>\n",
       "      <td>1770</td>\n",
       "      <td>0</td>\n",
       "      <td>NaT</td>\n",
       "      <td>42.054252</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18185</th>\n",
       "      <td>1770</td>\n",
       "      <td>0</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5230 rows Ã— 66 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         id redcap_event_name   qol_date  age_diagnosis gender  \\\n",
       "0         1                 0        NaT            NaN    1.0   \n",
       "14        1                 0        NaT            NaN    NaN   \n",
       "18        2                 0        NaT            NaN    NaN   \n",
       "22        2                 0        NaT            NaN    1.0   \n",
       "25        2                 0 2007-01-12            NaN    NaN   \n",
       "...     ...               ...        ...            ...    ...   \n",
       "18178  1769                 0        NaT            NaN    NaN   \n",
       "18179  1769                 0 2025-03-10            NaN    NaN   \n",
       "18180  1769                 0        NaT            NaN    NaN   \n",
       "18181  1770                 0        NaT      42.054252    NaN   \n",
       "18185  1770                 0        NaT            NaN    1.0   \n",
       "\n",
       "      overall_primary_tumour overall_regional_ln  overall_distant_metastasis  \\\n",
       "0                        NaN                 NaN                         NaN   \n",
       "14                         2                   0                         NaN   \n",
       "18                         3                   1                         0.0   \n",
       "22                       NaN                 NaN                         NaN   \n",
       "25                       NaN                 NaN                         NaN   \n",
       "...                      ...                 ...                         ...   \n",
       "18178                    NaN                 NaN                         NaN   \n",
       "18179                    NaN                 NaN                         NaN   \n",
       "18180                      3                   1                         0.0   \n",
       "18181                      3                   0                         0.0   \n",
       "18185                    NaN                 NaN                         NaN   \n",
       "\n",
       "      neotx___notx neotx___chemo  ... a_e4 a_e5 a_e6 a_e7 a_c6 a_c2 a_act11  \\\n",
       "0              NaN           NaN  ...  NaN  NaN  NaN  NaN  NaN  NaN     NaN   \n",
       "14             NaN           NaN  ...  NaN  NaN  NaN  NaN  NaN  NaN     NaN   \n",
       "18             NaN           NaN  ...  NaN  NaN  NaN  NaN  NaN  NaN     NaN   \n",
       "22             NaN           NaN  ...  NaN  NaN  NaN  NaN  NaN  NaN     NaN   \n",
       "25             NaN           NaN  ...  3.0  0.0  4.0  0.0  4.0  0.0     1.0   \n",
       "...            ...           ...  ...  ...  ...  ...  ...  ...  ...     ...   \n",
       "18178          NaN           NaN  ...  NaN  NaN  NaN  NaN  NaN  NaN     NaN   \n",
       "18179          NaN           NaN  ...  2.0  0.0  0.0  0.0  0.0  1.0     1.0   \n",
       "18180          NaN           NaN  ...  NaN  NaN  NaN  NaN  NaN  NaN     NaN   \n",
       "18181          NaN           NaN  ...  NaN  NaN  NaN  NaN  NaN  NaN     NaN   \n",
       "18185          NaN           NaN  ...  NaN  NaN  NaN  NaN  NaN  NaN     NaN   \n",
       "\n",
       "      readmission_30d  postop_comp  los  \n",
       "0                 NaN          0.0  NaN  \n",
       "14                NaN          NaN  NaN  \n",
       "18                NaN          NaN  NaN  \n",
       "22                NaN          NaN  NaN  \n",
       "25                NaN          NaN  NaN  \n",
       "...               ...          ...  ...  \n",
       "18178             NaN          NaN  NaN  \n",
       "18179             NaN          NaN  NaN  \n",
       "18180             NaN          NaN  NaN  \n",
       "18181             NaN          NaN  NaN  \n",
       "18185             NaN          NaN  NaN  \n",
       "\n",
       "[5230 rows x 66 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 1: Preprocess categorical variables\n",
    "# Low-cardinality variables: Convert to category dtype\n",
    "low_cardinality_cols = [\n",
    "    'postop_comp', 'readmission_30d', 'gender', 'neotx___notx', 'neotx___chemo',\n",
    "    'neotx___rads', 'neotx___chemorads', 'neotx___immuno', 'neotx___other',\n",
    "    'expectation_treatment'\n",
    "]\n",
    "for col in low_cardinality_cols:\n",
    "    df[col] = df[col].astype('category')\n",
    "\n",
    "# High-cardinality variables: Label encode and convert to category\n",
    "le_redcap = LabelEncoder()\n",
    "df['redcap_event_name'] = df['redcap_event_name'].astype(str)  # Convert to string to handle NaN\n",
    "df['redcap_event_name'] = le_redcap.fit_transform(df['redcap_event_name'])\n",
    "df['redcap_event_name'] = df['redcap_event_name'].astype('category')\n",
    "\n",
    "# procedure123456 is already numerical but should be treated as categorical\n",
    "df['procedure123456'] = df['procedure123456'].astype('category')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>redcap_event_name</th>\n",
       "      <th>qol_date</th>\n",
       "      <th>age_diagnosis</th>\n",
       "      <th>gender</th>\n",
       "      <th>overall_primary_tumour</th>\n",
       "      <th>overall_regional_ln</th>\n",
       "      <th>overall_distant_metastasis</th>\n",
       "      <th>neotx___notx</th>\n",
       "      <th>neotx___chemo</th>\n",
       "      <th>...</th>\n",
       "      <th>a_e4</th>\n",
       "      <th>a_e5</th>\n",
       "      <th>a_e6</th>\n",
       "      <th>a_e7</th>\n",
       "      <th>a_c6</th>\n",
       "      <th>a_c2</th>\n",
       "      <th>a_act11</th>\n",
       "      <th>readmission_30d</th>\n",
       "      <th>postop_comp</th>\n",
       "      <th>los</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>nan</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2007-01-12</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18178</th>\n",
       "      <td>1769</td>\n",
       "      <td>0</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18179</th>\n",
       "      <td>1769</td>\n",
       "      <td>0</td>\n",
       "      <td>2025-03-10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18180</th>\n",
       "      <td>1769</td>\n",
       "      <td>0</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18181</th>\n",
       "      <td>1770</td>\n",
       "      <td>0</td>\n",
       "      <td>NaT</td>\n",
       "      <td>42.054252</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18185</th>\n",
       "      <td>1770</td>\n",
       "      <td>0</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5230 rows Ã— 66 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         id redcap_event_name   qol_date  age_diagnosis gender  \\\n",
       "0         1                 0        NaT            NaN    1.0   \n",
       "14        1                 0        NaT            NaN    NaN   \n",
       "18        2                 0        NaT            NaN    NaN   \n",
       "22        2                 0        NaT            NaN    1.0   \n",
       "25        2                 0 2007-01-12            NaN    NaN   \n",
       "...     ...               ...        ...            ...    ...   \n",
       "18178  1769                 0        NaT            NaN    NaN   \n",
       "18179  1769                 0 2025-03-10            NaN    NaN   \n",
       "18180  1769                 0        NaT            NaN    NaN   \n",
       "18181  1770                 0        NaT      42.054252    NaN   \n",
       "18185  1770                 0        NaT            NaN    1.0   \n",
       "\n",
       "      overall_primary_tumour overall_regional_ln overall_distant_metastasis  \\\n",
       "0                        nan                 nan                        nan   \n",
       "14                         2                   0                        nan   \n",
       "18                         3                   1                        0.0   \n",
       "22                       nan                 nan                        nan   \n",
       "25                       nan                 nan                        nan   \n",
       "...                      ...                 ...                        ...   \n",
       "18178                    nan                 nan                        nan   \n",
       "18179                    nan                 nan                        nan   \n",
       "18180                      3                   1                        0.0   \n",
       "18181                      3                   0                        0.0   \n",
       "18185                    nan                 nan                        nan   \n",
       "\n",
       "      neotx___notx neotx___chemo  ... a_e4 a_e5 a_e6 a_e7 a_c6 a_c2 a_act11  \\\n",
       "0              NaN           NaN  ...  NaN  NaN  NaN  NaN  NaN  NaN     NaN   \n",
       "14             NaN           NaN  ...  NaN  NaN  NaN  NaN  NaN  NaN     NaN   \n",
       "18             NaN           NaN  ...  NaN  NaN  NaN  NaN  NaN  NaN     NaN   \n",
       "22             NaN           NaN  ...  NaN  NaN  NaN  NaN  NaN  NaN     NaN   \n",
       "25             NaN           NaN  ...  3.0  0.0  4.0  0.0  4.0  0.0     1.0   \n",
       "...            ...           ...  ...  ...  ...  ...  ...  ...  ...     ...   \n",
       "18178          NaN           NaN  ...  NaN  NaN  NaN  NaN  NaN  NaN     NaN   \n",
       "18179          NaN           NaN  ...  2.0  0.0  0.0  0.0  0.0  1.0     1.0   \n",
       "18180          NaN           NaN  ...  NaN  NaN  NaN  NaN  NaN  NaN     NaN   \n",
       "18181          NaN           NaN  ...  NaN  NaN  NaN  NaN  NaN  NaN     NaN   \n",
       "18185          NaN           NaN  ...  NaN  NaN  NaN  NaN  NaN  NaN     NaN   \n",
       "\n",
       "      readmission_30d postop_comp  los  \n",
       "0                 NaN         0.0  NaN  \n",
       "14                NaN         NaN  NaN  \n",
       "18                NaN         NaN  NaN  \n",
       "22                NaN         NaN  NaN  \n",
       "25                NaN         NaN  NaN  \n",
       "...               ...         ...  ...  \n",
       "18178             NaN         NaN  NaN  \n",
       "18179             NaN         NaN  NaN  \n",
       "18180             NaN         NaN  NaN  \n",
       "18181             NaN         NaN  NaN  \n",
       "18185             NaN         NaN  NaN  \n",
       "\n",
       "[5230 rows x 66 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 2: Preprocess numerical and ordinal columns\n",
    "# True numerical columns: Ensure float/int dtype\n",
    "numerical_cols = ['los', 'age_diagnosis']\n",
    "for col in numerical_cols:\n",
    "    df[col] = df[col].astype(float)\n",
    "\n",
    "#Step 3\n",
    "# Ordinal columns: Treat as numerical (already float)\n",
    "ordinal_cols = (\n",
    "    [f\"gp{i}\" for i in range(1, 8)] + [f\"gs{i}\" for i in range(1, 8)] +\n",
    "    [f\"ge{i}\" for i in range(1, 7)] + [f\"gf{i}\" for i in range(1, 8)] +\n",
    "    [f\"a_hn{i}\" for i in range(1, 6)] + [\"a_hn7\", \"a_hn10\"] +\n",
    "    [f\"a_e{i}\" for i in range(1, 8)] + [\"a_c6\", \"a_c2\", \"a_act11\"]\n",
    ")\n",
    "# Subset for this example\n",
    "ordinal_cols = [col for col in ordinal_cols if col in df.columns]\n",
    "for col in ordinal_cols:\n",
    "    df[col] = df[col].astype(float)\n",
    "\n",
    "#Step 4\n",
    "# Categorical-like columns: Treat as categorical\n",
    "categorical_like_cols = [\n",
    "    'overall_primary_tumour', 'overall_regional_ln', 'overall_distant_metastasis', \n",
    "    'path_esoph_primtumour', 'path_esoph_regionalln', 'path_esoph_distantmetast'\n",
    "]\n",
    "for col in categorical_like_cols:\n",
    "    df[col] = df[col].astype(str)  # Convert to string to handle mixed types\n",
    "    df[col] = df[col].astype('category')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column 'redcap_event_name' has only one unique value: 0\n",
      "Column 'path_esoph_primtumour' has only one unique value: nan\n",
      "Column 'path_esoph_regionalln' has only one unique value: nan\n",
      "Column 'path_esoph_distantmetast' has only one unique value: nan\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def analyze_categorical_columns(dataframe):\n",
    "    \"\"\"\n",
    "    Analyzes all categorical columns in a dataframe to check if they contain only one unique value.\n",
    "    \n",
    "    Args:\n",
    "        dataframe: pandas DataFrame to analyze\n",
    "        \n",
    "    Returns:\n",
    "        dict: Dictionary with column names as keys and tuples (has_single_category, unique_value) as values\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    \n",
    "    # Identify categorical columns\n",
    "    # This includes object dtype, category dtype, and any other non-numeric columns\n",
    "    categorical_columns = dataframe.select_dtypes(\n",
    "        include=['object', 'category', 'bool']\n",
    "    ).columns.tolist()\n",
    "    \n",
    "    # For each categorical column, check if it has only one unique value\n",
    "    for column in categorical_columns:\n",
    "        # Skip columns with all null values\n",
    "        if dataframe[column].isna().all():\n",
    "            results[column] = (False, None, \"All values are null\")\n",
    "            continue\n",
    "            \n",
    "        # Get unique non-null values\n",
    "        unique_values = dataframe[column].dropna().unique()\n",
    "        \n",
    "        # Check if there's only one unique category\n",
    "        has_single_category = len(unique_values) == 1\n",
    "        \n",
    "        if has_single_category:\n",
    "            results[column] = (True, unique_values[0], None)\n",
    "        else:\n",
    "            results[column] = (False, None, f\"Found {len(unique_values)} unique values\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Example usage:\n",
    "column_analysis = analyze_categorical_columns(df)\n",
    "\n",
    "# Print results\n",
    "for column, (has_single_value, unique_value, message) in column_analysis.items():\n",
    "    if has_single_value:\n",
    "        print(f\"Column '{column}' has only one unique value: {unique_value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropped 4 columns with only one unique value:\n",
      "  - redcap_event_name\n",
      "  - path_esoph_primtumour\n",
      "  - path_esoph_regionalln\n",
      "  - path_esoph_distantmetast\n"
     ]
    }
   ],
   "source": [
    "def drop_single_value_categorical_columns(dataframe, inplace=False):\n",
    "    \"\"\"\n",
    "    Identifies and drops all categorical columns that contain only one unique value.\n",
    "    \n",
    "    Args:\n",
    "        dataframe: pandas DataFrame to process\n",
    "        inplace: Whether to modify the original dataframe (True) or return a copy (False)\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame: Modified dataframe with single-value categorical columns removed\n",
    "        list: List of column names that were dropped\n",
    "    \"\"\"\n",
    "    # Make a copy if not inplace\n",
    "    df = dataframe if inplace else dataframe.copy()\n",
    "    \n",
    "    # Identify categorical columns\n",
    "    categorical_columns = df.select_dtypes(\n",
    "        include=['object', 'category', 'bool']\n",
    "    ).columns.tolist()\n",
    "    \n",
    "    # Track columns to drop\n",
    "    columns_to_drop = []\n",
    "    \n",
    "    # Check each categorical column\n",
    "    for column in categorical_columns:\n",
    "        # Skip columns with all null values\n",
    "        if df[column].isna().all():\n",
    "            continue\n",
    "            \n",
    "        # Get unique non-null values\n",
    "        unique_values = df[column].dropna().unique()\n",
    "        \n",
    "        # If only one unique value, add to drop list\n",
    "        if len(unique_values) == 1:\n",
    "            columns_to_drop.append(column)\n",
    "    \n",
    "    # Drop the identified columns\n",
    "    if columns_to_drop:\n",
    "        df.drop(columns=columns_to_drop, inplace=True)\n",
    "        \n",
    "    return df, columns_to_drop\n",
    "\n",
    "# Example usage:\n",
    "# Drop single-value categorical columns\n",
    "df_cleaned, dropped_columns = drop_single_value_categorical_columns(df, inplace=False)\n",
    "\n",
    "# Report results\n",
    "if dropped_columns:\n",
    "    print(f\"Dropped {len(dropped_columns)} columns with only one unique value:\")\n",
    "    for col in dropped_columns:\n",
    "        print(f\"  - {col}\")\n",
    "else:\n",
    "    print(\"No single-value categorical columns found.\")\n",
    "\n",
    "df = df_cleaned.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model - MICE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_mice_imputation(df, columns_to_impute, validation_df=None, validation_masks=None, original_values=None):\n",
    "    \"\"\"\n",
    "    Apply MICE imputation using miceforest package\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pandas.DataFrame\n",
    "        Data with missing values\n",
    "    columns_to_impute : list\n",
    "        List of column names to impute\n",
    "    validation_df : pandas.DataFrame, optional\n",
    "        Validation dataset with artificially missing values\n",
    "    validation_masks : dict, optional\n",
    "        Dictionary of masks for validation data\n",
    "    original_values : dict, optional\n",
    "        Dictionary of original values for validation\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    imputed_df : pandas.DataFrame\n",
    "        Data with imputed values\n",
    "    validation_results : dict, optional\n",
    "        Validation results if validation data provided\n",
    "    \"\"\"\n",
    "\n",
    "    # Set threads for LightGBM\n",
    "    os.environ['OMP_NUM_THREADS'] = '10'\n",
    "    \n",
    "    # Initialize the imputation kernel\n",
    "    kernel = mf.ImputationKernel(\n",
    "        df,\n",
    "        datasets=1,\n",
    "        variable_schema={\n",
    "            col: [c for c in df.columns if c != col] for col in columns_to_impute\n",
    "        },\n",
    "        random_state=42  # Using fixed seed for reproducibility, can be parameterized\n",
    "    )\n",
    "    \n",
    "    # Run imputation\n",
    "    for _ in tqdm(range(5), desc=\"MICE Imputation\"):\n",
    "        kernel.mice(\n",
    "            iterations=1,\n",
    "            verbose=False,\n",
    "            num_boost_round=80,\n",
    "            max_depth=10,\n",
    "            num_threads=10\n",
    "        )\n",
    "    \n",
    "    # Get imputed data\n",
    "    imputed_df = kernel.complete_data(0)\n",
    "    \n",
    "    # Check if there's a label encoder for redcap_event_name that needs inverse transformation\n",
    "    if 'redcap_event_name' in imputed_df.columns:\n",
    "        try:\n",
    "            # This is optional - only execute if le_redcap exists in the global scope\n",
    "            if 'le_redcap' in globals():\n",
    "                # Check if we're dealing with numeric values (could be int or float)\n",
    "                if pd.api.types.is_numeric_dtype(imputed_df['redcap_event_name']) or \\\n",
    "                   (hasattr(imputed_df['redcap_event_name'], 'cat') and pd.api.types.is_numeric_dtype(imputed_df['redcap_event_name'].cat.categories)):\n",
    "                    imputed_df['redcap_event_name'] = globals()['le_redcap'].inverse_transform(imputed_df['redcap_event_name'].astype(int))\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Could not inverse transform redcap_event_name: {e}\")\n",
    "    \n",
    "    # Validate if validation data provided\n",
    "    validation_results = None\n",
    "    if validation_df is not None and validation_masks is not None and original_values is not None:\n",
    "        validation_results = {}\n",
    "        \n",
    "        # Compare imputed values to real values\n",
    "        for col in columns_to_impute:\n",
    "            # Get indices where values were artificially set to NaN\n",
    "            mask = validation_masks[col] & validation_df[col].isna()\n",
    "            \n",
    "            if mask.sum() == 0:\n",
    "                validation_results[col] = {\n",
    "                    'error': \"No artificially missing values\"\n",
    "                }\n",
    "                continue\n",
    "                \n",
    "            real_vals = original_values[col][mask]\n",
    "            imputed_vals = imputed_df[col][mask]\n",
    "            \n",
    "            # Calculate MAE and RMSE\n",
    "            mae = mean_absolute_error(real_vals, imputed_vals)\n",
    "            rmse = np.sqrt(mean_squared_error(real_vals, imputed_vals))\n",
    "            \n",
    "            validation_results[col] = {\n",
    "                'mae': mae,\n",
    "                'rmse': rmse,\n",
    "                'real_distribution': real_vals.describe(),\n",
    "                'imputed_distribution': imputed_vals.describe()\n",
    "            }\n",
    "    \n",
    "    return imputed_df, validation_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MICE Imputation:   0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MICE Imputation:   0%|          | 0/5 [00:05<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[49], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m columns_to_impute \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mge1\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mge2\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mge3\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mge4\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mge5\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mge6\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m----> 2\u001b[0m imputed_df_mice, validation_results_mice \u001b[38;5;241m=\u001b[39m \u001b[43mapply_mice_imputation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns_to_impute\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[48], line 41\u001b[0m, in \u001b[0;36mapply_mice_imputation\u001b[0;34m(df, columns_to_impute, validation_df, validation_masks, original_values)\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;66;03m# Run imputation\u001b[39;00m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m5\u001b[39m), desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMICE Imputation\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m---> 41\u001b[0m     \u001b[43mkernel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmice\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     42\u001b[0m \u001b[43m        \u001b[49m\u001b[43miterations\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     43\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     44\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_boost_round\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m80\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     45\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_depth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     46\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_threads\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\n\u001b[1;32m     47\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;66;03m# Get imputed data\u001b[39;00m\n\u001b[1;32m     50\u001b[0m imputed_df \u001b[38;5;241m=\u001b[39m kernel\u001b[38;5;241m.\u001b[39mcomplete_data(\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[0;32m~/jinenv/lib/python3.11/site-packages/miceforest/ImputationKernel.py:1114\u001b[0m, in \u001b[0;36mImputationKernel.mice\u001b[0;34m(self, iterations, verbose, variable_parameters, compile_candidates, **kwlgb)\u001b[0m\n\u001b[1;32m   1112\u001b[0m logger\u001b[38;5;241m.\u001b[39mrecord_time(timed_event\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprepare_xy\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mlog_context)\n\u001b[1;32m   1113\u001b[0m logger\u001b[38;5;241m.\u001b[39mset_start_time()\n\u001b[0;32m-> 1114\u001b[0m current_model \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1115\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlgbpars\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1116\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_set\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_pointer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1117\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_boost_round\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_iterations\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1118\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcategorical_feature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfeature_cat_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1119\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1120\u001b[0m logger\u001b[38;5;241m.\u001b[39mrecord_time(timed_event\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtraining\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mlog_context)\n\u001b[1;32m   1122\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m save_model:\n",
      "File \u001b[0;32m~/jinenv/lib/python3.11/site-packages/lightgbm/engine.py:307\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(params, train_set, num_boost_round, valid_sets, valid_names, feval, init_model, feature_name, categorical_feature, keep_training_booster, callbacks)\u001b[0m\n\u001b[1;32m    295\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m cb \u001b[38;5;129;01min\u001b[39;00m callbacks_before_iter:\n\u001b[1;32m    296\u001b[0m     cb(\n\u001b[1;32m    297\u001b[0m         callback\u001b[38;5;241m.\u001b[39mCallbackEnv(\n\u001b[1;32m    298\u001b[0m             model\u001b[38;5;241m=\u001b[39mbooster,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    304\u001b[0m         )\n\u001b[1;32m    305\u001b[0m     )\n\u001b[0;32m--> 307\u001b[0m \u001b[43mbooster\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfobj\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    309\u001b[0m evaluation_result_list: List[_LGBM_BoosterEvalMethodResultType] \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    310\u001b[0m \u001b[38;5;66;03m# check evaluation result.\u001b[39;00m\n",
      "File \u001b[0;32m~/jinenv/lib/python3.11/site-packages/lightgbm/basic.py:4136\u001b[0m, in \u001b[0;36mBooster.update\u001b[0;34m(self, train_set, fobj)\u001b[0m\n\u001b[1;32m   4133\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__set_objective_to_none:\n\u001b[1;32m   4134\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m LightGBMError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot update due to null objective function.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   4135\u001b[0m _safe_call(\n\u001b[0;32m-> 4136\u001b[0m     \u001b[43m_LIB\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mLGBM_BoosterUpdateOneIter\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   4137\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4138\u001b[0m \u001b[43m        \u001b[49m\u001b[43mctypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbyref\u001b[49m\u001b[43m(\u001b[49m\u001b[43mis_finished\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4139\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4140\u001b[0m )\n\u001b[1;32m   4141\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__is_predicted_cur_iter \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;01mFalse\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__num_dataset)]\n\u001b[1;32m   4142\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m is_finished\u001b[38;5;241m.\u001b[39mvalue \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "columns_to_impute = ['ge1', 'ge2', 'ge3', 'ge4', 'ge5', 'ge6']\n",
    "imputed_df_mice, validation_results_mice = apply_mice_imputation(df, columns_to_impute)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model - VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import sys\n",
    "\n",
    "class VAE(nn.Module):\n",
    "    \"\"\"\n",
    "    Variational VAE optimized for CPU usage\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim, hidden_dims=(64, 32, 16), latent_dim=8, dropout_rate=0.1):\n",
    "        \"\"\"\n",
    "        Initialize VAE model\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        input_dim : int\n",
    "            Dimension of input feature space\n",
    "        hidden_dims : tuple\n",
    "            Sizes of hidden layers in the encoder and decoder\n",
    "        latent_dim : int\n",
    "            Dimension of the latent space\n",
    "        dropout_rate : float\n",
    "            Dropout rate for regularization\n",
    "        \"\"\"\n",
    "        super(VAE, self).__init__()\n",
    "        \n",
    "        # Determine if we should use batch normalization (can be slow on CPU)\n",
    "        self.use_cpu_efficient = True  # Flag for CPU optimization\n",
    "        \n",
    "        # Encoder layers\n",
    "        encoder_layers = []\n",
    "        prev_dim = input_dim\n",
    "        \n",
    "        for h_dim in hidden_dims:\n",
    "            layer_components = [nn.Linear(prev_dim, h_dim)]\n",
    "            \n",
    "            # Use layer norm instead of batch norm if optimizing for CPU\n",
    "            if self.use_cpu_efficient:\n",
    "                layer_components.append(nn.LayerNorm(h_dim))\n",
    "            else:\n",
    "                layer_components.append(nn.BatchNorm1d(h_dim))\n",
    "                \n",
    "            layer_components.extend([\n",
    "                nn.LeakyReLU(),\n",
    "                nn.Dropout(dropout_rate)\n",
    "            ])\n",
    "            \n",
    "            encoder_layers.append(nn.Sequential(*layer_components))\n",
    "            prev_dim = h_dim\n",
    "        \n",
    "        self.encoder_layers = nn.ModuleList(encoder_layers)\n",
    "        \n",
    "        # Latent space mapping\n",
    "        self.mu_layer = nn.Linear(hidden_dims[-1], latent_dim)\n",
    "        self.logvar_layer = nn.Linear(hidden_dims[-1], latent_dim)\n",
    "        \n",
    "        # Decoder layers\n",
    "        decoder_layers = []\n",
    "        decoder_dims = list(reversed(hidden_dims))\n",
    "        \n",
    "        # First decoder layer from latent space\n",
    "        first_layer_components = [nn.Linear(latent_dim, decoder_dims[0])]\n",
    "        \n",
    "        # Use layer norm instead of batch norm if optimizing for CPU\n",
    "        if self.use_cpu_efficient:\n",
    "            first_layer_components.append(nn.LayerNorm(decoder_dims[0]))\n",
    "        else:\n",
    "            first_layer_components.append(nn.BatchNorm1d(decoder_dims[0]))\n",
    "            \n",
    "        first_layer_components.extend([\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Dropout(dropout_rate)\n",
    "        ])\n",
    "        \n",
    "        decoder_layers.append(nn.Sequential(*first_layer_components))\n",
    "        \n",
    "        # Remaining decoder layers\n",
    "        for i in range(len(decoder_dims) - 1):\n",
    "            layer_components = [nn.Linear(decoder_dims[i], decoder_dims[i+1])]\n",
    "            \n",
    "            # Use layer norm instead of batch norm if optimizing for CPU\n",
    "            if self.use_cpu_efficient:\n",
    "                layer_components.append(nn.LayerNorm(decoder_dims[i+1]))\n",
    "            else:\n",
    "                layer_components.append(nn.BatchNorm1d(decoder_dims[i+1]))\n",
    "                \n",
    "            layer_components.extend([\n",
    "                nn.LeakyReLU(),\n",
    "                nn.Dropout(dropout_rate)\n",
    "            ])\n",
    "            \n",
    "            decoder_layers.append(nn.Sequential(*layer_components))\n",
    "        \n",
    "        # Output layer\n",
    "        decoder_layers.append(nn.Linear(decoder_dims[-1], input_dim))\n",
    "        \n",
    "        self.decoder_layers = nn.ModuleList(decoder_layers)\n",
    "        \n",
    "    def encode(self, x):\n",
    "        \"\"\"\n",
    "        Encode input to latent parameters\n",
    "        \"\"\"\n",
    "        h = x\n",
    "        for layer in self.encoder_layers:\n",
    "            h = layer(h)\n",
    "            \n",
    "        mu = self.mu_layer(h)\n",
    "        logvar = self.logvar_layer(h)\n",
    "        \n",
    "        return mu, logvar\n",
    "    \n",
    "    def reparameterize(self, mu, logvar):\n",
    "        \"\"\"\n",
    "        Reparameterization trick\n",
    "        \"\"\"\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        z = mu + eps * std\n",
    "        return z\n",
    "    \n",
    "    def decode(self, z):\n",
    "        \"\"\"\n",
    "        Decode latent vector to reconstructed input\n",
    "        \"\"\"\n",
    "        h = z\n",
    "        for layer in self.decoder_layers:\n",
    "            h = layer(h)\n",
    "            \n",
    "        return h\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass through the VAE\n",
    "        \"\"\"\n",
    "        mu, logvar = self.encode(x)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        x_reconstructed = self.decode(z)\n",
    "        \n",
    "        return x_reconstructed, mu, logvar\n",
    "    \n",
    "    \n",
    "class MaskedMSELoss(nn.Module):\n",
    "    \"\"\"\n",
    "    MSE loss that only considers non-missing values\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(MaskedMSELoss, self).__init__()\n",
    "        \n",
    "    def forward(self, pred, target, mask):\n",
    "        \"\"\"\n",
    "        Calculate MSE loss ignoring missing values\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        pred : torch.Tensor\n",
    "            Predicted values\n",
    "        target : torch.Tensor\n",
    "            Target values\n",
    "        mask : torch.Tensor\n",
    "            Binary mask (1 for observed, 0 for missing)\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        loss : torch.Tensor\n",
    "            Masked MSE loss\n",
    "        \"\"\"\n",
    "        # Only calculate loss for observed values\n",
    "        masked_pred = pred * mask\n",
    "        masked_target = target * mask\n",
    "        \n",
    "        # Calculate squared error\n",
    "        se = (masked_pred - masked_target) ** 2\n",
    "        \n",
    "        # Sum squared error and count observed values\n",
    "        sse = torch.sum(se)\n",
    "        count = torch.sum(mask)\n",
    "        \n",
    "        # Return MSE\n",
    "        return sse / (count + 1e-8)\n",
    "\n",
    "\n",
    "def kl_divergence_loss(mu, logvar):\n",
    "    \"\"\"\n",
    "    Calculate KL divergence loss\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    mu : torch.Tensor\n",
    "        Mean of the latent space\n",
    "    logvar : torch.Tensor\n",
    "        Log variance of the latent space\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    kl_loss : torch.Tensor\n",
    "        KL divergence loss\n",
    "    \"\"\"\n",
    "    # KL divergence between q(z|x) and p(z)\n",
    "    kl_loss = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    return kl_loss\n",
    "\n",
    "\n",
    "class VAEImputer:\n",
    "    \"\"\"\n",
    "    Variational VAE Imputation model using PyTorch\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 hidden_dims=(64, 32, 16),\n",
    "                 latent_dim=8,\n",
    "                 batch_size=64,\n",
    "                 learning_rate=0.001,\n",
    "                 epochs=100,\n",
    "                 beta=1.0,\n",
    "                 device=None,\n",
    "                 verbose=True):\n",
    "        \"\"\"\n",
    "        Initialize VAE imputer\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        hidden_dims : tuple\n",
    "            Sizes of hidden layers in the encoder and decoder\n",
    "        latent_dim : int\n",
    "            Dimension of the latent space\n",
    "        batch_size : int\n",
    "            Batch size for training\n",
    "        learning_rate : float\n",
    "            Learning rate for optimizer\n",
    "        epochs : int\n",
    "            Number of training epochs\n",
    "        beta : float\n",
    "            Weight of the KL divergence loss (beta-VAE)\n",
    "        device : torch.device\n",
    "            Device to use for training (CPU or GPU)\n",
    "        verbose : bool\n",
    "            Whether to print progress\n",
    "        \"\"\"\n",
    "        self.hidden_dims = hidden_dims\n",
    "        self.latent_dim = latent_dim\n",
    "        self.batch_size = batch_size\n",
    "        self.learning_rate = learning_rate\n",
    "        self.epochs = epochs\n",
    "        self.beta = beta\n",
    "        self.verbose = verbose\n",
    "        \n",
    "        # Set device - Compute Canada specific approach\n",
    "        if device is None:\n",
    "            # Check for SLURM environment variables that indicate we're on a cluster\n",
    "            slurm_job_id = os.environ.get('SLURM_JOB_ID')\n",
    "            \n",
    "            if slurm_job_id:\n",
    "                print(f\"Running on Compute Canada cluster (Job ID: {slurm_job_id})\")\n",
    "                \n",
    "                # Check if GPUs were allocated\n",
    "                slurm_gpus = os.environ.get('SLURM_GPUS')\n",
    "                if slurm_gpus:\n",
    "                    print(f\"GPUs allocated: {slurm_gpus}\")\n",
    "                    \n",
    "                # On Compute Canada, we need to look at the environment variables\n",
    "                visible_devices = os.environ.get('CUDA_VISIBLE_DEVICES')\n",
    "                print(f\"CUDA_VISIBLE_DEVICES: {visible_devices}\")\n",
    "                \n",
    "                if torch.cuda.is_available():\n",
    "                    self.device = torch.device('cuda')\n",
    "                    n_gpus = torch.cuda.device_count()\n",
    "                    print(f\"PyTorch sees {n_gpus} GPUs\")\n",
    "                    if n_gpus > 0:\n",
    "                        print(f\"Using GPU: {torch.cuda.get_device_name(0)}\")\n",
    "                        # Show GPU memory\n",
    "                        print(f\"GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "                    else:\n",
    "                        print(\"Warning: torch.cuda.is_available() is True but device_count() is 0\")\n",
    "                        self.device = torch.device('cpu')\n",
    "                else:\n",
    "                    print(\"CUDA not available according to PyTorch\")\n",
    "                    # Try to provide more diagnostics\n",
    "                    try:\n",
    "                        if os.path.exists('/usr/bin/nvidia-smi'):\n",
    "                            import subprocess\n",
    "                            result = subprocess.run(['nvidia-smi'], stdout=subprocess.PIPE)\n",
    "                            print(\"nvidia-smi output:\")\n",
    "                            print(result.stdout.decode('utf-8'))\n",
    "                        else:\n",
    "                            print(\"nvidia-smi not found in /usr/bin\")\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error checking for nvidia-smi: {e}\")\n",
    "                    \n",
    "                    self.device = torch.device('cpu')\n",
    "            else:\n",
    "                # Not on a cluster, use standard detection\n",
    "                if torch.cuda.is_available():\n",
    "                    self.device = torch.device('cuda')\n",
    "                    print(f\"Using GPU: {torch.cuda.get_device_name(0)}\")\n",
    "                else:\n",
    "                    self.device = torch.device('cpu')\n",
    "                    print(\"Using CPU\")\n",
    "        else:\n",
    "            self.device = device\n",
    "            \n",
    "        # Initialize model, scalers and masks\n",
    "        self.model = None\n",
    "        self.scaler = None\n",
    "        self.columns = None\n",
    "        \n",
    "    def fit(self, X, columns_to_impute=None):\n",
    "        \"\"\"\n",
    "        Fit VAE model for imputation\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X : pandas.DataFrame\n",
    "            Data with missing values\n",
    "        columns_to_impute : list, optional\n",
    "            List of column names to impute. If None, all columns with missing values.\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        self : object\n",
    "            Returns self\n",
    "        \"\"\"\n",
    "        # Identify columns to impute if not specified\n",
    "        if columns_to_impute is None:\n",
    "            columns_to_impute = [col for col in X.columns if X[col].isna().any()]\n",
    "        \n",
    "        # Store list of columns to impute and all columns\n",
    "        self.columns_to_impute = columns_to_impute\n",
    "        self.columns = list(X.columns)\n",
    "        \n",
    "        # Create missing value mask (1 for observed, 0 for missing)\n",
    "        missing_mask = ~X.isna()\n",
    "        \n",
    "        # Initialize scaler\n",
    "        self.scaler = StandardScaler()\n",
    "        \n",
    "        # Simple imputer for initial values (will be refined by VAE)\n",
    "        simple_imputer = SimpleImputer(strategy='mean')\n",
    "        X_simple_imputed = pd.DataFrame(\n",
    "            simple_imputer.fit_transform(X),\n",
    "            columns=X.columns,\n",
    "            index=X.index\n",
    "        )\n",
    "        \n",
    "        # Scale data\n",
    "        X_scaled = self.scaler.fit_transform(X_simple_imputed)\n",
    "        \n",
    "        # Convert to PyTorch tensors\n",
    "        X_tensor = torch.FloatTensor(X_scaled)\n",
    "        mask_tensor = torch.FloatTensor(missing_mask.values)\n",
    "        \n",
    "        # Create dataset and dataloader\n",
    "        dataset = TensorDataset(X_tensor, mask_tensor)\n",
    "        # Use multiple workers if on CPU for better performance\n",
    "        num_workers = 0\n",
    "        if self.device.type == 'cpu':\n",
    "            import multiprocessing\n",
    "            num_workers = min(2, multiprocessing.cpu_count() // 2)  # Use at most half the cores\n",
    "        \n",
    "        dataloader = DataLoader(\n",
    "            dataset, \n",
    "            batch_size=self.batch_size, \n",
    "            shuffle=True,\n",
    "            num_workers=num_workers,\n",
    "            pin_memory=(self.device.type == 'cuda')  # Only pin memory if using CUDA\n",
    "        )\n",
    "        \n",
    "        # Initialize model\n",
    "        input_dim = X.shape[1]\n",
    "        self.model = VAE(\n",
    "            input_dim=input_dim,\n",
    "            hidden_dims=self.hidden_dims,\n",
    "            latent_dim=self.latent_dim\n",
    "        ).to(self.device)\n",
    "        \n",
    "        # Initialize optimizer\n",
    "        optimizer = optim.Adam(self.model.parameters(), lr=self.learning_rate)\n",
    "        \n",
    "        # Initialize loss function\n",
    "        recon_loss_fn = MaskedMSELoss()\n",
    "        \n",
    "        # Training loop\n",
    "        self.model.train()\n",
    "        \n",
    "        # Create progress bar if verbose\n",
    "        pbar = range(self.epochs)\n",
    "        if self.verbose:\n",
    "            pbar = tqdm(pbar, desc=\"Training VAE\")\n",
    "            \n",
    "        for epoch in pbar:\n",
    "            epoch_loss = 0.0\n",
    "            epoch_recon_loss = 0.0\n",
    "            epoch_kl_loss = 0.0\n",
    "            \n",
    "            for x, mask in dataloader:\n",
    "                # Move tensors to device\n",
    "                x = x.to(self.device)\n",
    "                mask = mask.to(self.device)\n",
    "                \n",
    "                # Forward pass\n",
    "                x_reconstructed, mu, logvar = self.model(x)\n",
    "                \n",
    "                # Calculate losses\n",
    "                recon_loss = recon_loss_fn(x_reconstructed, x, mask)\n",
    "                kl_loss = kl_divergence_loss(mu, logvar) / x.size(0)  # Normalize by batch size\n",
    "                \n",
    "                # Total loss (beta-VAE formulation)\n",
    "                loss = recon_loss + self.beta * kl_loss\n",
    "                \n",
    "                # Backward pass and optimize\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "                # Accumulate losses\n",
    "                epoch_loss += loss.item()\n",
    "                epoch_recon_loss += recon_loss.item()\n",
    "                epoch_kl_loss += kl_loss.item()\n",
    "            \n",
    "            # Update progress bar\n",
    "            if self.verbose:\n",
    "                pbar.set_postfix({\n",
    "                    \"loss\": epoch_loss / len(dataloader),\n",
    "                    \"recon\": epoch_recon_loss / len(dataloader),\n",
    "                    \"kl\": epoch_kl_loss / len(dataloader)\n",
    "                })\n",
    "                \n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        \"\"\"\n",
    "        Impute missing values using trained VAE model\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X : pandas.DataFrame\n",
    "            Data with missing values\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        imputed_df : pandas.DataFrame\n",
    "            Data with imputed values\n",
    "        \"\"\"\n",
    "        # Create copy to avoid modifying original\n",
    "        imputed_df = X.copy()\n",
    "        \n",
    "        # Get missing value mask\n",
    "        missing_mask = X.isna()\n",
    "        \n",
    "        # Use simple imputation for initial values\n",
    "        simple_imputer = SimpleImputer(strategy='mean')\n",
    "        X_simple_imputed = pd.DataFrame(\n",
    "            simple_imputer.fit_transform(X),\n",
    "            columns=X.columns,\n",
    "            index=X.index\n",
    "        )\n",
    "        \n",
    "        # Scale data\n",
    "        X_scaled = self.scaler.transform(X_simple_imputed)\n",
    "        \n",
    "        # Convert to PyTorch tensor\n",
    "        X_tensor = torch.FloatTensor(X_scaled).to(self.device)\n",
    "        \n",
    "        # Set model to evaluation mode\n",
    "        self.model.eval()\n",
    "        \n",
    "        # Disable gradient calculation\n",
    "        with torch.no_grad():\n",
    "            # Forward pass through VAE\n",
    "            X_reconstructed, _, _ = self.model(X_tensor)\n",
    "            \n",
    "            # Convert reconstructed values to numpy\n",
    "            X_reconstructed_np = X_reconstructed.cpu().numpy()\n",
    "            \n",
    "            # Inverse transform to original scale\n",
    "            X_reconstructed_orig = self.scaler.inverse_transform(X_reconstructed_np)\n",
    "            \n",
    "            # Create DataFrame with reconstructed values\n",
    "            X_reconstructed_df = pd.DataFrame(\n",
    "                X_reconstructed_orig,\n",
    "                columns=X.columns,\n",
    "                index=X.index\n",
    "            )\n",
    "            \n",
    "            # Replace missing values with imputed values for specified columns\n",
    "            for col in self.columns_to_impute:\n",
    "                # Only replace missing values\n",
    "                imputed_df.loc[missing_mask[col], col] = X_reconstructed_df.loc[missing_mask[col], col]\n",
    "        \n",
    "        return imputed_df\n",
    "    \n",
    "    def fit_transform(self, X, columns_to_impute=None):\n",
    "        \"\"\"\n",
    "        Fit model and impute missing values\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X : pandas.DataFrame\n",
    "            Data with missing values\n",
    "        columns_to_impute : list, optional\n",
    "            List of column names to impute. If None, all columns with missing values.\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        imputed_df : pandas.DataFrame\n",
    "            Data with imputed values\n",
    "        \"\"\"\n",
    "        self.fit(X, columns_to_impute)\n",
    "        return self.transform(X)\n",
    "\n",
    "\n",
    "def apply_vae_imputation(df, columns_to_impute, validation_df=None, validation_masks=None, original_values=None,\n",
    "                        hidden_dims=(64, 32, 16), latent_dim=8, batch_size=64, epochs=100, learning_rate=0.001):\n",
    "    \"\"\"\n",
    "    Apply VAE imputation to data\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pandas.DataFrame\n",
    "        Data with missing values\n",
    "    columns_to_impute : list\n",
    "        List of column names to impute\n",
    "    validation_df : pandas.DataFrame, optional\n",
    "        Validation dataset with artificially missing values\n",
    "    validation_masks : dict, optional\n",
    "        Dictionary of masks for validation data\n",
    "    original_values : dict, optional\n",
    "        Dictionary of original values for validation\n",
    "    hidden_dims : tuple\n",
    "        Sizes of hidden layers\n",
    "    latent_dim : int\n",
    "        Dimension of the latent space\n",
    "    batch_size : int\n",
    "        Batch size for training\n",
    "    epochs : int\n",
    "        Number of training epochs\n",
    "    learning_rate : float\n",
    "        Learning rate for optimizer\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    imputed_df : pandas.DataFrame\n",
    "        Data with imputed values\n",
    "    validation_results : dict, optional\n",
    "        Validation results if validation data provided\n",
    "    \"\"\"\n",
    "    # Extract relevant columns including potential predictors\n",
    "    # Use all columns except those with excessive missing values\n",
    "    threshold = 0.5  # Columns with more than 50% missing values are excluded\n",
    "    columns_to_use = [col for col in df.columns \n",
    "                     if df[col].isna().mean() < threshold]\n",
    "    \n",
    "    # Ensure all columns_to_impute are included\n",
    "    for col in columns_to_impute:\n",
    "        if col not in columns_to_use:\n",
    "            columns_to_use.append(col)\n",
    "    \n",
    "    # Extract subset of data\n",
    "    X = df[columns_to_use].copy()\n",
    "    \n",
    "    # Ensure all columns are numeric\n",
    "    for col in X.columns:\n",
    "        X[col] = pd.to_numeric(X[col], errors='coerce')\n",
    "    \n",
    "    # Compute Canada specific detection\n",
    "    print(\"\\n--- Compute Canada / Cluster Environment Detection ---\")\n",
    "    slurm_info = {\n",
    "        'SLURM_JOB_ID': os.environ.get('SLURM_JOB_ID', 'Not found'),\n",
    "        'SLURM_JOB_GPUS': os.environ.get('SLURM_JOB_GPUS', 'Not found'),\n",
    "        'SLURM_GPUS': os.environ.get('SLURM_GPUS', 'Not found'), \n",
    "        'CUDA_VISIBLE_DEVICES': os.environ.get('CUDA_VISIBLE_DEVICES', 'Not found'),\n",
    "        'SLURM_JOB_PARTITION': os.environ.get('SLURM_JOB_PARTITION', 'Not found'),\n",
    "    }\n",
    "    \n",
    "    for key, value in slurm_info.items():\n",
    "        print(f\"{key}: {value}\")\n",
    "    \n",
    "    # Check CUDA availability with Compute Canada settings\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device('cuda')\n",
    "        n_gpus = torch.cuda.device_count()\n",
    "        print(f\"\\nPyTorch detects {n_gpus} GPUs\")\n",
    "        \n",
    "        # On Compute Canada clusters, sometimes device_count is incorrect\n",
    "        # but we can still use cuda:0 if CUDA_VISIBLE_DEVICES is set\n",
    "        if n_gpus == 0 and os.environ.get('CUDA_VISIBLE_DEVICES') is not None:\n",
    "            print(\"Detected potential Compute Canada environment mismatch\")\n",
    "            print(\"Attempting to force GPU usage...\")\n",
    "            try:\n",
    "                # Try to explicitly set the device\n",
    "                device = torch.device('cuda:0')\n",
    "                # Test if it works\n",
    "                test_tensor = torch.zeros(1).to(device)\n",
    "                print(\"Successfully forced GPU usage!\")\n",
    "            except Exception as e:\n",
    "                print(f\"Force attempt failed: {e}\")\n",
    "                device = torch.device('cpu')\n",
    "                print(\"Falling back to CPU\")\n",
    "        else:\n",
    "            for i in range(n_gpus):\n",
    "                print(f\"GPU #{i}: {torch.cuda.get_device_name(i)}\")\n",
    "                props = torch.cuda.get_device_properties(i)\n",
    "                print(f\"  Memory: {props.total_memory / 1e9:.2f} GB\")\n",
    "                print(f\"  Compute Capability: {props.major}.{props.minor}\")\n",
    "    else:\n",
    "        device = torch.device('cpu')\n",
    "        print(\"\\nCUDA not available, using CPU\")\n",
    "        \n",
    "        # Check if we're on a compute node that should have GPUs\n",
    "        if 'SLURM_JOB_GPUS' in os.environ or 'SLURM_GPUS' in os.environ:\n",
    "            print(\"Warning: GPUs were allocated in SLURM but PyTorch cannot detect them\")\n",
    "            print(\"This might be due to a configuration issue.\")\n",
    "            print(\"Try running 'module load cuda' before starting your script\")\n",
    "    \n",
    "    print(\"\\n--- End of Environment Detection ---\\n\")\n",
    "    \n",
    "    # Initialize VAE imputer\n",
    "    imputer = VAEImputer(\n",
    "        hidden_dims=hidden_dims,\n",
    "        latent_dim=latent_dim,\n",
    "        batch_size=batch_size,\n",
    "        learning_rate=learning_rate,\n",
    "        epochs=epochs,\n",
    "        device=device,\n",
    "        verbose=True\n",
    "    )\n",
    "    \n",
    "    # Fit and transform\n",
    "    print(\"Training VAE imputation model...\")\n",
    "    X_imputed = imputer.fit_transform(X, columns_to_impute)\n",
    "    \n",
    "    # Create imputed dataframe\n",
    "    imputed_df = df.copy()\n",
    "    imputed_df[columns_to_impute] = X_imputed[columns_to_impute]\n",
    "    \n",
    "    # Validate if validation data provided\n",
    "    validation_results = None\n",
    "    if validation_df is not None and validation_masks is not None and original_values is not None:\n",
    "        validation_results = {}\n",
    "        \n",
    "        # Extract validation data\n",
    "        X_val = validation_df[columns_to_use].copy()\n",
    "        \n",
    "        # Ensure all validation columns are numeric\n",
    "        for col in X_val.columns:\n",
    "            X_val[col] = pd.to_numeric(X_val[col], errors='coerce')\n",
    "            \n",
    "        # Impute validation data\n",
    "        X_val_imputed = imputer.transform(X_val)\n",
    "        \n",
    "        # Compare imputed values to real values\n",
    "        with tqdm(columns_to_impute, desc=\"Validating results\") as pbar:\n",
    "            for col in pbar:\n",
    "                pbar.set_description(f\"Validating {col}\")\n",
    "                # Get indices where values were artificially set to NaN\n",
    "                mask = validation_masks[col] & validation_df[col].isna()\n",
    "                \n",
    "                if mask.sum() == 0:\n",
    "                    validation_results[col] = {\n",
    "                        'error': \"No artificially missing values\"\n",
    "                    }\n",
    "                    continue\n",
    "                    \n",
    "                real_vals = original_values[col][mask]\n",
    "                imputed_vals = X_val_imputed.loc[mask, col]\n",
    "                \n",
    "                # Calculate MAE and RMSE\n",
    "                mae = mean_absolute_error(real_vals, imputed_vals)\n",
    "                rmse = np.sqrt(mean_squared_error(real_vals, imputed_vals))\n",
    "                \n",
    "                validation_results[col] = {\n",
    "                    'mae': mae,\n",
    "                    'rmse': rmse,\n",
    "                    'real_distribution': real_vals.describe(),\n",
    "                    'imputed_distribution': pd.Series(imputed_vals).describe()\n",
    "                }\n",
    "                \n",
    "                # Update progress\n",
    "                pbar.set_postfix({\"MAE\": f\"{mae:.4f}\", \"RMSE\": f\"{rmse:.4f}\"})\n",
    "    \n",
    "    return imputed_df, validation_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== CUDA Diagnostics ===\n",
      "PyTorch version: 2.6.0\n",
      "CUDA version: 12.2\n",
      "\n",
      "CUDA availability check:\n",
      "torch.cuda.is_available(): False\n",
      "Could not run nvidia-smi\n",
      "\n",
      "CUDA Environment Variables:\n",
      "CUDA_VISIBLE_DEVICES: Not set\n",
      "CUDA_DEVICE_ORDER: Not set\n",
      "CUDA_HOME: Not set\n",
      "LD_LIBRARY_PATH: Not set\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "No GPUs detected by PyTorch.\n",
      "If you see GPUs in nvidia-smi but not here, check CUDA version compatibility.\n",
      "\n",
      "=== End of Diagnostics ===\n",
      "\n",
      "\n",
      "--- Compute Canada / Cluster Environment Detection ---\n",
      "SLURM_JOB_ID: Not found\n",
      "SLURM_JOB_GPUS: Not found\n",
      "SLURM_GPUS: Not found\n",
      "CUDA_VISIBLE_DEVICES: Not found\n",
      "SLURM_JOB_PARTITION: Not found\n",
      "\n",
      "CUDA not available, using CPU\n",
      "\n",
      "--- End of Environment Detection ---\n",
      "\n",
      "Training VAE imputation model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training VAE:   0%|          | 0/100 [00:08<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[51], line 73\u001b[0m\n\u001b[1;32m     70\u001b[0m columns_to_impute \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mge1\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mge2\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mge3\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mge4\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mge5\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mge6\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     72\u001b[0m \u001b[38;5;66;03m# Apply VAE imputation\u001b[39;00m\n\u001b[0;32m---> 73\u001b[0m imputed_df_vae, validation_results_vae  \u001b[38;5;241m=\u001b[39m \u001b[43mapply_vae_imputation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns_to_impute\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[50], line 638\u001b[0m, in \u001b[0;36mapply_vae_imputation\u001b[0;34m(df, columns_to_impute, validation_df, validation_masks, original_values, hidden_dims, latent_dim, batch_size, epochs, learning_rate)\u001b[0m\n\u001b[1;32m    636\u001b[0m \u001b[38;5;66;03m# Fit and transform\u001b[39;00m\n\u001b[1;32m    637\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining VAE imputation model...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 638\u001b[0m X_imputed \u001b[38;5;241m=\u001b[39m \u001b[43mimputer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns_to_impute\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    640\u001b[0m \u001b[38;5;66;03m# Create imputed dataframe\u001b[39;00m\n\u001b[1;32m    641\u001b[0m imputed_df \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mcopy()\n",
      "Cell \u001b[0;32mIn[50], line 516\u001b[0m, in \u001b[0;36mVAEImputer.fit_transform\u001b[0;34m(self, X, columns_to_impute)\u001b[0m\n\u001b[1;32m    500\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mfit_transform\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, columns_to_impute\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    501\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    502\u001b[0m \u001b[38;5;124;03m    Fit model and impute missing values\u001b[39;00m\n\u001b[1;32m    503\u001b[0m \u001b[38;5;124;03m    \u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    514\u001b[0m \u001b[38;5;124;03m        Data with imputed values\u001b[39;00m\n\u001b[1;32m    515\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 516\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns_to_impute\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    517\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform(X)\n",
      "Cell \u001b[0;32mIn[50], line 420\u001b[0m, in \u001b[0;36mVAEImputer.fit\u001b[0;34m(self, X, columns_to_impute)\u001b[0m\n\u001b[1;32m    418\u001b[0m \u001b[38;5;66;03m# Backward pass and optimize\u001b[39;00m\n\u001b[1;32m    419\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m--> 420\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    421\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m    423\u001b[0m \u001b[38;5;66;03m# Accumulate losses\u001b[39;00m\n",
      "File \u001b[0;32m~/jinenv/lib/python3.11/site-packages/torch/_tensor.py:626\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    616\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    617\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    618\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    619\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    624\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    625\u001b[0m     )\n\u001b[0;32m--> 626\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    627\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    628\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/jinenv/lib/python3.11/site-packages/torch/autograd/__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/jinenv/lib/python3.11/site-packages/torch/autograd/graph.py:823\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    821\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    822\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 823\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    824\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    825\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    826\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    827\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # More detailed CUDA diagnostics\n",
    "    print(\"\\n=== CUDA Diagnostics ===\")\n",
    "    print(f\"PyTorch version: {torch.__version__}\")\n",
    "    print(f\"CUDA version: {torch.version.cuda}\")\n",
    "    \n",
    "    # Check CUDA availability\n",
    "    print(f\"\\nCUDA availability check:\")\n",
    "    print(f\"torch.cuda.is_available(): {torch.cuda.is_available()}\")\n",
    "    \n",
    "    # Show NVIDIA driver info if available\n",
    "    try:\n",
    "        import subprocess\n",
    "        nvidia_info = subprocess.check_output(\"nvidia-smi\", shell=True).decode()\n",
    "        print(\"\\nNVIDIA-SMI output:\")\n",
    "        print(nvidia_info)\n",
    "    except:\n",
    "        print(\"Could not run nvidia-smi\")\n",
    "    \n",
    "    # CUDA environment variables\n",
    "    print(\"\\nCUDA Environment Variables:\")\n",
    "    import os\n",
    "    cuda_vars = [\n",
    "        \"CUDA_VISIBLE_DEVICES\",\n",
    "        \"CUDA_DEVICE_ORDER\",\n",
    "        \"CUDA_HOME\",\n",
    "        \"LD_LIBRARY_PATH\"\n",
    "    ]\n",
    "    for var in cuda_vars:\n",
    "        print(f\"{var}: {os.environ.get(var, 'Not set')}\")\n",
    "    \n",
    "    # Try to force CUDA if available\n",
    "    if torch.cuda.device_count() > 0:\n",
    "        print(\"\\nGPUs detected by PyTorch:\")\n",
    "        for i in range(torch.cuda.device_count()):\n",
    "            print(f\"  GPU #{i}: {torch.cuda.get_device_name(i)}\")\n",
    "            \n",
    "        # Try to force CUDA initialization\n",
    "        try:\n",
    "            print(\"\\nAttempting to force CUDA initialization...\")\n",
    "            x = torch.zeros(1).cuda()\n",
    "            print(f\"  Success! Test tensor on: {x.device}\")\n",
    "        except Exception as e:\n",
    "            print(f\"  Failed to initialize CUDA: {e}\")\n",
    "            \n",
    "            # Suggest potential fixes\n",
    "            print(\"\\nPotential fixes:\")\n",
    "            print(\"1. Make sure your NVIDIA drivers match the CUDA version PyTorch was built with\")\n",
    "            print(\"2. Check if you have enough GPU memory available\")\n",
    "            print(\"3. Try setting: os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\")\n",
    "            print(\"4. Reinstall PyTorch with the correct CUDA version:\")\n",
    "            print(\"   pip install torch==2.0.0+cu118 -f https://download.pytorch.org/whl/cu118/torch_stable.html\")\n",
    "            \n",
    "            # Try enabling TF32 or reduced precision\n",
    "            try:\n",
    "                print(\"\\nTrying to enable TF32...\")\n",
    "                torch.backends.cuda.matmul.allow_tf32 = True\n",
    "                torch.backends.cudnn.allow_tf32 = True\n",
    "                x = torch.zeros(1).cuda()\n",
    "                print(f\"  Success with TF32! Test tensor on: {x.device}\")\n",
    "            except Exception as e:\n",
    "                print(f\"  Failed with TF32: {e}\")\n",
    "    else:\n",
    "        print(\"\\nNo GPUs detected by PyTorch.\")\n",
    "        print(\"If you see GPUs in nvidia-smi but not here, check CUDA version compatibility.\")\n",
    "    \n",
    "    print(\"\\n=== End of Diagnostics ===\\n\")\n",
    "    \n",
    "    columns_to_impute = ['ge1', 'ge2', 'ge3', 'ge4', 'ge5', 'ge6']\n",
    "\n",
    "    # Apply VAE imputation\n",
    "    imputed_df_vae, validation_results_vae  = apply_vae_imputation(df, columns_to_impute)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model - Denoising Autoencoder (DAE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SklearnDAE(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Denoising VAE implementation using scikit-learn's MLPRegressor\n",
    "    This avoids TensorFlow execution mode issues completely\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                    hidden_layer_sizes=(64, 32, 16, 32, 64),\n",
    "                    activation='relu',\n",
    "                    max_iter=200,\n",
    "                    learning_rate='adaptive',\n",
    "                    learning_rate_init=0.001,\n",
    "                    noise_factor=0.1,\n",
    "                    alpha=0.0001,\n",
    "                    verbose=True):\n",
    "        \"\"\"\n",
    "        Initialize DAE model parameters\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        hidden_layer_sizes : tuple\n",
    "            Size of hidden layers\n",
    "        activation : str\n",
    "            Activation function ('relu', 'tanh', 'logistic')\n",
    "        max_iter : int\n",
    "            Maximum number of iterations\n",
    "        learning_rate : str\n",
    "            Learning rate schedule\n",
    "        learning_rate_init : float\n",
    "            Initial learning rate\n",
    "        noise_factor : float\n",
    "            Amount of noise to add for denoising effect\n",
    "        alpha : float\n",
    "            L2 regularization parameter\n",
    "        verbose : bool\n",
    "            Whether to print progress\n",
    "        \"\"\"\n",
    "        self.hidden_layer_sizes = hidden_layer_sizes\n",
    "        self.activation = activation\n",
    "        self.max_iter = max_iter\n",
    "        self.learning_rate = learning_rate\n",
    "        self.learning_rate_init = learning_rate_init\n",
    "        self.noise_factor = noise_factor\n",
    "        self.alpha = alpha\n",
    "        self.verbose = verbose\n",
    "        self.model = None\n",
    "        self.scaler_X = StandardScaler()\n",
    "        \n",
    "    def fit(self, X, missing_mask=None):\n",
    "        \"\"\"\n",
    "        Fit the DAE model to input data\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X : numpy.ndarray or pandas.DataFrame\n",
    "            Input data with missing values (NaN)\n",
    "        missing_mask : numpy.ndarray\n",
    "            Binary mask where 1 indicates observed value, 0 for missing\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        self : object\n",
    "            Returns self\n",
    "        \"\"\"\n",
    "        # Convert to numpy array if DataFrame\n",
    "        is_df = isinstance(X, pd.DataFrame)\n",
    "        if is_df:\n",
    "            if self.verbose:\n",
    "                print(\"Converting DataFrame to numpy array\")\n",
    "            X_values = X.values\n",
    "        else:\n",
    "            X_values = X\n",
    "        \n",
    "        # Create missing mask if not provided\n",
    "        if missing_mask is None:\n",
    "            missing_mask = ~np.isnan(X_values)\n",
    "        \n",
    "        # Initial imputation (replace missing values with column means)\n",
    "        X_imputed = np.copy(X_values)\n",
    "        col_means = np.nanmean(X_values, axis=0)\n",
    "        for col in range(X_values.shape[1]):\n",
    "            mask = np.isnan(X_values[:, col])\n",
    "            X_imputed[mask, col] = col_means[col]\n",
    "        \n",
    "        # Scale the data\n",
    "        X_scaled = self.scaler_X.fit_transform(X_imputed)\n",
    "        \n",
    "        # Add noise to create denoising effect\n",
    "        X_noisy = X_scaled + np.random.normal(0, self.noise_factor, X_scaled.shape)\n",
    "        \n",
    "        # Apply mask to noise (only add noise to observed values)\n",
    "        X_noisy = X_noisy * missing_mask + X_scaled * (~missing_mask)\n",
    "        \n",
    "        # Create the model\n",
    "        self.model = MLPRegressor(\n",
    "            hidden_layer_sizes=self.hidden_layer_sizes,\n",
    "            activation=self.activation,\n",
    "            solver='adam',\n",
    "            alpha=self.alpha,\n",
    "            batch_size='auto',\n",
    "            learning_rate=self.learning_rate,\n",
    "            learning_rate_init=self.learning_rate_init,\n",
    "            max_iter=self.max_iter,\n",
    "            shuffle=True,\n",
    "            random_state=42,\n",
    "            verbose=self.verbose\n",
    "        )\n",
    "        \n",
    "        if self.verbose:\n",
    "            print(\"Fitting DAE model...\")\n",
    "        \n",
    "        # Train the model to reconstruct the original data\n",
    "        self.model.fit(X_noisy, X_scaled)\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, missing_mask=None):\n",
    "        \"\"\"\n",
    "        Impute missing values\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X : numpy.ndarray or pandas.DataFrame\n",
    "            Data with missing values (NaN)\n",
    "        missing_mask : numpy.ndarray\n",
    "            Binary mask where 1 indicates observed value, 0 for missing\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        X_imputed : numpy.ndarray or pandas.DataFrame\n",
    "            Data with imputed values\n",
    "        \"\"\"\n",
    "        # Handle DataFrame input\n",
    "        is_df = isinstance(X, pd.DataFrame)\n",
    "        if is_df:\n",
    "            columns = X.columns\n",
    "            index = X.index\n",
    "            X_values = X.values\n",
    "        else:\n",
    "            X_values = X\n",
    "        \n",
    "        # Create missing mask if not provided\n",
    "        if missing_mask is None:\n",
    "            missing_mask = ~np.isnan(X_values)\n",
    "        \n",
    "        # Initial imputation for model input\n",
    "        X_imputed = np.copy(X_values)\n",
    "        col_means = np.nanmean(X_values, axis=0)\n",
    "        for col in range(X_values.shape[1]):\n",
    "            mask = np.isnan(X_values[:, col])\n",
    "            X_imputed[mask, col] = col_means[col]\n",
    "        \n",
    "        # Scale data\n",
    "        X_scaled = self.scaler_X.transform(X_imputed)\n",
    "        \n",
    "        # Reconstruct data\n",
    "        X_reconstructed_scaled = self.model.predict(X_scaled)\n",
    "        \n",
    "        # Unscale data\n",
    "        X_reconstructed = self.scaler_X.inverse_transform(X_reconstructed_scaled)\n",
    "        \n",
    "        # Only replace missing values with reconstructed values\n",
    "        X_final = np.copy(X_values)\n",
    "        mask_missing = ~missing_mask\n",
    "        X_final[mask_missing] = X_reconstructed[mask_missing]\n",
    "        \n",
    "        # Return DataFrame if input was DataFrame\n",
    "        if is_df:\n",
    "            return pd.DataFrame(X_final, index=index, columns=columns)\n",
    "        \n",
    "        return X_final\n",
    "    \n",
    "    def fit_transform(self, X, missing_mask=None):\n",
    "        \"\"\"\n",
    "        Fit the model and impute missing values\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X : numpy.ndarray or pandas.DataFrame\n",
    "            Data with missing values (NaN)\n",
    "        missing_mask : numpy.ndarray\n",
    "            Binary mask where 1 indicates observed value, 0 for missing\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        X_imputed : numpy.ndarray or pandas.DataFrame\n",
    "            Data with imputed values\n",
    "        \"\"\"\n",
    "        return self.fit(X, missing_mask).transform(X, missing_mask)\n",
    "\n",
    "\n",
    "def apply_sklearn_dae_imputation(df, columns_to_impute, validation_df=None, validation_masks=None, original_values=None):\n",
    "    \"\"\"\n",
    "    Apply scikit-learn based DAE imputation to data\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pandas.DataFrame\n",
    "        Data with missing values\n",
    "    columns_to_impute : list\n",
    "        List of column names to impute\n",
    "    validation_df : pandas.DataFrame, optional\n",
    "        Validation data with artificially missing values\n",
    "    validation_masks : dict, optional\n",
    "        Dictionary of masks for validation\n",
    "    original_values : dict, optional\n",
    "        Dictionary of original values for validation\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    imputed_df : pandas.DataFrame\n",
    "        Data with imputed values\n",
    "    validation_results : dict, optional\n",
    "        Validation results if validation data provided\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Extract data to impute\n",
    "        X = df[columns_to_impute].copy()\n",
    "        \n",
    "        # Ensure all columns are numeric\n",
    "        for col in columns_to_impute:\n",
    "            X[col] = pd.to_numeric(X[col], errors='coerce')\n",
    "        \n",
    "        # Create missing mask\n",
    "        missing_mask = ~X.isna()\n",
    "        \n",
    "        print(f\"Data shape: {X.shape}\")\n",
    "        print(f\"Missing values: {X.isna().sum().sum()}\")\n",
    "        \n",
    "        # Initialize DAE model with adaptive parameters based on data size\n",
    "        neurons_per_layer = min(128, max(16, X.shape[0] // 100))\n",
    "        print(f\"Using {neurons_per_layer} neurons per layer\")\n",
    "        \n",
    "        dae = SklearnDAE(\n",
    "            hidden_layer_sizes=(neurons_per_layer, neurons_per_layer//2, neurons_per_layer//4, neurons_per_layer//2, neurons_per_layer),\n",
    "            activation='relu',\n",
    "            max_iter=200,\n",
    "            learning_rate='adaptive',\n",
    "            learning_rate_init=0.001,\n",
    "            noise_factor=0.1,\n",
    "            alpha=0.0001,\n",
    "            verbose=True\n",
    "        )\n",
    "        \n",
    "        # Fit and transform\n",
    "        X_imputed = dae.fit_transform(X, missing_mask.values)\n",
    "        \n",
    "        # Create imputed dataframe\n",
    "        imputed_df = df.copy()\n",
    "        imputed_df[columns_to_impute] = X_imputed\n",
    "        \n",
    "        # Validate if required\n",
    "        validation_results = None\n",
    "        if validation_df is not None and validation_masks is not None and original_values is not None:\n",
    "            validation_results = {}\n",
    "            \n",
    "            # Extract validation data\n",
    "            X_val = validation_df[columns_to_impute].copy()\n",
    "            \n",
    "            # Ensure numeric\n",
    "            for col in columns_to_impute:\n",
    "                X_val[col] = pd.to_numeric(X_val[col], errors='coerce')\n",
    "                \n",
    "            # Create validation mask\n",
    "            val_mask = ~X_val.isna()\n",
    "            \n",
    "            # Impute validation data\n",
    "            X_val_imputed = dae.transform(X_val, val_mask.values)\n",
    "            \n",
    "            # Compare imputed values to real values\n",
    "            for col in columns_to_impute:\n",
    "                mask = validation_masks[col] & validation_df[col].isna()\n",
    "                \n",
    "                if mask.sum() == 0:\n",
    "                    validation_results[col] = {'error': \"No artificially missing values\"}\n",
    "                    continue\n",
    "                    \n",
    "                real_vals = original_values[col][mask]\n",
    "                imputed_vals = X_val_imputed[col][mask]\n",
    "                \n",
    "                mae = mean_absolute_error(real_vals, imputed_vals)\n",
    "                rmse = np.sqrt(mean_squared_error(real_vals, imputed_vals))\n",
    "                \n",
    "                validation_results[col] = {\n",
    "                    'mae': mae,\n",
    "                    'rmse': rmse,\n",
    "                    'real_distribution': real_vals.describe(),\n",
    "                    'imputed_distribution': imputed_vals.describe()\n",
    "                }\n",
    "        \n",
    "        return imputed_df, validation_results\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error in scikit-learn DAE imputation: {e}\")\n",
    "        # Fallback to simple imputation\n",
    "        result_df = df.copy()\n",
    "        for col in columns_to_impute:\n",
    "            result_df[col] = pd.to_numeric(result_df[col], errors='coerce')\n",
    "            result_df[col] = result_df[col].fillna(result_df[col].mean())\n",
    "        return result_df, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape: (5230, 6)\n",
      "Missing values: 25215\n",
      "Using 52 neurons per layer\n",
      "Converting DataFrame to numpy array\n",
      "Fitting DAE model...\n",
      "Iteration 1, loss = 0.47601284\n",
      "Iteration 2, loss = 0.29175511\n",
      "Iteration 3, loss = 0.18024715\n",
      "Iteration 4, loss = 0.11930503\n",
      "Iteration 5, loss = 0.07219841\n",
      "Iteration 6, loss = 0.04385158\n",
      "Iteration 7, loss = 0.02998478\n",
      "Iteration 8, loss = 0.01816212\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 9, loss = 0.01060953\n",
      "Iteration 10, loss = 0.00804210\n",
      "Iteration 11, loss = 0.00674570\n",
      "Iteration 12, loss = 0.00594408\n",
      "Iteration 13, loss = 0.00511390\n",
      "Iteration 14, loss = 0.00467576\n",
      "Iteration 15, loss = 0.00453567\n",
      "Iteration 16, loss = 0.00410161\n",
      "Iteration 17, loss = 0.00387669\n",
      "Iteration 18, loss = 0.00411478\n",
      "Iteration 19, loss = 0.00399567\n",
      "Iteration 20, loss = 0.00351376\n",
      "Iteration 21, loss = 0.00323945\n",
      "Iteration 22, loss = 0.00351890\n",
      "Iteration 23, loss = 0.00324152\n",
      "Iteration 24, loss = 0.00305600\n",
      "Iteration 25, loss = 0.00295288\n",
      "Iteration 26, loss = 0.00283412\n",
      "Iteration 27, loss = 0.00285598\n",
      "Iteration 28, loss = 0.00270962\n",
      "Iteration 29, loss = 0.00267738\n",
      "Iteration 30, loss = 0.00272910\n",
      "Iteration 31, loss = 0.00257689\n",
      "Iteration 32, loss = 0.00247942\n",
      "Iteration 33, loss = 0.00248776\n",
      "Iteration 34, loss = 0.00257514\n",
      "Iteration 35, loss = 0.00258318\n",
      "Iteration 36, loss = 0.00243032\n",
      "Iteration 37, loss = 0.00224727\n",
      "Iteration 38, loss = 0.00256814\n",
      "Iteration 39, loss = 0.00255014\n",
      "Iteration 40, loss = 0.00233078\n",
      "Iteration 41, loss = 0.00229485\n",
      "Iteration 42, loss = 0.00214793\n",
      "Iteration 43, loss = 0.00223688\n",
      "Iteration 44, loss = 0.00204215\n",
      "Iteration 45, loss = 0.00197279\n",
      "Iteration 46, loss = 0.00205419\n",
      "Iteration 47, loss = 0.00201385\n",
      "Iteration 48, loss = 0.00198907\n",
      "Iteration 49, loss = 0.00202261\n",
      "Iteration 50, loss = 0.00197503\n",
      "Iteration 51, loss = 0.00180916\n",
      "Iteration 52, loss = 0.00183605\n",
      "Iteration 53, loss = 0.00193084\n",
      "Iteration 54, loss = 0.00191796\n",
      "Iteration 55, loss = 0.00186554\n",
      "Iteration 56, loss = 0.00175530\n",
      "Iteration 57, loss = 0.00192119\n",
      "Iteration 58, loss = 0.00192222\n",
      "Iteration 59, loss = 0.00197559\n",
      "Iteration 60, loss = 0.00193852\n",
      "Iteration 61, loss = 0.00180791\n",
      "Iteration 62, loss = 0.00189187\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    }
   ],
   "source": [
    "# Define columns to impute\n",
    "columns_to_impute = ['ge1', 'ge2', 'ge3', 'ge4', 'ge5', 'ge6']\n",
    "\n",
    "# Apply DAE imputation\n",
    "imputed_df_dae, validation_results_dae = apply_sklearn_dae_imputation(df, columns_to_impute)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        1.311341\n",
       "14       1.311341\n",
       "18       1.311341\n",
       "22       1.311341\n",
       "25       1.000000\n",
       "           ...   \n",
       "18178    1.311341\n",
       "18179    1.000000\n",
       "18180    1.311341\n",
       "18181    1.311341\n",
       "18185    1.311341\n",
       "Name: ge1, Length: 5230, dtype: float64"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imputed_df_dae['ge1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model - Bayesian PCA "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.decomposition import PCA\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.distributions as dist\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import time\n",
    "\n",
    "\n",
    "class BayesianPCATorch(nn.Module):\n",
    "    \"\"\"\n",
    "    PyTorch implementation of Bayesian PCA model\n",
    "    \"\"\"\n",
    "    def __init__(self, n_features, n_components):\n",
    "        super(BayesianPCATorch, self).__init__()\n",
    "        self.n_features = n_features\n",
    "        self.n_components = n_components\n",
    "        \n",
    "        # Priors for loadings (W)\n",
    "        self.w_mu = nn.Parameter(torch.zeros(n_features, n_components), requires_grad=True)\n",
    "        self.w_log_sigma = nn.Parameter(torch.zeros(n_features, n_components), requires_grad=True)\n",
    "        \n",
    "        # Noise precision (inverse variance)\n",
    "        self.log_tau = nn.Parameter(torch.zeros(1), requires_grad=True)\n",
    "        \n",
    "    def forward(self, z):\n",
    "        # Sample from approximate posterior for W\n",
    "        w_sigma = torch.exp(self.w_log_sigma)\n",
    "        epsilon_w = torch.randn_like(self.w_mu)\n",
    "        w = self.w_mu + w_sigma * epsilon_w\n",
    "        \n",
    "        # Compute reconstruction\n",
    "        tau = torch.exp(self.log_tau)\n",
    "        reconstruction = torch.matmul(z, w.t())\n",
    "        \n",
    "        return reconstruction, w, tau\n",
    "    \n",
    "    def sample_loadings(self, n_samples=1):\n",
    "        \"\"\"Sample loadings (W) from the approximate posterior\"\"\"\n",
    "        w_sigma = torch.exp(self.w_log_sigma)\n",
    "        epsilon_w = torch.randn(n_samples, self.n_features, self.n_components, device=self.w_mu.device)\n",
    "        w_samples = self.w_mu.unsqueeze(0) + w_sigma.unsqueeze(0) * epsilon_w\n",
    "        return w_samples\n",
    "\n",
    "\n",
    "class BayesianPCAImputer(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    A scikit-learn compatible Bayesian PCA imputation model\n",
    "    using PyTorch for GPU acceleration\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self,\n",
    "                 n_components=5,\n",
    "                 n_samples=1000,\n",
    "                 batch_size=64,\n",
    "                 n_epochs=100,\n",
    "                 learning_rate=0.01,\n",
    "                 device=None,\n",
    "                 verbose=True):\n",
    "        \"\"\"\n",
    "        Initialize PyTorch-based Bayesian PCA imputer\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        n_components : int\n",
    "            Number of principal components\n",
    "        n_samples : int\n",
    "            Number of posterior samples to draw\n",
    "        batch_size : int\n",
    "            Batch size for training\n",
    "        n_epochs : int\n",
    "            Number of training epochs\n",
    "        learning_rate : float\n",
    "            Learning rate for optimizer\n",
    "        device : str or torch.device\n",
    "            Device to use ('cuda' or 'cpu'), defaults to CUDA if available\n",
    "        verbose : bool\n",
    "            Whether to print progress\n",
    "        \"\"\"\n",
    "        self.n_components = n_components\n",
    "        self.n_samples = n_samples\n",
    "        self.batch_size = batch_size\n",
    "        self.n_epochs = n_epochs\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "        # Set device\n",
    "        if device is None:\n",
    "            self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        else:\n",
    "            self.device = torch.device(device)\n",
    "            \n",
    "        self.verbose = verbose\n",
    "        \n",
    "        # Initialize model components\n",
    "        self.pca_model = None  # For standard PCA initialization\n",
    "        self.scaler = None     # For data scaling\n",
    "        self.model = None      # The PyTorch model\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        \"\"\"\n",
    "        Fit Bayesian PCA model using PyTorch\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X : pandas.DataFrame\n",
    "            Data with missing values\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        self : object\n",
    "            Returns self\n",
    "        \"\"\"\n",
    "        # Store column names and index\n",
    "        self.columns = X.columns\n",
    "        self.index = X.index\n",
    "        \n",
    "        # Create a copy of data\n",
    "        X_data = X.copy()\n",
    "        \n",
    "        # Convert to numpy array\n",
    "        X_array = X_data.values\n",
    "        \n",
    "        # Initialize scaler\n",
    "        self.scaler = StandardScaler()\n",
    "        \n",
    "        # Simple imputation for initial values (for fitting the scaler)\n",
    "        simple_imputer = SimpleImputer(strategy='mean')\n",
    "        X_imputed_for_scaling = simple_imputer.fit_transform(X_array)\n",
    "        \n",
    "        # Scale data\n",
    "        X_scaled = self.scaler.fit_transform(X_imputed_for_scaling)\n",
    "        \n",
    "        # Create a mask for missing values\n",
    "        self.missing_mask = np.isnan(X_array)\n",
    "        \n",
    "        # Perform standard PCA for initialization\n",
    "        pca = PCA(n_components=self.n_components)\n",
    "        pca.fit(X_scaled)\n",
    "        self.pca_model = pca\n",
    "        \n",
    "        # Initialize PyTorch model\n",
    "        n_samples, n_features = X_scaled.shape\n",
    "        self.model = BayesianPCATorch(n_features, self.n_components).to(self.device)\n",
    "        \n",
    "        # Initialize model parameters using standard PCA\n",
    "        with torch.no_grad():\n",
    "            # Initialize W to PCA loadings\n",
    "            self.model.w_mu.data = torch.tensor(pca.components_.T, dtype=torch.float32).to(self.device)\n",
    "            \n",
    "            # Initialize noise precision (tau) based on explained variance\n",
    "            explained_var = pca.explained_variance_\n",
    "            noise_var = np.mean(np.var(X_scaled, axis=0) - np.sum(explained_var))\n",
    "            noise_var = max(noise_var, 1e-6)  # Ensure positive variance\n",
    "            self.model.log_tau.data = torch.tensor([np.log(1.0 / noise_var)], dtype=torch.float32).to(self.device)\n",
    "        \n",
    "        # Prepare data for PyTorch training\n",
    "        # Fill missing values with zeros (will be handled by the mask)\n",
    "        X_for_torch = X_scaled.copy()\n",
    "        X_for_torch[self.missing_mask] = 0.0\n",
    "        \n",
    "        # Convert to PyTorch tensors\n",
    "        X_tensor = torch.tensor(X_for_torch, dtype=torch.float32).to(self.device)\n",
    "        mask_tensor = torch.tensor(~self.missing_mask, dtype=torch.float32).to(self.device)\n",
    "        \n",
    "        # Initialize latent variables (z) using PCA scores\n",
    "        z_init = pca.transform(X_scaled)\n",
    "        \n",
    "        # Store the latent variables as a model parameter\n",
    "        self.latent_z = nn.Parameter(torch.tensor(z_init, dtype=torch.float32, device=self.device))\n",
    "        \n",
    "        # Train the model\n",
    "        self._train_model(X_tensor, mask_tensor)\n",
    "        \n",
    "        # Store final latent variables for later use\n",
    "        self.z = self.latent_z.detach().cpu().numpy()\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def _train_model(self, X, mask):\n",
    "        \"\"\"\n",
    "        Train the Bayesian PCA model using stochastic variational inference\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X : torch.Tensor\n",
    "            Data tensor\n",
    "        mask : torch.Tensor\n",
    "            Mask tensor (1 for observed, 0 for missing)\n",
    "        \"\"\"\n",
    "        # Setup optimizer - include latent_z as a parameter of self\n",
    "        parameters = list(self.model.parameters()) + [self.latent_z]\n",
    "        optimizer = optim.Adam(parameters, lr=self.learning_rate)\n",
    "        \n",
    "        # Setup scheduler for learning rate decay\n",
    "        scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "            optimizer, mode='min', factor=0.5, patience=5, verbose=self.verbose\n",
    "        )\n",
    "        \n",
    "        # Training loop\n",
    "        if self.verbose:\n",
    "            print(f\"Training Bayesian PCA model on {self.device}...\")\n",
    "            print(f\"Data shape: {X.shape}, Components: {self.n_components}\")\n",
    "            print(f\"Missing values: {torch.sum(mask == 0).item()} out of {X.numel()}\")\n",
    "            \n",
    "        epoch_pbar = range(self.n_epochs)\n",
    "        if self.verbose:\n",
    "            epoch_pbar = tqdm(epoch_pbar, desc=\"Training\")\n",
    "            \n",
    "        # For early stopping\n",
    "        best_loss = float('inf')\n",
    "        patience = 40  # Increased patience to 40\n",
    "        patience_counter = 0\n",
    "        \n",
    "        # Store z for later use\n",
    "        for epoch in epoch_pbar:\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass\n",
    "            x_recon, w_samples, tau = self.model(self.latent_z)\n",
    "            \n",
    "            # Compute loss - only for observed values\n",
    "            # Likelihood term (reconstruction error)\n",
    "            mse_loss = torch.sum(mask * (X - x_recon) ** 2)\n",
    "            \n",
    "            # Prior on z (standard normal)\n",
    "            z_prior_loss = 0.5 * torch.sum(self.latent_z ** 2)\n",
    "            \n",
    "            # Prior on W (standard normal)\n",
    "            w_mu = self.model.w_mu\n",
    "            w_sigma = torch.exp(self.model.w_log_sigma)\n",
    "            w_prior_loss = 0.5 * torch.sum(w_mu ** 2 + w_sigma ** 2 - torch.log(w_sigma ** 2) - 1)\n",
    "            \n",
    "            # Total loss\n",
    "            loss = mse_loss * tau + z_prior_loss + w_prior_loss\n",
    "            \n",
    "            # Backward pass and optimization step\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Get current loss\n",
    "            current_loss = loss.item()\n",
    "            \n",
    "            # Update learning rate\n",
    "            scheduler.step(current_loss)\n",
    "            \n",
    "            # Update progress bar with current loss\n",
    "            if self.verbose:\n",
    "                epoch_pbar.set_postfix({\"Loss\": f\"{current_loss:.4f}\"})\n",
    "            \n",
    "            # Early stopping\n",
    "            if current_loss < best_loss:\n",
    "                best_loss = current_loss\n",
    "                patience_counter = 0\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "                \n",
    "            if patience_counter >= patience:\n",
    "                if self.verbose:\n",
    "                    print(f\"Early stopping at epoch {epoch + 1}\")\n",
    "                break\n",
    "                \n",
    "    def transform(self, X):\n",
    "        \"\"\"\n",
    "        Impute missing values using Bayesian PCA\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X : pandas.DataFrame\n",
    "            Data with missing values\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        imputed_df : pandas.DataFrame\n",
    "            Data with imputed values\n",
    "        \"\"\"\n",
    "        # Create copy to avoid modifying original\n",
    "        imputed_df = X.copy()\n",
    "        \n",
    "        # Check if this is the same data used for fitting\n",
    "        new_data = not np.array_equal(X.index, self.index) or not np.array_equal(X.columns, self.columns)\n",
    "        \n",
    "        if new_data:\n",
    "            # For new data, we need to project onto the learned components\n",
    "            # This is a simplified approach and could be improved\n",
    "            \n",
    "            # Create a copy of data\n",
    "            X_data = X.copy()\n",
    "            \n",
    "            # Convert to numpy array\n",
    "            X_array = X_data.values\n",
    "            \n",
    "            # Simple imputation for initial values\n",
    "            simple_imputer = SimpleImputer(strategy='mean')\n",
    "            X_imputed = simple_imputer.fit_transform(X_array)\n",
    "            \n",
    "            # Scale data\n",
    "            X_scaled = self.scaler.transform(X_imputed)\n",
    "            \n",
    "            # Create a mask for missing values\n",
    "            missing_mask = np.isnan(X_array)\n",
    "            \n",
    "            # Project data onto principal components\n",
    "            z = self.pca_model.transform(X_scaled)\n",
    "            \n",
    "            # Get mean of W from model\n",
    "            w_samples = self.model.sample_loadings(n_samples=self.n_samples)\n",
    "            w_mean = w_samples.mean(dim=0).cpu().detach().numpy()\n",
    "            \n",
    "            # Reconstruct data\n",
    "            X_reconstructed = np.dot(z, w_mean.T)\n",
    "            \n",
    "            # Inverse transform to original scale\n",
    "            X_reconstructed = self.scaler.inverse_transform(X_reconstructed)\n",
    "            \n",
    "            # Only replace missing values\n",
    "            X_array_imputed = X_array.copy()\n",
    "            X_array_imputed[missing_mask] = X_reconstructed[missing_mask]\n",
    "            \n",
    "            # Convert back to DataFrame\n",
    "            imputed_df = pd.DataFrame(X_array_imputed, \n",
    "                                      columns=X.columns, \n",
    "                                      index=X.index)\n",
    "        else:\n",
    "            # For the same data used in fitting, use posterior samples\n",
    "            \n",
    "            # Sample from the model multiple times to get uncertainty estimates\n",
    "            w_samples = self.model.sample_loadings(n_samples=self.n_samples)  # [n_samples, n_features, n_components]\n",
    "            w_mean = w_samples.mean(dim=0).cpu().detach().numpy()  # [n_features, n_components]\n",
    "            \n",
    "            # Reconstruct data from latent variables (z)\n",
    "            X_reconstructed = np.dot(self.z, w_mean.T)  # [n_samples, n_features]\n",
    "            \n",
    "            # Inverse transform to original scale\n",
    "            X_reconstructed = self.scaler.inverse_transform(X_reconstructed)\n",
    "            \n",
    "            # Convert to DataFrame\n",
    "            X_reconstructed_df = pd.DataFrame(X_reconstructed, \n",
    "                                             columns=self.columns, \n",
    "                                             index=self.index)\n",
    "            \n",
    "            # Only replace missing values\n",
    "            for col in imputed_df.columns:\n",
    "                missing_idx = imputed_df[col].isna()\n",
    "                if missing_idx.any():\n",
    "                    imputed_df.loc[missing_idx, col] = X_reconstructed_df.loc[missing_idx, col].values\n",
    "        \n",
    "        return imputed_df\n",
    "    \n",
    "    def fit_transform(self, X, y=None):\n",
    "        \"\"\"\n",
    "        Fit and transform\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X : pandas.DataFrame\n",
    "            Data with missing values\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        imputed_df : pandas.DataFrame\n",
    "            Data with imputed values\n",
    "        \"\"\"\n",
    "        self.fit(X)\n",
    "        return self.transform(X)\n",
    "\n",
    "\n",
    "def apply_bpca_imputation(df, columns_to_impute, validation_df=None, validation_masks=None, original_values=None, display_progress=True, use_gpu=True):\n",
    "    \"\"\"\n",
    "    Apply Bayesian PCA imputation to patient data using PyTorch\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pandas.DataFrame\n",
    "        Patient data with missing values\n",
    "    columns_to_impute : list\n",
    "        List of column names to impute\n",
    "    validation_df : pandas.DataFrame, optional\n",
    "        Validation dataset with artificially missing values\n",
    "    validation_masks : dict, optional\n",
    "        Dictionary of masks for validation data\n",
    "    original_values : dict, optional\n",
    "        Dictionary of original values for validation\n",
    "    display_progress : bool\n",
    "        Whether to display progress\n",
    "    use_gpu : bool\n",
    "        Whether to use GPU acceleration\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    imputed_df : pandas.DataFrame\n",
    "        Data with imputed values\n",
    "    validation_results : dict, optional\n",
    "        Validation results if validation data provided\n",
    "    \"\"\"\n",
    "    # Extract relevant columns including potential predictors\n",
    "    # Use all columns except those with excessive missing values\n",
    "    threshold = 0.5  # Columns with more than 50% missing values are excluded\n",
    "    columns_to_use = [col for col in df.columns \n",
    "                      if df[col].isna().mean() < threshold]\n",
    "    \n",
    "    # Ensure all columns_to_impute are included\n",
    "    for col in columns_to_impute:\n",
    "        if col not in columns_to_use:\n",
    "            columns_to_use.append(col)\n",
    "    \n",
    "    # Extract subset of data\n",
    "    X = df[columns_to_use].copy()\n",
    "    \n",
    "    # Ensure all columns are numeric\n",
    "    for col in X.columns:\n",
    "        X[col] = pd.to_numeric(X[col], errors='coerce')\n",
    "    \n",
    "    # Initialize Bayesian PCA imputer\n",
    "    n_components = min(5, len(columns_to_use) - 1)  # Ensure n_components is valid\n",
    "    \n",
    "    # Set device based on user preference and availability\n",
    "    device = 'cuda' if use_gpu and torch.cuda.is_available() else 'cpu'\n",
    "    \n",
    "    # Determine batch size based on data size\n",
    "    batch_size = min(64, len(X))  # Default 64, but smaller if dataset is tiny\n",
    "    \n",
    "    # Configure epochs based on data size\n",
    "    n_epochs = max(100, min(300, 10000 // len(X) + 30))  # Updated minimum to 100 epochs\n",
    "    \n",
    "    if display_progress:\n",
    "        print(f\"Starting PyTorch Bayesian PCA imputation with {n_components} components\")\n",
    "        print(f\"Using device: {device}\")\n",
    "        print(f\"Training for up to {n_epochs} epochs with batch size {batch_size}\")\n",
    "        \n",
    "        if device == 'cuda':\n",
    "            print(f\"GPU Info: {torch.cuda.get_device_name(0)}\")\n",
    "            print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "        \n",
    "    # Determine reasonable number of posterior samples based on data size\n",
    "    n_samples = 1000  # Default\n",
    "    \n",
    "    imputer = BayesianPCAImputer(\n",
    "        n_components=n_components,\n",
    "        n_samples=n_samples,\n",
    "        batch_size=batch_size,\n",
    "        n_epochs=n_epochs,\n",
    "        learning_rate=0.01,\n",
    "        device=device,\n",
    "        verbose=display_progress\n",
    "    )\n",
    "    \n",
    "    # Fit and transform with progress tracking\n",
    "    start_time = None\n",
    "    if display_progress:\n",
    "        start_time = time.time()\n",
    "        print(\"Training PyTorch Bayesian PCA imputation model...\")\n",
    "    \n",
    "    X_imputed = imputer.fit_transform(X)\n",
    "    \n",
    "    if display_progress and start_time:\n",
    "        elapsed = time.time() - start_time\n",
    "        print(f\"BPCA imputation completed in {elapsed:.2f} seconds ({elapsed/60:.2f} minutes)\")\n",
    "    \n",
    "    # Create imputed dataframe\n",
    "    imputed_df = df.copy()\n",
    "    imputed_df[columns_to_impute] = X_imputed[columns_to_impute]\n",
    "    \n",
    "    # Validate if validation data provided\n",
    "    validation_results = None\n",
    "    if validation_df is not None and validation_masks is not None and original_values is not None:\n",
    "        validation_results = {}\n",
    "        validation_start_time = None\n",
    "        \n",
    "        if display_progress:\n",
    "            validation_start_time = time.time()\n",
    "            print(\"\\nStarting validation on test data...\")\n",
    "        \n",
    "        # Extract validation data\n",
    "        X_val = validation_df[columns_to_use].copy()\n",
    "        \n",
    "        # Ensure all validation columns are numeric\n",
    "        for col in X_val.columns:\n",
    "            X_val[col] = pd.to_numeric(X_val[col], errors='coerce')\n",
    "            \n",
    "        # Impute validation data\n",
    "        X_val_imputed = imputer.transform(X_val)\n",
    "        \n",
    "        # For storing overall metrics\n",
    "        all_real_vals = []\n",
    "        all_imputed_vals = []\n",
    "        column_metrics = []\n",
    "        \n",
    "        # Compare imputed values to real values\n",
    "        with tqdm(columns_to_impute, desc=\"Validating results\") as pbar:\n",
    "            for col in pbar:\n",
    "                pbar.set_description(f\"Validating {col}\")\n",
    "                # Get indices where values were artificially set to NaN\n",
    "                mask = validation_masks[col] & validation_df[col].isna()\n",
    "                \n",
    "                if mask.sum() == 0:\n",
    "                    validation_results[col] = {\n",
    "                        'error': \"No artificially missing values\"\n",
    "                    }\n",
    "                    continue\n",
    "                    \n",
    "                real_vals = original_values[col][mask]\n",
    "                imputed_vals = X_val_imputed[col][mask]\n",
    "                \n",
    "                # Collect all values for overall metrics\n",
    "                all_real_vals.extend(real_vals.values)\n",
    "                all_imputed_vals.extend(imputed_vals.values)\n",
    "                \n",
    "                # Calculate MAE and RMSE\n",
    "                mae = mean_absolute_error(real_vals, imputed_vals)\n",
    "                rmse = np.sqrt(mean_squared_error(real_vals, imputed_vals))\n",
    "                \n",
    "                # Store metrics for summary\n",
    "                column_metrics.append({\n",
    "                    'column': col,\n",
    "                    'mae': mae,\n",
    "                    'rmse': rmse,\n",
    "                    'count': len(real_vals)\n",
    "                })\n",
    "                \n",
    "                validation_results[col] = {\n",
    "                    'mae': mae,\n",
    "                    'rmse': rmse,\n",
    "                    'real_distribution': real_vals.describe(),\n",
    "                    'imputed_distribution': imputed_vals.describe()\n",
    "                }\n",
    "                \n",
    "                # Update progress\n",
    "                pbar.set_postfix({\"MAE\": f\"{mae:.4f}\", \"RMSE\": f\"{rmse:.4f}\"})\n",
    "        \n",
    "        # Calculate overall metrics\n",
    "        if all_real_vals:\n",
    "            overall_mae = mean_absolute_error(all_real_vals, all_imputed_vals)\n",
    "            overall_rmse = np.sqrt(mean_squared_error(all_real_vals, all_imputed_vals))\n",
    "            \n",
    "            validation_results['overall'] = {\n",
    "                'mae': overall_mae,\n",
    "                'rmse': overall_rmse,\n",
    "                'total_values': len(all_real_vals)\n",
    "            }\n",
    "        \n",
    "        # Print detailed summary\n",
    "        if display_progress:\n",
    "            validation_time = time.time() - validation_start_time if validation_start_time else 0\n",
    "            total_time = validation_time + (time.time() - start_time if start_time else 0)\n",
    "            \n",
    "            print(\"\\n\" + \"=\"*80)\n",
    "            print(f\"PYTORCH BAYESIAN PCA IMPUTATION SUMMARY (Device: {device})\")\n",
    "            print(\"=\"*80)\n",
    "            \n",
    "            # Time information\n",
    "            print(f\"\\nTIMING INFORMATION:\")\n",
    "            print(f\"  Training Time: {elapsed:.2f} seconds ({elapsed/60:.2f} minutes)\")\n",
    "            print(f\"  Validation Time: {validation_time:.2f} seconds ({validation_time/60:.2f} minutes)\")\n",
    "            print(f\"  Total Time: {total_time:.2f} seconds ({total_time/60:.2f} minutes)\")\n",
    "            \n",
    "            # Overall metrics\n",
    "            if 'overall' in validation_results:\n",
    "                print(f\"\\nOVERALL METRICS (across {validation_results['overall']['total_values']} values):\")\n",
    "                print(f\"  MAE: {validation_results['overall']['mae']:.4f}\")\n",
    "                print(f\"  RMSE: {validation_results['overall']['rmse']:.4f}\")\n",
    "            \n",
    "            # Per-column metrics\n",
    "            print(\"\\nPER-COLUMN METRICS:\")\n",
    "            print(\"-\"*80)\n",
    "            print(f\"{'Column':<20} {'MAE':<10} {'RMSE':<10} {'Count':<10}\")\n",
    "            print(\"-\"*80)\n",
    "            \n",
    "            for metric in sorted(column_metrics, key=lambda x: x['mae']):\n",
    "                print(f\"{metric['column']:<20} {metric['mae']:<10.4f} {metric['rmse']:<10.4f} {metric['count']:<10}\")\n",
    "            \n",
    "            print(\"=\"*80)\n",
    "    \n",
    "    return imputed_df, validation_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting PyTorch Bayesian PCA imputation with 5 components\n",
      "Using device: cpu\n",
      "Training for up to 100 epochs with batch size 64\n",
      "Training PyTorch Bayesian PCA imputation model...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Bayesian PCA model on cpu...\n",
      "Data shape: torch.Size([5230, 10]), Components: 5\n",
      "Missing values: 36725 out of 52300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   3%|â–Ž         | 3/100 [00:04<02:25,  1.50s/it, Loss=143057371136.0000]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[55], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m columns_to_impute \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mge1\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mge2\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mge3\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mge4\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mge5\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mge6\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Apply Transformer imputation\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m imputed_df_bpca, validation_results_bpca \u001b[38;5;241m=\u001b[39m \u001b[43mapply_bpca_imputation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns_to_impute\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[54], line 460\u001b[0m, in \u001b[0;36mapply_bpca_imputation\u001b[0;34m(df, columns_to_impute, validation_df, validation_masks, original_values, display_progress, use_gpu)\u001b[0m\n\u001b[1;32m    457\u001b[0m     start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m    458\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining PyTorch Bayesian PCA imputation model...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 460\u001b[0m X_imputed \u001b[38;5;241m=\u001b[39m \u001b[43mimputer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    462\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m display_progress \u001b[38;5;129;01mand\u001b[39;00m start_time:\n\u001b[1;32m    463\u001b[0m     elapsed \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start_time\n",
      "File \u001b[0;32m~/jinenv/lib/python3.11/site-packages/sklearn/utils/_set_output.py:319\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[0;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[1;32m    318\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 319\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    320\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m    321\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[1;32m    322\u001b[0m         return_tuple \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    323\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[1;32m    324\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[1;32m    325\u001b[0m         )\n",
      "Cell \u001b[0;32mIn[54], line 370\u001b[0m, in \u001b[0;36mBayesianPCAImputer.fit_transform\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    356\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mfit_transform\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    357\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    358\u001b[0m \u001b[38;5;124;03m    Fit and transform\u001b[39;00m\n\u001b[1;32m    359\u001b[0m \u001b[38;5;124;03m    \u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    368\u001b[0m \u001b[38;5;124;03m        Data with imputed values\u001b[39;00m\n\u001b[1;32m    369\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 370\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    371\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform(X)\n",
      "Cell \u001b[0;32mIn[54], line 179\u001b[0m, in \u001b[0;36mBayesianPCAImputer.fit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    176\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlatent_z \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mParameter(torch\u001b[38;5;241m.\u001b[39mtensor(z_init, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice))\n\u001b[1;32m    178\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[0;32m--> 179\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_train_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask_tensor\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    181\u001b[0m \u001b[38;5;66;03m# Store final latent variables for later use\u001b[39;00m\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mz \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlatent_z\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\n",
      "Cell \u001b[0;32mIn[54], line 244\u001b[0m, in \u001b[0;36mBayesianPCAImputer._train_model\u001b[0;34m(self, X, mask)\u001b[0m\n\u001b[1;32m    241\u001b[0m loss \u001b[38;5;241m=\u001b[39m mse_loss \u001b[38;5;241m*\u001b[39m tau \u001b[38;5;241m+\u001b[39m z_prior_loss \u001b[38;5;241m+\u001b[39m w_prior_loss\n\u001b[1;32m    243\u001b[0m \u001b[38;5;66;03m# Backward pass and optimization step\u001b[39;00m\n\u001b[0;32m--> 244\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    245\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m    247\u001b[0m \u001b[38;5;66;03m# Get current loss\u001b[39;00m\n",
      "File \u001b[0;32m~/jinenv/lib/python3.11/site-packages/torch/_tensor.py:626\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    616\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    617\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    618\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    619\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    624\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    625\u001b[0m     )\n\u001b[0;32m--> 626\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    627\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    628\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/jinenv/lib/python3.11/site-packages/torch/autograd/__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/jinenv/lib/python3.11/site-packages/torch/autograd/graph.py:823\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    821\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    822\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 823\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    824\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    825\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    826\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    827\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Define columns to impute\n",
    "columns_to_impute = ['ge1', 'ge2', 'ge3', 'ge4', 'ge5', 'ge6']\n",
    "\n",
    "# Apply Transformer imputation\n",
    "imputed_df_bpca, validation_results_bpca = apply_bpca_imputation(df, columns_to_impute)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        1.286870\n",
       "14       1.268549\n",
       "18       1.276519\n",
       "22       1.286916\n",
       "25       1.000000\n",
       "           ...   \n",
       "18178    1.341442\n",
       "18179    1.000000\n",
       "18180    1.354321\n",
       "18181    1.354068\n",
       "18185    1.341488\n",
       "Name: ge1, Length: 5230, dtype: float64"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imputed_df_bpca['ge1']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model - Da Xu (interview paper) DL method"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "https://www.sciencedirect.com/science/article/pii/S1532046420302045\n",
    "https://ieeexplore.ieee.org/document/9238392"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from tqdm import tqdm\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "class DeepAutoencoderModel(nn.Module):\n",
    "    \"\"\"\n",
    "    Deep Autoencoder model for missing value imputation with patient embedding\n",
    "    \n",
    "    Based on the papers:\n",
    "    - \"A Deep Learningâ€“Based Unsupervised Method to Impute Missing Values in Patient Records\"\n",
    "    - \"A deep learningâ€“based, unsupervised method to impute missing values in electronic health records\"\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim, patient_vocab_size, embedding_dim=16, hidden_dims=(64, 32, 16, 32, 64), \n",
    "                 dropout_rate=0.2, activation='relu'):\n",
    "        \"\"\"\n",
    "        Initialize the model\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        input_dim : int\n",
    "            Dimension of input data\n",
    "        patient_vocab_size : int\n",
    "            Number of unique patients for embedding\n",
    "        embedding_dim : int\n",
    "            Size of patient embedding vectors\n",
    "        hidden_dims : tuple\n",
    "            Sizes of hidden layers\n",
    "        dropout_rate : float\n",
    "            Dropout rate for regularization\n",
    "        activation : str\n",
    "            Activation function ('relu' or 'elu')\n",
    "        \"\"\"\n",
    "        super(DeepAutoencoderModel, self).__init__()\n",
    "        \n",
    "        # Patient embedding layer\n",
    "        self.patient_embedding = nn.Embedding(patient_vocab_size + 1, embedding_dim)\n",
    "        \n",
    "        # Build encoder layers\n",
    "        self.encoder_layers = nn.ModuleList()\n",
    "        prev_dim = input_dim\n",
    "        \n",
    "        # Get the number of layers for encoder (half of hidden_dims rounded up)\n",
    "        encoder_size = (len(hidden_dims) + 1) // 2\n",
    "        \n",
    "        for i in range(encoder_size):\n",
    "            layer = nn.Sequential(\n",
    "                nn.Linear(prev_dim, hidden_dims[i]),\n",
    "                nn.BatchNorm1d(hidden_dims[i]),\n",
    "                nn.ReLU() if activation == 'relu' else nn.ELU(),\n",
    "                nn.Dropout(dropout_rate)\n",
    "            )\n",
    "            self.encoder_layers.append(layer)\n",
    "            prev_dim = hidden_dims[i]\n",
    "        \n",
    "        # Code layer dimension (bottleneck)\n",
    "        self.code_dim = hidden_dims[encoder_size - 1]\n",
    "        \n",
    "        # Build decoder layers\n",
    "        self.decoder_layers = nn.ModuleList()\n",
    "        \n",
    "        # First decoder layer takes concatenated code and patient embedding\n",
    "        prev_dim = self.code_dim + embedding_dim\n",
    "        \n",
    "        for i in range(encoder_size, len(hidden_dims)):\n",
    "            layer = nn.Sequential(\n",
    "                nn.Linear(prev_dim, hidden_dims[i]),\n",
    "                nn.BatchNorm1d(hidden_dims[i]),\n",
    "                nn.ReLU() if activation == 'relu' else nn.ELU(),\n",
    "                nn.Dropout(dropout_rate)\n",
    "            )\n",
    "            self.decoder_layers.append(layer)\n",
    "            prev_dim = hidden_dims[i]\n",
    "        \n",
    "        # Output layer\n",
    "        self.output_layer = nn.Linear(prev_dim, input_dim)\n",
    "        \n",
    "    def encode(self, x):\n",
    "        \"\"\"\n",
    "        Encode input data to latent representation\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        x : torch.Tensor\n",
    "            Input data\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        torch.Tensor\n",
    "            Encoded representation\n",
    "        \"\"\"\n",
    "        h = x\n",
    "        for layer in self.encoder_layers:\n",
    "            h = layer(h)\n",
    "        return h\n",
    "    \n",
    "    def decode(self, code, patient_emb):\n",
    "        \"\"\"\n",
    "        Decode latent representation to reconstructed input\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        code : torch.Tensor\n",
    "            Encoded representation\n",
    "        patient_emb : torch.Tensor\n",
    "            Patient embedding vectors\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        torch.Tensor\n",
    "            Reconstructed input\n",
    "        \"\"\"\n",
    "        # Concatenate code and patient embedding\n",
    "        h = torch.cat([code, patient_emb], dim=1)\n",
    "        \n",
    "        for layer in self.decoder_layers:\n",
    "            h = layer(h)\n",
    "            \n",
    "        return self.output_layer(h)\n",
    "    \n",
    "    def forward(self, x, patient_ids):\n",
    "        \"\"\"\n",
    "        Forward pass through the model\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        x : torch.Tensor\n",
    "            Input data\n",
    "        patient_ids : torch.Tensor\n",
    "            Patient IDs\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        torch.Tensor\n",
    "            Reconstructed input\n",
    "        \"\"\"\n",
    "        # Get patient embeddings\n",
    "        patient_emb = self.patient_embedding(patient_ids).squeeze(1)\n",
    "        \n",
    "        # Encode input\n",
    "        code = self.encode(x)\n",
    "        \n",
    "        # Decode latent representation\n",
    "        output = self.decode(code, patient_emb)\n",
    "        \n",
    "        return output\n",
    "\n",
    "\n",
    "class MaskedMSELoss(nn.Module):\n",
    "    \"\"\"\n",
    "    MSE loss that ignores missing values\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(MaskedMSELoss, self).__init__()\n",
    "        \n",
    "    def forward(self, pred, target, mask):\n",
    "        \"\"\"\n",
    "        Calculate MSE loss only on observed values\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        pred : torch.Tensor\n",
    "            Predicted values\n",
    "        target : torch.Tensor\n",
    "            Target values\n",
    "        mask : torch.Tensor\n",
    "            Binary mask (1 for observed, 0 for missing)\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        torch.Tensor\n",
    "            Masked MSE loss\n",
    "        \"\"\"\n",
    "        # Apply mask to predictions and targets\n",
    "        masked_pred = pred * mask\n",
    "        masked_target = target * mask\n",
    "        \n",
    "        # Calculate squared error\n",
    "        squared_error = (masked_pred - masked_target) ** 2\n",
    "        \n",
    "        # Sum squared error and count observed values\n",
    "        sum_squared_error = torch.sum(squared_error)\n",
    "        count = torch.sum(mask) + 1e-8  # Add small epsilon to avoid division by zero\n",
    "        \n",
    "        # Return MSE\n",
    "        return sum_squared_error / count\n",
    "\n",
    "\n",
    "class TemporalSimilarityLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Loss component for temporal similarity regularization\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(TemporalSimilarityLoss, self).__init__()\n",
    "        \n",
    "    def forward(self, pred, patient_batch_indices):\n",
    "        \"\"\"\n",
    "        Calculate temporal similarity loss based on batch proximity\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        pred : torch.Tensor\n",
    "            Predicted values\n",
    "        patient_batch_indices : torch.Tensor\n",
    "            Indices grouping patients in the batch\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        torch.Tensor\n",
    "            Temporal similarity loss\n",
    "        \"\"\"\n",
    "        batch_size = pred.size(0)\n",
    "        \n",
    "        # If batch size is too small, return zero loss\n",
    "        if batch_size <= 1:\n",
    "            return torch.tensor(0.0, device=pred.device)\n",
    "        \n",
    "        # Create batch position indices\n",
    "        indices = torch.arange(batch_size, device=pred.device).float()\n",
    "        \n",
    "        # Expand dimensions for pairwise operations\n",
    "        pred_expanded_1 = pred.unsqueeze(1)  # [batch, 1, features]\n",
    "        pred_expanded_2 = pred.unsqueeze(0)  # [1, batch, features]\n",
    "        \n",
    "        indices_1 = indices.unsqueeze(1)  # [batch, 1]\n",
    "        indices_2 = indices.unsqueeze(0)  # [1, batch]\n",
    "        \n",
    "        # Calculate pairwise differences between predictions\n",
    "        pred_diff = torch.sum((pred_expanded_1 - pred_expanded_2) ** 2, dim=2)\n",
    "        \n",
    "        # Create patient similarity mask (1 where same patient, 0 otherwise)\n",
    "        patient_expanded_1 = patient_batch_indices.unsqueeze(1)\n",
    "        patient_expanded_2 = patient_batch_indices.unsqueeze(0)\n",
    "        patient_mask = (patient_expanded_1 == patient_expanded_2).float()\n",
    "        \n",
    "        # Create diagonal mask to exclude self-comparisons\n",
    "        diag_mask = 1.0 - torch.eye(batch_size, device=pred.device)\n",
    "        \n",
    "        # Combine masks\n",
    "        combined_mask = patient_mask * diag_mask\n",
    "        \n",
    "        # If no patient pairs exist, return zero loss\n",
    "        if torch.sum(combined_mask) == 0:\n",
    "            return torch.tensor(0.0, device=pred.device)\n",
    "        \n",
    "        # Calculate temporal similarity weights based on proximity in batch\n",
    "        # (Simple approach: closer indices = more similar)\n",
    "        temporal_diff = torch.abs(indices_1 - indices_2)\n",
    "        similarity_weights = 1.0 / (temporal_diff + 1.0)\n",
    "        \n",
    "        # Apply masks and weights\n",
    "        weighted_diff = pred_diff * similarity_weights * combined_mask\n",
    "        \n",
    "        # Normalize and return loss\n",
    "        return torch.sum(weighted_diff) / (torch.sum(combined_mask) + 1e-8)\n",
    "\n",
    "\n",
    "class DeepAutoencoderImputer:\n",
    "    \"\"\"\n",
    "    Deep Learning-Based Unsupervised Method for Missing Value Imputation\n",
    "    \n",
    "    Based on the papers:\n",
    "    - \"A Deep Learningâ€“Based Unsupervised Method to Impute Missing Values in Patient Records\"\n",
    "    - \"A deep learningâ€“based, unsupervised method to impute missing values in electronic health records\"\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 embedding_dim=16,\n",
    "                 hidden_layers=(64, 32, 16, 32, 64),\n",
    "                 activation='relu',\n",
    "                 dropout_rate=0.2,\n",
    "                 learning_rate=0.001,\n",
    "                 weight_decay=0.00001,\n",
    "                 temporal_weight=0.3,\n",
    "                 batch_size=16,\n",
    "                 epochs=100,\n",
    "                 patience=10,\n",
    "                 device=None,\n",
    "                 verbose=1):\n",
    "        \"\"\"\n",
    "        Initialize imputer\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        embedding_dim : int\n",
    "            Size of patient embedding vectors\n",
    "        hidden_layers : tuple\n",
    "            Sizes of hidden layers\n",
    "        activation : str\n",
    "            Activation function ('relu' or 'elu')\n",
    "        dropout_rate : float\n",
    "            Dropout rate for regularization\n",
    "        learning_rate : float\n",
    "            Learning rate for optimizer\n",
    "        weight_decay : float\n",
    "            L2 regularization strength\n",
    "        temporal_weight : float\n",
    "            Weight for temporal similarity regularization\n",
    "        batch_size : int\n",
    "            Batch size for training\n",
    "        epochs : int\n",
    "            Maximum number of training epochs\n",
    "        patience : int\n",
    "            Patience for early stopping\n",
    "        device : str or torch.device\n",
    "            Device to use ('cpu', 'cuda', or None for auto-detection)\n",
    "        verbose : int\n",
    "            Verbosity level (0 or 1)\n",
    "        \"\"\"\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_layers = hidden_layers\n",
    "        self.activation = activation\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.learning_rate = learning_rate\n",
    "        self.weight_decay = weight_decay\n",
    "        self.temporal_weight = temporal_weight\n",
    "        self.batch_size = batch_size\n",
    "        self.epochs = epochs\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        \n",
    "        # Set device\n",
    "        if device is None:\n",
    "            self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        else:\n",
    "            self.device = torch.device(device)\n",
    "            \n",
    "        if self.verbose:\n",
    "            print(f\"Using device: {self.device}\")\n",
    "            \n",
    "            # Print CUDA details if using GPU\n",
    "            if self.device.type == 'cuda':\n",
    "                print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")\n",
    "                print(f\"CUDA memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "                print(f\"CUDA capability: {torch.cuda.get_device_properties(0).major}.{torch.cuda.get_device_properties(0).minor}\")\n",
    "        \n",
    "        # Will be initialized during fitting\n",
    "        self.model = None\n",
    "        self.scaler = None\n",
    "        self.columns = None\n",
    "        self.patient_id_col = None\n",
    "        self.time_col = None\n",
    "        self.numerical_cols = []\n",
    "        self.categorical_cols = []\n",
    "        self.ordinal_cols = []\n",
    "        self.cat_encoders = {}\n",
    "        self.columns_to_impute = None\n",
    "        self.history = {'train_loss': [], 'val_loss': []}\n",
    "        \n",
    "    def fit(self, X, patient_id_col, time_col=None, numerical_cols=None, \n",
    "            categorical_cols=None, ordinal_cols=None):\n",
    "        \"\"\"\n",
    "        Fit the imputer model\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X : DataFrame\n",
    "            Input data with missing values\n",
    "        patient_id_col : str\n",
    "            Name of the column containing patient IDs\n",
    "        time_col : str, optional\n",
    "            Name of the column containing time information\n",
    "        numerical_cols : list, optional\n",
    "            Names of numerical columns. If None, autodetect.\n",
    "        categorical_cols : list, optional\n",
    "            Names of categorical columns. If None, autodetect.\n",
    "        ordinal_cols : list, optional\n",
    "            Names of ordinal columns. If None, autodetect.\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        self : object\n",
    "            Returns self\n",
    "        \"\"\"\n",
    "        self.original_data = X.copy()\n",
    "        self.original_columns = X.columns.tolist()\n",
    "        self.patient_id_col = patient_id_col\n",
    "        self.time_col = time_col\n",
    "        \n",
    "        # Autodetect column types if not provided\n",
    "        if numerical_cols is None:\n",
    "            numerical_cols = X.select_dtypes(include=['float', 'int']).columns.tolist()\n",
    "            # Exclude patient_id_col\n",
    "            if patient_id_col in numerical_cols:\n",
    "                numerical_cols.remove(patient_id_col)\n",
    "        self.numerical_cols = numerical_cols\n",
    "        \n",
    "        if categorical_cols is None:\n",
    "            categorical_cols = X.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "            # Exclude patient_id_col and time_col\n",
    "            if patient_id_col in categorical_cols:\n",
    "                categorical_cols.remove(patient_id_col)\n",
    "            if time_col and time_col in categorical_cols:\n",
    "                categorical_cols.remove(time_col)\n",
    "        self.categorical_cols = categorical_cols\n",
    "        \n",
    "        if ordinal_cols is None:\n",
    "            ordinal_cols = []\n",
    "        self.ordinal_cols = ordinal_cols\n",
    "        \n",
    "        # Preprocess data\n",
    "        X_processed, missing_mask, cat_indices = self._preprocess_data(X, fit=True)\n",
    "        \n",
    "        # Extract patient IDs\n",
    "        patient_ids = X[patient_id_col].values\n",
    "        \n",
    "        # Train model\n",
    "        self._train_model(X_processed, patient_ids, missing_mask)\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def _preprocess_data(self, X, fit=True):\n",
    "        \"\"\"\n",
    "        Preprocess data for the imputation model\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X : DataFrame\n",
    "            Input data\n",
    "        fit : bool\n",
    "            Whether to fit or transform\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        tuple\n",
    "            (processed data, missing mask, categorical indices)\n",
    "        \"\"\"\n",
    "        # Make a copy of the input data\n",
    "        X_processed = X.copy()\n",
    "\n",
    "        # Track the derived date columns to avoid adding them to columns_to_impute\n",
    "        self.derived_date_cols = []\n",
    "\n",
    "        # Handle datetime column if present\n",
    "        if self.time_col and self.time_col in X.columns and pd.api.types.is_datetime64_any_dtype(X[self.time_col]):\n",
    "            # Extract features from datetime\n",
    "            date_year_col = f'{self.time_col}_year'\n",
    "            date_month_col = f'{self.time_col}_month'\n",
    "            date_day_col = f'{self.time_col}_day'\n",
    "            \n",
    "            X_processed[date_year_col] = X[self.time_col].dt.year\n",
    "            X_processed[date_month_col] = X[self.time_col].dt.month\n",
    "            X_processed[date_day_col] = X[self.time_col].dt.day\n",
    "            \n",
    "            # Store derived date columns\n",
    "            self.derived_date_cols = [date_year_col, date_month_col, date_day_col]\n",
    "            \n",
    "            # Add these new columns to numerical cols but NOT to columns_to_impute\n",
    "            if fit:\n",
    "                for col in self.derived_date_cols:\n",
    "                    if col not in self.numerical_cols:\n",
    "                        self.numerical_cols.append(col)\n",
    "                    \n",
    "            # Drop the original datetime column\n",
    "            X_processed = X_processed.drop(columns=[self.time_col])\n",
    "\n",
    "        # Create missing value mask (1 for observed, 0 for missing)\n",
    "        missing_mask = ~X_processed.isna()\n",
    "\n",
    "        # Scale numerical columns\n",
    "        if self.numerical_cols:\n",
    "            # Use only available numeric columns\n",
    "            numeric_cols_present = [col for col in self.numerical_cols if col in X_processed.columns]\n",
    "            \n",
    "            if fit:\n",
    "                self.num_scaler = StandardScaler()\n",
    "                if numeric_cols_present:\n",
    "                    # Temporarily fill missing values for scaling\n",
    "                    temp_imputer = SimpleImputer(strategy='mean')\n",
    "                    X_temp = pd.DataFrame(\n",
    "                        temp_imputer.fit_transform(X_processed[numeric_cols_present]),\n",
    "                        columns=numeric_cols_present,\n",
    "                        index=X_processed.index\n",
    "                    )\n",
    "                    \n",
    "                    # Fit scaler on non-missing data\n",
    "                    X_numerical_scaled = self.num_scaler.fit_transform(X_temp)\n",
    "                    \n",
    "                    # Update processed data with scaled values\n",
    "                    for i, col in enumerate(numeric_cols_present):\n",
    "                        X_processed[col] = X_numerical_scaled[:, i]\n",
    "            else:\n",
    "                if numeric_cols_present:\n",
    "                    # Temporarily fill missing values for scaling\n",
    "                    temp_imputer = SimpleImputer(strategy='mean')\n",
    "                    X_temp = pd.DataFrame(\n",
    "                        temp_imputer.fit_transform(X_processed[numeric_cols_present]),\n",
    "                        columns=numeric_cols_present,\n",
    "                        index=X_processed.index\n",
    "                    )\n",
    "                    \n",
    "                    # Transform with scaler\n",
    "                    X_numerical_scaled = self.num_scaler.transform(X_temp)\n",
    "                    \n",
    "                    # Update processed data with scaled values\n",
    "                    for i, col in enumerate(numeric_cols_present):\n",
    "                        X_processed[col] = X_numerical_scaled[:, i]\n",
    "\n",
    "        # Encode categorical columns\n",
    "        cat_indices = {}\n",
    "        current_idx = len(self.numerical_cols)\n",
    "\n",
    "        for col in self.categorical_cols:\n",
    "            if col in X_processed.columns:\n",
    "                # Handle categorical data safely\n",
    "                if pd.api.types.is_categorical_dtype(X_processed[col]):\n",
    "                    # Convert to string first to avoid category errors\n",
    "                    filled_col = X_processed[col].astype(str).fillna('MISSING')\n",
    "                else:\n",
    "                    # Fill NaN values with a placeholder\n",
    "                    filled_col = X_processed[col].fillna('MISSING')\n",
    "                \n",
    "                if fit:\n",
    "                    # Handle both older and newer scikit-learn versions\n",
    "                    try:\n",
    "                        # For newer scikit-learn versions\n",
    "                        encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "                    except TypeError:\n",
    "                        # For older scikit-learn versions\n",
    "                        encoder = OneHotEncoder(sparse=False, handle_unknown='ignore')\n",
    "                        \n",
    "                    encoded = encoder.fit_transform(filled_col.values.reshape(-1, 1))\n",
    "                    self.cat_encoders[col] = encoder\n",
    "                else:\n",
    "                    encoder = self.cat_encoders[col]\n",
    "                    encoded = encoder.transform(filled_col.values.reshape(-1, 1))\n",
    "                \n",
    "                # Add encoded columns\n",
    "                encoded_cols = [f\"{col}_{cat}\" for cat in encoder.categories_[0]]\n",
    "                cat_indices[col] = (current_idx, current_idx + len(encoded_cols))\n",
    "                current_idx += len(encoded_cols)\n",
    "                \n",
    "                for i, encoded_col in enumerate(encoded_cols):\n",
    "                    X_processed[encoded_col] = encoded[:, i]\n",
    "                \n",
    "                # Drop original categorical column\n",
    "                X_processed = X_processed.drop(columns=[col])\n",
    "                \n",
    "                # Update missing mask for one-hot encoded columns\n",
    "                for encoded_col in encoded_cols:\n",
    "                    missing_mask[encoded_col] = missing_mask[col]\n",
    "\n",
    "        # Drop non-numeric columns and ensure all data is numeric\n",
    "        X_processed = X_processed.select_dtypes(include=[np.number])\n",
    "        missing_mask = missing_mask[X_processed.columns]\n",
    "\n",
    "        if fit:\n",
    "            self.processed_columns = X_processed.columns.tolist()\n",
    "            \n",
    "        return X_processed, missing_mask, cat_indices\n",
    "    \n",
    "    def _train_model(self, X_processed, patient_ids, missing_mask):\n",
    "        \"\"\"\n",
    "        Train the imputation model\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X_processed : DataFrame\n",
    "            Preprocessed data\n",
    "        patient_ids : ndarray\n",
    "            Patient IDs for each record\n",
    "        missing_mask : DataFrame\n",
    "            Binary mask for observed values\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        None\n",
    "        \"\"\"\n",
    "        # Convert to numpy arrays\n",
    "        X_np = X_processed.values\n",
    "        mask_np = missing_mask.values\n",
    "        \n",
    "        # Create patient ID mapping\n",
    "        unique_patient_ids = np.unique(patient_ids)\n",
    "        patient_id_to_idx = {id_: i for i, id_ in enumerate(unique_patient_ids)}\n",
    "        \n",
    "        # Convert patient IDs to numeric indices\n",
    "        patient_indices = np.array([patient_id_to_idx.get(id_, len(patient_id_to_idx)) for id_ in patient_ids])\n",
    "        \n",
    "        # Also create a mapping from record index to patient index for temporal loss\n",
    "        record_to_patient_idx = patient_indices.copy()\n",
    "        \n",
    "        # Convert to PyTorch tensors\n",
    "        X_tensor = torch.FloatTensor(X_np)\n",
    "        mask_tensor = torch.FloatTensor(mask_np)\n",
    "        patient_tensor = torch.LongTensor(patient_indices)\n",
    "        patient_batch_tensor = torch.LongTensor(record_to_patient_idx)\n",
    "        \n",
    "        # Create dataset and dataloader\n",
    "        dataset = TensorDataset(X_tensor, mask_tensor, patient_tensor, patient_batch_tensor)\n",
    "        \n",
    "        # Split into train and validation sets\n",
    "        train_size = int(0.8 * len(dataset))\n",
    "        val_size = len(dataset) - train_size\n",
    "        train_dataset, val_dataset = torch.utils.data.random_split(\n",
    "            dataset, [train_size, val_size]\n",
    "        )\n",
    "        \n",
    "        train_loader = DataLoader(\n",
    "            train_dataset, \n",
    "            batch_size=self.batch_size, \n",
    "            shuffle=True,\n",
    "            num_workers=0 if self.device.type == 'cuda' else 2  # Use multiple workers on CPU\n",
    "        )\n",
    "        \n",
    "        val_loader = DataLoader(\n",
    "            val_dataset, \n",
    "            batch_size=self.batch_size, \n",
    "            shuffle=False,\n",
    "            num_workers=0 if self.device.type == 'cuda' else 2\n",
    "        )\n",
    "        \n",
    "        # Initialize model\n",
    "        input_dim = X_np.shape[1]\n",
    "        patient_vocab_size = len(unique_patient_ids)\n",
    "        \n",
    "        if self.verbose:\n",
    "            print(f\"Building model with input dimension {input_dim} and {patient_vocab_size} unique patients\")\n",
    "            \n",
    "        self.model = DeepAutoencoderModel(\n",
    "            input_dim=input_dim,\n",
    "            patient_vocab_size=patient_vocab_size,\n",
    "            embedding_dim=self.embedding_dim,\n",
    "            hidden_dims=self.hidden_layers,\n",
    "            dropout_rate=self.dropout_rate,\n",
    "            activation=self.activation\n",
    "        ).to(self.device)\n",
    "        \n",
    "        # Initialize loss functions\n",
    "        masked_loss_fn = MaskedMSELoss().to(self.device)\n",
    "        temporal_loss_fn = TemporalSimilarityLoss().to(self.device)\n",
    "        \n",
    "        # Initialize optimizer\n",
    "        optimizer = optim.Adam(\n",
    "            self.model.parameters(), \n",
    "            lr=self.learning_rate, \n",
    "            weight_decay=self.weight_decay\n",
    "        )\n",
    "        \n",
    "        # Initialize learning rate scheduler\n",
    "        scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "            optimizer, \n",
    "            mode='min', \n",
    "            factor=0.5, \n",
    "            patience=self.patience // 2, \n",
    "            verbose=self.verbose\n",
    "        )\n",
    "        \n",
    "        # Initialize early stopping variables\n",
    "        best_val_loss = float('inf')\n",
    "        best_model_state = None\n",
    "        patience_counter = 0\n",
    "        \n",
    "        # Training loop\n",
    "        if self.verbose:\n",
    "            print(f\"Starting training for {self.epochs} epochs\")\n",
    "            \n",
    "        # Use tqdm for progress tracking\n",
    "        pbar = tqdm(range(self.epochs), desc=\"Training\", disable=not self.verbose)\n",
    "        \n",
    "        for epoch in pbar:\n",
    "            # Training phase\n",
    "            self.model.train()\n",
    "            train_loss = 0.0\n",
    "            train_recon_loss = 0.0\n",
    "            train_temporal_loss = 0.0\n",
    "            batch_count = 0\n",
    "            \n",
    "            for X_batch, mask_batch, patient_batch, patient_idx_batch in train_loader:\n",
    "                # Move tensors to device\n",
    "                X_batch = X_batch.to(self.device)\n",
    "                mask_batch = mask_batch.to(self.device)\n",
    "                patient_batch = patient_batch.to(self.device)\n",
    "                patient_idx_batch = patient_idx_batch.to(self.device)\n",
    "                \n",
    "                # Forward pass\n",
    "                output = self.model(X_batch, patient_batch)\n",
    "                \n",
    "                # Calculate loss\n",
    "                recon_loss = masked_loss_fn(output, X_batch, mask_batch)\n",
    "                \n",
    "                if self.temporal_weight > 0:\n",
    "                    temporal_loss = temporal_loss_fn(output, patient_idx_batch)\n",
    "                    loss = recon_loss + self.temporal_weight * temporal_loss\n",
    "                else:\n",
    "                    temporal_loss = torch.tensor(0.0, device=self.device)\n",
    "                    loss = recon_loss\n",
    "                \n",
    "                # Backward pass and optimize\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "                # Accumulate losses\n",
    "                train_loss += loss.item()\n",
    "                train_recon_loss += recon_loss.item()\n",
    "                train_temporal_loss += temporal_loss.item()\n",
    "                batch_count += 1\n",
    "            \n",
    "            # Calculate average training losses\n",
    "            avg_train_loss = train_loss / batch_count\n",
    "            avg_train_recon_loss = train_recon_loss / batch_count\n",
    "            avg_train_temporal_loss = train_temporal_loss / batch_count\n",
    "            \n",
    "            # Validation phase\n",
    "            self.model.eval()\n",
    "            val_loss = 0.0\n",
    "            val_batch_count = 0\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                for X_batch, mask_batch, patient_batch, patient_idx_batch in val_loader:\n",
    "                    # Move tensors to device\n",
    "                    X_batch = X_batch.to(self.device)\n",
    "                    mask_batch = mask_batch.to(self.device)\n",
    "                    patient_batch = patient_batch.to(self.device)\n",
    "                    patient_idx_batch = patient_idx_batch.to(self.device)\n",
    "                    \n",
    "                    # Forward pass\n",
    "                    output = self.model(X_batch, patient_batch)\n",
    "                    \n",
    "                    # Calculate loss\n",
    "                    recon_loss = masked_loss_fn(output, X_batch, mask_batch)\n",
    "                    \n",
    "                    if self.temporal_weight > 0:\n",
    "                        temporal_loss = temporal_loss_fn(output, patient_idx_batch)\n",
    "                        loss = recon_loss + self.temporal_weight * temporal_loss\n",
    "                    else:\n",
    "                        loss = recon_loss\n",
    "                    \n",
    "                    # Accumulate loss\n",
    "                    val_loss += loss.item()\n",
    "                    val_batch_count += 1\n",
    "            \n",
    "            # Calculate average validation loss\n",
    "            avg_val_loss = val_loss / val_batch_count\n",
    "            \n",
    "            # Update learning rate scheduler\n",
    "            scheduler.step(avg_val_loss)\n",
    "            \n",
    "            # Update history\n",
    "            self.history['train_loss'].append(avg_train_loss)\n",
    "            self.history['val_loss'].append(avg_val_loss)\n",
    "            \n",
    "            # Update progress bar\n",
    "            if self.verbose:\n",
    "                pbar.set_postfix({\n",
    "                    \"train_loss\": f\"{avg_train_loss:.4f}\",\n",
    "                    \"val_loss\": f\"{avg_val_loss:.4f}\",\n",
    "                    \"recon\": f\"{avg_train_recon_loss:.4f}\",\n",
    "                    \"temporal\": f\"{avg_train_temporal_loss:.4f}\"\n",
    "                })\n",
    "            \n",
    "            # Check for early stopping\n",
    "            if avg_val_loss < best_val_loss:\n",
    "                best_val_loss = avg_val_loss\n",
    "                best_model_state = self.model.state_dict()\n",
    "                patience_counter = 0\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "                if patience_counter >= self.patience:\n",
    "                    if self.verbose:\n",
    "                        print(f\"Early stopping at epoch {epoch+1}\")\n",
    "                    break\n",
    "        \n",
    "        # Load best model\n",
    "        if best_model_state is not None:\n",
    "            self.model.load_state_dict(best_model_state)\n",
    "            \n",
    "        # Return model to evaluation mode\n",
    "        self.model.eval()\n",
    "    \n",
    "    def transform(self, X):\n",
    "        \"\"\"\n",
    "        Impute missing values using trained model\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X : DataFrame\n",
    "            Input data with missing values\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        DataFrame\n",
    "            Data with imputed values\n",
    "        \"\"\"\n",
    "        # Check if model is trained\n",
    "        if self.model is None:\n",
    "            raise ValueError(\"Model not trained. Call fit() first.\")\n",
    "        \n",
    "        # Make a copy of the input data\n",
    "        X_imputed = X.copy()\n",
    "        \n",
    "        # Preprocess data\n",
    "        X_processed, missing_mask, cat_indices = self._preprocess_data(X, fit=False)\n",
    "        \n",
    "        # Get patient IDs\n",
    "        patient_ids = X[self.patient_id_col].values\n",
    "        \n",
    "        # Create patient ID mapping\n",
    "        unique_patient_ids = np.unique(self.original_data[self.patient_id_col])\n",
    "        patient_id_to_idx = {id_: i for i, id_ in enumerate(unique_patient_ids)}\n",
    "        \n",
    "        # Convert patient IDs to numeric indices (with fallback for unseen patients)\n",
    "        patient_indices = np.array([patient_id_to_idx.get(id_, len(patient_id_to_idx)) for id_ in patient_ids])\n",
    "        \n",
    "        # Convert to PyTorch tensors\n",
    "        X_tensor = torch.FloatTensor(X_processed.values).to(self.device)\n",
    "        patient_tensor = torch.LongTensor(patient_indices).to(self.device)\n",
    "        \n",
    "        # Perform imputation in batches to avoid memory issues\n",
    "        batch_size = self.batch_size * 2  # Use larger batch size for inference\n",
    "        n_samples = X_tensor.shape[0]\n",
    "        imputed_data = np.zeros_like(X_tensor.cpu().numpy())\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for i in range(0, n_samples, batch_size):\n",
    "                end_idx = min(i + batch_size, n_samples)\n",
    "                batch_X = X_tensor[i:end_idx]\n",
    "                batch_patient = patient_tensor[i:end_idx]\n",
    "                \n",
    "                # Get model predictions\n",
    "                batch_output = self.model(batch_X, batch_patient)\n",
    "                \n",
    "                # Store predictions\n",
    "                imputed_data[i:end_idx] = batch_output.cpu().numpy()\n",
    "        \n",
    "        # Convert back to DataFrame\n",
    "        imputed_df = pd.DataFrame(imputed_data, columns=X_processed.columns, index=X_processed.index)\n",
    "        \n",
    "        # Handle numerical columns\n",
    "        for col in self.numerical_cols:\n",
    "            if col in X.columns and not col.startswith(f'{self.time_col}_'):\n",
    "                # Get mask for missing values in original data\n",
    "                missing_indices = X[col].isna()\n",
    "                \n",
    "                if missing_indices.any():\n",
    "                    # Only update missing values\n",
    "                    # If column is in columns_to_impute, round to nearest integer\n",
    "                    if self.columns_to_impute and col in self.columns_to_impute:\n",
    "                        X_imputed.loc[missing_indices, col] = np.round(\n",
    "                            imputed_df.loc[missing_indices, col].values\n",
    "                        )\n",
    "                    else:\n",
    "                        X_imputed.loc[missing_indices, col] = imputed_df.loc[missing_indices, col].values\n",
    "        \n",
    "        # Handle categorical columns (need to convert one-hot back to original categories)\n",
    "        for col in self.categorical_cols:\n",
    "            if col in X.columns:\n",
    "                missing_indices = X[col].isna()\n",
    "                \n",
    "                if missing_indices.any() and col in cat_indices:\n",
    "                    # Get the one-hot encoded columns for this categorical variable\n",
    "                    start_idx, end_idx = cat_indices[col]\n",
    "                    encoded_cols = self.processed_columns[start_idx:end_idx]\n",
    "                    \n",
    "                    # Get indices where values are missing\n",
    "                    missing_idx = missing_indices[missing_indices].index\n",
    "                    \n",
    "                    # Extract one-hot encoded values for the missing entries\n",
    "                    one_hot_values = imputed_df.loc[missing_idx, encoded_cols].values\n",
    "                    \n",
    "                    # Get the index of the max value along axis 1\n",
    "                    max_indices = np.argmax(one_hot_values, axis=1)\n",
    "                    \n",
    "                    # Map back to original categories\n",
    "                    categories = self.cat_encoders[col].categories_[0]\n",
    "                    imputed_categories = [categories[idx] for idx in max_indices]\n",
    "                    \n",
    "                    # Handle categorical data types safely\n",
    "                    if pd.api.types.is_categorical_dtype(X[col]):\n",
    "                        # Get the existing categories\n",
    "                        existing_cats = X[col].cat.categories\n",
    "                        \n",
    "                        # Filter out categories that aren't in the existing ones\n",
    "                        valid_categories = []\n",
    "                        for cat in imputed_categories:\n",
    "                            if cat in existing_cats:\n",
    "                                valid_categories.append(cat)\n",
    "                            else:\n",
    "                                # Use the most common category as fallback\n",
    "                                most_common = X[col].value_counts().index[0]\n",
    "                                valid_categories.append(most_common)\n",
    "                        \n",
    "                        # Update missing values\n",
    "                        for idx, cat in zip(missing_idx, valid_categories):\n",
    "                            X_imputed.loc[idx, col] = cat\n",
    "                    else:\n",
    "                        # Update missing values in the original DataFrame\n",
    "                        X_imputed.loc[missing_idx, col] = imputed_categories\n",
    "        \n",
    "        return X_imputed\n",
    "    \n",
    "    def fit_transform(self, X, patient_id_col, time_col=None, numerical_cols=None, \n",
    "                     categorical_cols=None, ordinal_cols=None):\n",
    "        \"\"\"\n",
    "        Fit the model and impute missing values\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X : DataFrame\n",
    "            Input data with missing values\n",
    "        patient_id_col : str\n",
    "            Name of the column containing patient IDs\n",
    "        time_col : str, optional\n",
    "            Name of the column containing time information\n",
    "        numerical_cols : list, optional\n",
    "            Names of numerical columns. If None, autodetect.\n",
    "        categorical_cols : list, optional\n",
    "            Names of categorical columns. If None, autodetect.\n",
    "        ordinal_cols : list, optional\n",
    "            Names of ordinal columns. If None, autodetect.\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        DataFrame\n",
    "            Data with imputed values\n",
    "        \"\"\"\n",
    "        self.fit(X, patient_id_col, time_col, numerical_cols, categorical_cols, ordinal_cols)\n",
    "        return self.transform(X)\n",
    "    \n",
    "    def plot_loss(self):\n",
    "        \"\"\"\n",
    "        Plot the training and validation loss curves\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        matplotlib figure\n",
    "            Loss curve plot\n",
    "        \"\"\"\n",
    "        if not self.history['train_loss']:\n",
    "            raise ValueError(\"Model has not been trained yet. Call fit() first.\")\n",
    "        \n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.plot(self.history['train_loss'], label='Training Loss')\n",
    "        plt.plot(self.history['val_loss'], label='Validation Loss')\n",
    "        plt.title('Deep Autoencoder Imputation Loss')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        return plt\n",
    "\n",
    "\n",
    "def apply_deep_autoencoder_imputation(df, columns_to_impute, validation_df=None, validation_masks=None, original_values=None, \n",
    "                                     hidden_layers=(64, 32, 16, 32, 64), latent_dim=8, batch_size=16, epochs=100, learning_rate=0.001):\n",
    "    \"\"\"\n",
    "    Apply deep autoencoder imputation to data with an interface matching your VAE implementation\n",
    "    \n",
    "        Parameters:\n",
    "    -----------\n",
    "    df : pandas.DataFrame\n",
    "        Data with missing values\n",
    "    columns_to_impute : list\n",
    "        List of column names to impute\n",
    "    validation_df : pandas.DataFrame, optional\n",
    "        Validation dataset with artificially missing values\n",
    "    validation_masks : dict, optional\n",
    "        Dictionary of masks for validation data\n",
    "    original_values : dict, optional\n",
    "        Dictionary of original values for validation\n",
    "    hidden_layers : tuple\n",
    "        Sizes of hidden layers\n",
    "    latent_dim : int\n",
    "        Dimension of the patient embedding (equivalent to latent_dim in VAE)\n",
    "    batch_size : int\n",
    "        Batch size for training\n",
    "    epochs : int\n",
    "        Number of training epochs\n",
    "    learning_rate : float\n",
    "        Learning rate for optimizer\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    imputed_df : pandas.DataFrame\n",
    "        Data with imputed values\n",
    "    validation_results : dict, optional\n",
    "        Validation results if validation data provided\n",
    "    \"\"\"\n",
    "    # Print column information\n",
    "    print(f\"Data has {len(df)} rows and {len(df.columns)} columns\")\n",
    "    print(f\"Columns to impute: {columns_to_impute}\")\n",
    "\n",
    "    # IMPORTANT: Make a copy of columns_to_impute to avoid modifying the original list\n",
    "    local_columns_to_impute = [col for col in columns_to_impute if col in df.columns]\n",
    "\n",
    "    # Remove any date-derived columns that might have been added from a previous run\n",
    "    date_derived_cols = [col for col in columns_to_impute if col.startswith('qol_date_')]\n",
    "    if date_derived_cols:\n",
    "        print(f\"Warning: Date-derived columns found in columns_to_impute: {date_derived_cols}\")\n",
    "        print(f\"These will not be used for imputation.\")\n",
    "        for col in date_derived_cols:\n",
    "            if col in local_columns_to_impute:\n",
    "                local_columns_to_impute.remove(col)\n",
    "\n",
    "    # Print missingness information\n",
    "    print(\"\\nMissingness rates in columns to impute:\")\n",
    "    for col in local_columns_to_impute:\n",
    "        if col in df.columns:\n",
    "            miss_rate = df[col].isna().mean() * 100\n",
    "            print(f\"{col}: {miss_rate:.2f}%\")\n",
    "        else:\n",
    "            print(f\"{col}: column not found in data\")\n",
    "\n",
    "    # Extract relevant columns including potential predictors\n",
    "    # Use all columns except those with excessive missing values\n",
    "    threshold = 0.5  # Columns with more than 50% missing values are excluded\n",
    "    columns_to_use = [col for col in df.columns \n",
    "                    if df[col].isna().mean() < threshold]\n",
    "\n",
    "    # Ensure all columns_to_impute are included\n",
    "    for col in local_columns_to_impute:\n",
    "        if col not in columns_to_use and col in df.columns:\n",
    "            columns_to_use.append(col)\n",
    "            \n",
    "    # Always include id and qol_date if available\n",
    "    id_col = 'id'\n",
    "    if id_col not in columns_to_use and id_col in df.columns:\n",
    "        columns_to_use.append(id_col)\n",
    "        \n",
    "    time_col = 'qol_date'\n",
    "    if time_col not in columns_to_use and time_col in df.columns:\n",
    "        columns_to_use.append(time_col)\n",
    "\n",
    "    print(f\"\\nUsing {len(columns_to_use)} columns for imputation model\")\n",
    "\n",
    "    # Extract subset of data\n",
    "    X = df[columns_to_use].copy()\n",
    "\n",
    "    # Ensure all columns to impute are numeric\n",
    "    print(\"\\nConverting columns to numeric...\")\n",
    "    with tqdm(local_columns_to_impute) as pbar:\n",
    "        for col in pbar:\n",
    "            pbar.set_description(f\"Processing {col}\")\n",
    "            if col in X.columns:\n",
    "                # Convert to numeric if possible\n",
    "                X[col] = pd.to_numeric(X[col], errors='coerce')\n",
    "\n",
    "    # Check CUDA availability\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    # Initialize deep autoencoder imputer\n",
    "    print(\"\\nInitializing deep autoencoder imputer...\")\n",
    "    imputer = DeepAutoencoderImputer(\n",
    "        embedding_dim=latent_dim,\n",
    "        hidden_layers=hidden_layers,\n",
    "        activation='elu',\n",
    "        dropout_rate=0.3,\n",
    "        learning_rate=learning_rate,\n",
    "        weight_decay=0.0001,\n",
    "        temporal_weight=0.4,\n",
    "        batch_size=batch_size,\n",
    "        epochs=epochs,\n",
    "        patience=20,\n",
    "        device=device,\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    # Store the columns to impute in the imputer object\n",
    "    imputer.columns_to_impute = local_columns_to_impute\n",
    "\n",
    "    # Fit and transform\n",
    "    print(\"\\nTraining deep autoencoder imputation model...\")\n",
    "    X_imputed = imputer.fit_transform(\n",
    "        X, \n",
    "        patient_id_col=id_col,  # Use 'id' as the patient ID column\n",
    "        time_col=\"qol_date\",  # Use qol_date for temporal information\n",
    "        numerical_cols=local_columns_to_impute  # Only specify columns to impute as numerical\n",
    "    )\n",
    "\n",
    "    # Create imputed dataframe\n",
    "    print(\"\\nCreating final imputed dataframe...\")\n",
    "    imputed_df = df.copy()\n",
    "\n",
    "    # Copy imputed values to the output dataframe\n",
    "    for col in local_columns_to_impute:\n",
    "        if col in X_imputed.columns and col in imputed_df.columns:\n",
    "            # Get mask for missing values in the original data\n",
    "            missing_mask = df[col].isna()\n",
    "            # Only update missing values\n",
    "            imputed_df.loc[missing_mask, col] = X_imputed.loc[missing_mask, col]\n",
    "\n",
    "    # Print post-imputation statistics\n",
    "    print(\"\\nPost-imputation statistics:\")\n",
    "    for col in local_columns_to_impute:\n",
    "        if col in imputed_df.columns:\n",
    "            # Count originally missing values that have been imputed\n",
    "            missing_before = df[col].isna().sum()\n",
    "            missing_after = imputed_df[col].isna().sum()\n",
    "            num_imputed = missing_before - missing_after\n",
    "            print(f\"{col}: {num_imputed} values imputed\")\n",
    "\n",
    "    # Validate if validation data provided\n",
    "    validation_results = None\n",
    "    if validation_df is not None and validation_masks is not None and original_values is not None:\n",
    "        validation_results = {}\n",
    "        \n",
    "        print(\"\\nPerforming validation on held-out data...\")\n",
    "        \n",
    "        # Extract validation data\n",
    "        X_val = validation_df[columns_to_use].copy()\n",
    "        \n",
    "        # Ensure all validation columns are numeric\n",
    "        for col in X_val.columns:\n",
    "            if col in local_columns_to_impute:\n",
    "                X_val[col] = pd.to_numeric(X_val[col], errors='coerce')\n",
    "            \n",
    "        # Impute validation data\n",
    "        print(\"Imputing validation data...\")\n",
    "        X_val_imputed = imputer.transform(X_val)\n",
    "        \n",
    "        # Compare imputed values to real values\n",
    "        print(\"Calculating validation metrics...\")\n",
    "        with tqdm(local_columns_to_impute, desc=\"Validating results\") as pbar:\n",
    "            for col in pbar:\n",
    "                pbar.set_description(f\"Validating {col}\")\n",
    "                # Get indices where values were artificially set to NaN\n",
    "                if col not in validation_masks:\n",
    "                    validation_results[col] = {\n",
    "                        'error': f\"Column {col} not found in validation masks\"\n",
    "                    }\n",
    "                    continue\n",
    "                    \n",
    "                mask = validation_masks[col] & validation_df[col].isna()\n",
    "                \n",
    "                if mask.sum() == 0:\n",
    "                    validation_results[col] = {\n",
    "                        'error': \"No artificially missing values\"\n",
    "                    }\n",
    "                    continue\n",
    "                \n",
    "                if col not in original_values:\n",
    "                    validation_results[col] = {\n",
    "                        'error': f\"Column {col} not found in original values\"\n",
    "                    }\n",
    "                    continue\n",
    "                    \n",
    "                real_vals = original_values[col][mask]\n",
    "                \n",
    "                if col not in X_val_imputed.columns:\n",
    "                    validation_results[col] = {\n",
    "                        'error': f\"Column {col} not found in imputed values\"\n",
    "                    }\n",
    "                    continue\n",
    "                    \n",
    "                imputed_vals = X_val_imputed.loc[mask, col]\n",
    "                \n",
    "                # Calculate MAE and RMSE\n",
    "                mae = mean_absolute_error(real_vals, imputed_vals)\n",
    "                rmse = np.sqrt(mean_squared_error(real_vals, imputed_vals))\n",
    "                \n",
    "                validation_results[col] = {\n",
    "                    'mae': mae,\n",
    "                    'rmse': rmse,\n",
    "                    'real_distribution': real_vals.describe(),\n",
    "                    'imputed_distribution': pd.Series(imputed_vals).describe()\n",
    "                }\n",
    "                \n",
    "                # Update progress\n",
    "                pbar.set_postfix({\"MAE\": f\"{mae:.4f}\", \"RMSE\": f\"{rmse:.4f}\"})\n",
    "\n",
    "    # Plot the training loss curve\n",
    "    print(\"\\nGenerating training loss plot...\")\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(imputer.history['train_loss'], label='Training Loss')\n",
    "    plt.plot(imputer.history['val_loss'], label='Validation Loss')\n",
    "    plt.title('Deep Autoencoder Imputation Loss')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.savefig('deep_autoencoder_loss.png')\n",
    "    plt.close()\n",
    "\n",
    "    print(f\"\\nTraining loss plot saved to deep_autoencoder_loss.png\")\n",
    "    print(\"\\nImputation complete!\")\n",
    "\n",
    "    return imputed_df, validation_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data has 5230 rows and 62 columns\n",
      "Columns to impute: ['ge1', 'ge2', 'ge3', 'ge4', 'ge5', 'ge6']\n",
      "\n",
      "Missingness rates in columns to impute:\n",
      "ge1: 79.89%\n",
      "ge2: 80.52%\n",
      "ge3: 81.24%\n",
      "ge4: 80.15%\n",
      "ge5: 80.21%\n",
      "ge6: 80.11%\n",
      "\n",
      "Using 11 columns for imputation model\n",
      "\n",
      "Converting columns to numeric...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing ge6: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:00<00:00, 2011.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Initializing deep autoencoder imputer..."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Using device: cpu\n",
      "\n",
      "Training deep autoencoder imputation model...\n",
      "Building model with input dimension 34 and 1754 unique patients\n",
      "Starting training for 100 epochs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/100 [00:52<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m columns_to_impute \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mge1\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mge2\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mge3\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mge4\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mge5\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mge6\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# Apply deep autoencoder imputation with explicit column specifications\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m imputed_df, validation_results \u001b[38;5;241m=\u001b[39m \u001b[43mapply_deep_autoencoder_imputation\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcolumns_to_impute\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolumns_to_impute\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[11], line 1073\u001b[0m, in \u001b[0;36mapply_deep_autoencoder_imputation\u001b[0;34m(df, columns_to_impute, validation_df, validation_masks, original_values, hidden_layers, latent_dim, batch_size, epochs, learning_rate)\u001b[0m\n\u001b[1;32m   1071\u001b[0m \u001b[38;5;66;03m# Fit and transform\u001b[39;00m\n\u001b[1;32m   1072\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mTraining deep autoencoder imputation model...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 1073\u001b[0m X_imputed \u001b[38;5;241m=\u001b[39m \u001b[43mimputer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1074\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m   1075\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpatient_id_col\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mid_col\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Use 'id' as the patient ID column\u001b[39;49;00m\n\u001b[1;32m   1076\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtime_col\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mqol_date\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Use qol_date for temporal information\u001b[39;49;00m\n\u001b[1;32m   1077\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnumerical_cols\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_columns_to_impute\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Only specify columns to impute as numerical\u001b[39;49;00m\n\u001b[1;32m   1078\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1080\u001b[0m \u001b[38;5;66;03m# Create imputed dataframe\u001b[39;00m\n\u001b[1;32m   1081\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mCreating final imputed dataframe...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[11], line 928\u001b[0m, in \u001b[0;36mDeepAutoencoderImputer.fit_transform\u001b[0;34m(self, X, patient_id_col, time_col, numerical_cols, categorical_cols, ordinal_cols)\u001b[0m\n\u001b[1;32m    903\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mfit_transform\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, patient_id_col, time_col\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, numerical_cols\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \n\u001b[1;32m    904\u001b[0m                  categorical_cols\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, ordinal_cols\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    905\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    906\u001b[0m \u001b[38;5;124;03m    Fit the model and impute missing values\u001b[39;00m\n\u001b[1;32m    907\u001b[0m \u001b[38;5;124;03m    \u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    926\u001b[0m \u001b[38;5;124;03m        Data with imputed values\u001b[39;00m\n\u001b[1;32m    927\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 928\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpatient_id_col\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtime_col\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnumerical_cols\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcategorical_cols\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mordinal_cols\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    929\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform(X)\n",
      "Cell \u001b[0;32mIn[11], line 418\u001b[0m, in \u001b[0;36mDeepAutoencoderImputer.fit\u001b[0;34m(self, X, patient_id_col, time_col, numerical_cols, categorical_cols, ordinal_cols)\u001b[0m\n\u001b[1;32m    415\u001b[0m patient_ids \u001b[38;5;241m=\u001b[39m X[patient_id_col]\u001b[38;5;241m.\u001b[39mvalues\n\u001b[1;32m    417\u001b[0m \u001b[38;5;66;03m# Train model\u001b[39;00m\n\u001b[0;32m--> 418\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_train_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_processed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpatient_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmissing_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    420\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "Cell \u001b[0;32mIn[11], line 687\u001b[0m, in \u001b[0;36mDeepAutoencoderImputer._train_model\u001b[0;34m(self, X_processed, patient_ids, missing_mask)\u001b[0m\n\u001b[1;32m    684\u001b[0m patient_idx_batch \u001b[38;5;241m=\u001b[39m patient_idx_batch\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m    686\u001b[0m \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[0;32m--> 687\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpatient_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    689\u001b[0m \u001b[38;5;66;03m# Calculate loss\u001b[39;00m\n\u001b[1;32m    690\u001b[0m recon_loss \u001b[38;5;241m=\u001b[39m masked_loss_fn(output, X_batch, mask_batch)\n",
      "File \u001b[0;32m~/jinenv/lib/python3.11/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/jinenv/lib/python3.11/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[11], line 151\u001b[0m, in \u001b[0;36mDeepAutoencoderModel.forward\u001b[0;34m(self, x, patient_ids)\u001b[0m\n\u001b[1;32m    148\u001b[0m patient_emb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpatient_embedding(patient_ids)\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    150\u001b[0m \u001b[38;5;66;03m# Encode input\u001b[39;00m\n\u001b[0;32m--> 151\u001b[0m code \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    153\u001b[0m \u001b[38;5;66;03m# Decode latent representation\u001b[39;00m\n\u001b[1;32m    154\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecode(code, patient_emb)\n",
      "Cell \u001b[0;32mIn[11], line 104\u001b[0m, in \u001b[0;36mDeepAutoencoderModel.encode\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    102\u001b[0m h \u001b[38;5;241m=\u001b[39m x\n\u001b[1;32m    103\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder_layers:\n\u001b[0;32m--> 104\u001b[0m     h \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mh\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    105\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m h\n",
      "File \u001b[0;32m~/jinenv/lib/python3.11/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/jinenv/lib/python3.11/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/jinenv/lib/python3.11/site-packages/torch/nn/modules/container.py:250\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    248\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    249\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 250\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    251\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/jinenv/lib/python3.11/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/jinenv/lib/python3.11/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/jinenv/lib/python3.11/site-packages/torch/nn/modules/batchnorm.py:193\u001b[0m, in \u001b[0;36m_BatchNorm.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    186\u001b[0m     bn_training \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrunning_mean \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrunning_var \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    188\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    189\u001b[0m \u001b[38;5;124;03mBuffers are only updated if they are to be tracked and we are in training mode. Thus they only need to be\u001b[39;00m\n\u001b[1;32m    190\u001b[0m \u001b[38;5;124;03mpassed when the update should occur (i.e. in training mode when they are tracked), or when buffer stats are\u001b[39;00m\n\u001b[1;32m    191\u001b[0m \u001b[38;5;124;03mused for normalization (i.e. in eval mode when buffers are not None).\u001b[39;00m\n\u001b[1;32m    192\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m--> 193\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_norm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    194\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    195\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# If buffers are not to be tracked, ensure that they won't be updated\u001b[39;49;00m\n\u001b[1;32m    196\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrunning_mean\u001b[49m\n\u001b[1;32m    197\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrack_running_stats\u001b[49m\n\u001b[1;32m    198\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    199\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrunning_var\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrack_running_stats\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    200\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    201\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbn_training\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    203\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexponential_average_factor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    204\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    205\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/jinenv/lib/python3.11/site-packages/torch/nn/functional.py:2822\u001b[0m, in \u001b[0;36mbatch_norm\u001b[0;34m(input, running_mean, running_var, weight, bias, training, momentum, eps)\u001b[0m\n\u001b[1;32m   2819\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m training:\n\u001b[1;32m   2820\u001b[0m     _verify_batch_size(\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize())\n\u001b[0;32m-> 2822\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_norm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2823\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2824\u001b[0m \u001b[43m    \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2825\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2826\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrunning_mean\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2827\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrunning_var\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2828\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2829\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmomentum\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2830\u001b[0m \u001b[43m    \u001b[49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2831\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackends\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcudnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menabled\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2832\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "columns_to_impute = ['ge1', 'ge2', 'ge3', 'ge4', 'ge5', 'ge6']\n",
    "# Apply deep autoencoder imputation with explicit column specifications\n",
    "imputed_df, validation_results = apply_deep_autoencoder_imputation(\n",
    "    df=df, \n",
    "    columns_to_impute=columns_to_impute\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Real data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== Analyzing Dataset =====\n",
      "Dataset shape: (18187, 65)\n",
      "\n",
      "Missing value counts:\n",
      "ge1: 15454 missing values (84.97%)\n",
      "ge2: 15497 missing values (85.21%)\n",
      "ge3: 15543 missing values (85.46%)\n",
      "ge4: 15472 missing values (85.07%)\n",
      "ge5: 15482 missing values (85.13%)\n",
      "ge6: 15469 missing values (85.06%)\n",
      "\n",
      "Observed values per column:\n",
      "ge1: 2733 observed values (15.03%)\n",
      "  Range: 0.0 to 4.0\n",
      "  Mean: 1.0893, Std: 1.1332\n",
      "  Unique values: 5 (0.2% unique)\n",
      "ge2: 2690 observed values (14.79%)\n",
      "  Range: 0.0 to 4.0\n",
      "  Mean: 2.4647, Std: 1.1927\n",
      "  Unique values: 5 (0.2% unique)\n",
      "ge3: 2644 observed values (14.54%)\n",
      "  Range: 0.0 to 4.0\n",
      "  Mean: 0.5843, Std: 1.0138\n",
      "  Unique values: 5 (0.2% unique)\n",
      "ge4: 2715 observed values (14.93%)\n",
      "  Range: 0.0 to 4.0\n",
      "  Mean: 1.2891, Std: 1.2363\n",
      "  Unique values: 5 (0.2% unique)\n",
      "ge5: 2705 observed values (14.87%)\n",
      "  Range: 0.0 to 4.0\n",
      "  Mean: 0.9567, Std: 1.1935\n",
      "  Unique values: 5 (0.2% unique)\n",
      "ge6: 2718 observed values (14.94%)\n",
      "  Range: 0.0 to 4.0\n",
      "  Mean: 1.5839, Std: 1.2910\n",
      "  Unique values: 5 (0.2% unique)\n",
      "\n",
      "===== Running imputation on real data =====\n",
      "Missingness statistics before imputation:\n",
      "ge1: 15454 missing values (84.97%)\n",
      "ge2: 15497 missing values (85.21%)\n",
      "ge3: 15543 missing values (85.46%)\n",
      "ge4: 15472 missing values (85.07%)\n",
      "ge5: 15482 missing values (85.13%)\n",
      "ge6: 15469 missing values (85.06%)\n",
      "\n",
      "Running MICE imputation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MICE Imputation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:42<00:00,  8.51s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MICE completed in 42.63 seconds\n",
      "MICE - ge1 distribution similarity: KS=0.4182, p=0.0000\n",
      "MICE - ge2 distribution similarity: KS=0.5261, p=0.0000\n",
      "MICE - ge3 distribution similarity: KS=0.6244, p=0.0000\n",
      "MICE - ge4 distribution similarity: KS=0.5207, p=0.0000\n",
      "MICE - ge5 distribution similarity: KS=0.6050, p=0.0000\n",
      "MICE - ge6 distribution similarity: KS=0.4982, p=0.0000\n",
      "\n",
      "Running Autoencoder imputation...\n",
      "Training autoencoder imputation models...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training model for ge5:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 4/6 [02:25<01:14, 37.03s/it]"
     ]
    }
   ],
   "source": [
    "# Add these imports at the top of your file\n",
    "# from TransformerImputer import apply_transformer_imputation\n",
    "# from CUDATransformerImputer import apply_cudatransformer_imputation\n",
    "\n",
    "def evaluate_real_data_imputation(df, columns_to_impute):\n",
    "    \"\"\"\n",
    "    Evaluate imputation methods on real data with existing missing values\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pandas.DataFrame\n",
    "        Your real dataset with missing values\n",
    "    columns_to_impute : list\n",
    "        List of column names to impute\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    all_imputed_dfs : dict\n",
    "        Dictionary with imputed DataFrames from each method\n",
    "    original_df_with_missing : pandas.DataFrame\n",
    "        Original dataframe with missing values\n",
    "    execution_times : dict\n",
    "        Dictionary with execution times for each method\n",
    "    \"\"\"\n",
    "    # Define the methods to compare\n",
    "    methods = {\n",
    "        'MICE': apply_mice_imputation,\n",
    "        'VAE': apply_vae_imputation,\n",
    "        'DAE': apply_sklearn_dae_imputation,\n",
    "        'Bayesian PCA': apply_bpca_imputation,\n",
    "        'Da XU DL': apply_deep_autoencoder_imputation\n",
    "    }\n",
    "    \n",
    "    # Initialize dictionaries to store results\n",
    "    all_imputed_dfs = {}\n",
    "    execution_times = {}\n",
    "    distribution_similarity = {method: {} for method in methods}\n",
    "    \n",
    "    # Calculate missingness statistics before imputation\n",
    "    print(\"Missingness statistics before imputation:\")\n",
    "    for col in columns_to_impute:\n",
    "        missing_count = df[col].isna().sum()\n",
    "        missing_percent = (missing_count / len(df)) * 100\n",
    "        print(f\"{col}: {missing_count} missing values ({missing_percent:.2f}%)\")\n",
    "    \n",
    "    # Save a copy of the original data with missing values\n",
    "    original_df_with_missing = df.copy()\n",
    "    \n",
    "    # Run each imputation method\n",
    "    for method_name, method_func in methods.items():\n",
    "        print(f\"\\nRunning {method_name} imputation...\")\n",
    "        \n",
    "        try:\n",
    "            # Time the imputation\n",
    "            start_time = time.time()\n",
    "            \n",
    "            # Create a copy of the dataframe for this method\n",
    "            method_df = df.copy()\n",
    "            \n",
    "            # For methods other than 'Da XU DL', drop the qol_date column to avoid issues\n",
    "            if method_name != 'Da XU DL' and 'qol_date' in method_df.columns:\n",
    "                print(f\"Dropping qol_date column for {method_name} method\")\n",
    "                method_df = method_df.drop(columns=['qol_date'])\n",
    "            \n",
    "            # Apply the imputation method\n",
    "            imputed_df, _ = method_func(\n",
    "                method_df, \n",
    "                columns_to_impute\n",
    "            )\n",
    "            \n",
    "            end_time = time.time()\n",
    "            execution_time = end_time - start_time\n",
    "            execution_times[method_name] = execution_time\n",
    "            \n",
    "            # Store the imputed dataframe\n",
    "            all_imputed_dfs[method_name] = imputed_df\n",
    "            \n",
    "            # Print execution time\n",
    "            print(f\"{method_name} completed in {execution_time:.2f} seconds\")\n",
    "            \n",
    "            # Check if imputation was successful (no missing values in imputed columns)\n",
    "            for col in columns_to_impute:\n",
    "                if col not in imputed_df.columns:\n",
    "                    print(f\"Warning: Column {col} missing in {method_name} results\")\n",
    "                    continue\n",
    "                    \n",
    "                remaining_missing = imputed_df[col].isna().sum()\n",
    "                if remaining_missing > 0:\n",
    "                    print(f\"Warning: {method_name} left {remaining_missing} missing values in {col}\")\n",
    "                \n",
    "                # Measure distribution similarity between observed and imputed values\n",
    "                observed_values = original_df_with_missing.loc[original_df_with_missing[col].notna(), col]\n",
    "                imputed_indices = original_df_with_missing[col].isna() & ~imputed_df[col].isna()\n",
    "                \n",
    "                if imputed_indices.any():\n",
    "                    imputed_values = imputed_df.loc[imputed_indices, col]\n",
    "                    \n",
    "                    if len(observed_values) > 0 and len(imputed_values) > 0:\n",
    "                        # Kolmogorov-Smirnov test for distribution similarity\n",
    "                        ks_stat, ks_pval = ks_2samp(observed_values, imputed_values)\n",
    "                        distribution_similarity[method_name][col] = {\n",
    "                            'ks_stat': ks_stat,\n",
    "                            'ks_pval': ks_pval\n",
    "                        }\n",
    "                        print(f\"{method_name} - {col} distribution similarity: KS={ks_stat:.4f}, p={ks_pval:.4f}\")\n",
    "                        #Lower KS statistic and higher p-values indicate better distribution preservation\n",
    "                    \n",
    "        except Exception as e:\n",
    "            print(f\"Error running {method_name}: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()  # Print the full error trace for better diagnosis\n",
    "    \n",
    "    return all_imputed_dfs, original_df_with_missing, execution_times, distribution_similarity\n",
    "\n",
    "\n",
    "def evaluate_with_sparse_validation(df, columns_to_impute, n_folds=3):\n",
    "    \"\"\"\n",
    "    Evaluate imputation methods using cross-validation optimized for sparse data\n",
    "    \n",
    "        Parameters:\n",
    "    -----------\n",
    "    df : pandas.DataFrame\n",
    "        Your real dataset with missing values\n",
    "    columns_to_impute : list\n",
    "        List of column names to impute\n",
    "    n_folds : int\n",
    "        Number of validation folds\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    results : dict\n",
    "        Dictionary with validation results for each method\n",
    "    \"\"\"\n",
    "    # Define the methods to compare\n",
    "    methods = {\n",
    "        'MICE': apply_mice_imputation,\n",
    "        'VAE': apply_vae_imputation,\n",
    "        'DAE': apply_sklearn_dae_imputation,\n",
    "        'Bayesian PCA': apply_bpca_imputation,\n",
    "        'Da XU DL': apply_deep_autoencoder_imputation\n",
    "    }\n",
    "\n",
    "    # Initialize results dictionary\n",
    "    results = {method: {'mae': [], 'rmse': [], 'time': []} for method in methods}\n",
    "\n",
    "    # IMPORTANT: Filter out columns that don't exist in the dataframe and any derived date columns\n",
    "    valid_columns = []\n",
    "    for col in columns_to_impute:\n",
    "        if col in df.columns and not col.startswith('qol_date_'):\n",
    "            valid_columns.append(col)\n",
    "\n",
    "    # Print warning if columns were filtered\n",
    "    if len(valid_columns) != len(columns_to_impute):\n",
    "        excluded_cols = set(columns_to_impute) - set(valid_columns)\n",
    "        print(f\"Warning: Excluding {len(excluded_cols)} columns that are not in the dataframe or are derived date columns:\")\n",
    "        print(f\"  {sorted(excluded_cols)}\")\n",
    "        print(f\"Valid columns for validation: {sorted(valid_columns)}\")\n",
    "\n",
    "    # For each column, perform validation on non-missing values\n",
    "    for col in valid_columns:\n",
    "        print(f\"\\n===== Sparse validation for column: {col} =====\")\n",
    "        \n",
    "        # Get indices of non-missing values for this column\n",
    "        observed_indices = df.index[df[col].notna()].tolist()\n",
    "        n_observed = len(observed_indices)\n",
    "        \n",
    "        print(f\"Column {col}: {n_observed} observed values ({n_observed/len(df)*100:.2f}%)\")\n",
    "        \n",
    "        # Skip if too few observations\n",
    "        if n_observed < max(5, n_folds * 2):\n",
    "            print(f\"Too few observed values for validation. Skipping.\")\n",
    "            continue\n",
    "        \n",
    "        # Instead of standard KFold, use a more conservative approach for sparse data\n",
    "        fold_size = max(5, n_observed // n_folds)  # Ensure at least 5 samples per fold\n",
    "        \n",
    "        # Shuffle observed indices\n",
    "        np.random.seed(random.randint(0, 10000))  # For reproducibility\n",
    "        shuffled_indices = np.random.permutation(observed_indices)\n",
    "        \n",
    "        for fold in range(min(n_folds, n_observed // fold_size)):\n",
    "            # Select test indices for this fold\n",
    "            start_idx = fold * fold_size\n",
    "            end_idx = min(start_idx + fold_size, n_observed)\n",
    "            test_indices = shuffled_indices[start_idx:end_idx]\n",
    "            \n",
    "            print(f\"\\nFold {fold+1}/{min(n_folds, n_observed // fold_size)} - Testing on {len(test_indices)} samples\")\n",
    "            \n",
    "            # Create a copy of the original dataframe\n",
    "            df_fold = df.copy()\n",
    "            \n",
    "            # Store original values from test set\n",
    "            original_values = df_fold.loc[test_indices, col].copy()\n",
    "            \n",
    "            # Set test values to NaN (simulating missingness)\n",
    "            df_fold.loc[test_indices, col] = np.nan\n",
    "            \n",
    "            # For each imputation method\n",
    "            for method_name, method_func in methods.items():\n",
    "                print(f\"Running {method_name}...\")\n",
    "                \n",
    "                try:\n",
    "                    # Create a method-specific copy of the dataframe\n",
    "                    method_df = df_fold.copy()\n",
    "                    \n",
    "                    # For methods other than 'Da XU DL', drop the qol_date column\n",
    "                    if method_name != 'Da XU DL' and 'qol_date' in method_df.columns:\n",
    "                        method_df = method_df.drop(columns=['qol_date'])\n",
    "                    \n",
    "                    # Time the imputation\n",
    "                    start_time = time.time()\n",
    "                    \n",
    "                    # Apply imputation - use only valid_columns for imputation\n",
    "                    imputed_df, _ = method_func(method_df.copy(), valid_columns)\n",
    "                    \n",
    "                    end_time = time.time()\n",
    "                    execution_time = end_time - start_time\n",
    "                    \n",
    "                    # Get imputed values for the test indices\n",
    "                    if col not in imputed_df.columns:\n",
    "                        print(f\"Warning: Column {col} not found in {method_name} results. Skipping.\")\n",
    "                        continue\n",
    "                        \n",
    "                    imputed_values = imputed_df.loc[test_indices, col]\n",
    "                    \n",
    "                    # Check for any still-missing values\n",
    "                    still_missing = imputed_values.isna().sum()\n",
    "                    if still_missing > 0:\n",
    "                        print(f\"Warning: {method_name} failed to impute {still_missing}/{len(test_indices)} values\")\n",
    "                        # Use only successfully imputed values for evaluation\n",
    "                        valid_indices = [idx for idx in test_indices if not pd.isna(imputed_df.loc[idx, col])]\n",
    "                        \n",
    "                        if not valid_indices:\n",
    "                            print(f\"No valid imputations to evaluate for {method_name}\")\n",
    "                            continue\n",
    "                            \n",
    "                        original_values_filtered = df.loc[valid_indices, col]\n",
    "                        imputed_values_filtered = imputed_df.loc[valid_indices, col]\n",
    "                    else:\n",
    "                        original_values_filtered = original_values\n",
    "                        imputed_values_filtered = imputed_values\n",
    "                    \n",
    "                    # Calculate metrics\n",
    "                    mae = mean_absolute_error(original_values_filtered, imputed_values_filtered)\n",
    "                    rmse = np.sqrt(mean_squared_error(original_values_filtered, imputed_values_filtered))\n",
    "                    \n",
    "                    # Store results\n",
    "                    results[method_name]['mae'].append(mae)\n",
    "                    results[method_name]['rmse'].append(rmse)\n",
    "                    results[method_name]['time'].append(execution_time)\n",
    "                    \n",
    "                    print(f\"{method_name} - MAE: {mae:.4f}, RMSE: {rmse:.4f}, Time: {execution_time:.2f}s\")\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"Error with {method_name}: {e}\")\n",
    "                    import traceback\n",
    "                    traceback.print_exc()  # Print full traceback for better debugging\n",
    "\n",
    "    # Calculate average results across all columns and folds\n",
    "    for method in methods:\n",
    "        if results[method]['mae']:\n",
    "            results[method]['avg_mae'] = np.mean(results[method]['mae'])\n",
    "            results[method]['std_mae'] = np.std(results[method]['mae'])\n",
    "            results[method]['avg_rmse'] = np.mean(results[method]['rmse'])\n",
    "            results[method]['std_rmse'] = np.std(results[method]['rmse'])\n",
    "            results[method]['avg_time'] = np.mean(results[method]['time'])\n",
    "            results[method]['std_time'] = np.std(results[method]['time'])\n",
    "        else:\n",
    "            print(f\"No valid results for {method}\")\n",
    "            results[method]['avg_mae'] = np.nan\n",
    "            results[method]['std_mae'] = np.nan\n",
    "            results[method]['avg_rmse'] = np.nan\n",
    "            results[method]['std_rmse'] = np.nan\n",
    "            results[method]['avg_time'] = np.nan\n",
    "            results[method]['std_time'] = np.nan\n",
    "\n",
    "    return results\n",
    "def plot_execution_times(execution_times):\n",
    "    \"\"\"\n",
    "    Plot execution times for each imputation method\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    execution_times : dict\n",
    "        Dictionary with execution times for each method\n",
    "    \"\"\"\n",
    "    methods = list(execution_times.keys())\n",
    "    times = list(execution_times.values())\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    bars = plt.bar(methods, times, alpha=0.7, color='green')\n",
    "    \n",
    "    plt.title('Execution Time by Imputation Method', fontsize=14)\n",
    "    plt.ylabel('Time (seconds)', fontsize=12)\n",
    "    plt.xlabel('Method', fontsize=12)\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    \n",
    "    # Add values on top of bars\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        plt.text(bar.get_x() + bar.get_width()/2., height + 0.02 * max(times),\n",
    "                f'{height:.2f}s', ha='center', va='bottom', fontsize=10)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('imputation_execution_times.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "def plot_distribution_similarity(distribution_similarity):\n",
    "    \"\"\"\n",
    "    Plot KS statistics for distribution similarity\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    distribution_similarity : dict\n",
    "        Dictionary with KS test results\n",
    "    \"\"\"\n",
    "    # Prepare data for plotting\n",
    "    methods = list(distribution_similarity.keys())\n",
    "    columns = list(distribution_similarity[methods[0]].keys()) if methods else []\n",
    "    \n",
    "    if not methods or not columns:\n",
    "        print(\"No distribution similarity data to plot\")\n",
    "        return\n",
    "    \n",
    "    # Create a DataFrame for easier plotting\n",
    "    data = []\n",
    "    for method in methods:\n",
    "        for col in columns:\n",
    "            if col in distribution_similarity[method]:\n",
    "                data.append({\n",
    "                    'Method': method,\n",
    "                    'Column': col,\n",
    "                    'KS Statistic': distribution_similarity[method][col]['ks_stat'],\n",
    "                    'p-value': distribution_similarity[method][col]['ks_pval']\n",
    "                })\n",
    "    \n",
    "    if not data:\n",
    "        print(\"No valid distribution similarity data to plot\")\n",
    "        return\n",
    "    \n",
    "    df_plot = pd.DataFrame(data)\n",
    "    \n",
    "    # Plot KS statistics (lower is better - more similar distributions)\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    chart = sns.barplot(x='Method', y='KS Statistic', hue='Column', data=df_plot)\n",
    "    \n",
    "    plt.title('Distribution Similarity by Method (Lower KS = More Similar)', fontsize=14)\n",
    "    plt.ylabel('Kolmogorov-Smirnov Statistic', fontsize=12)\n",
    "    plt.xlabel('Method', fontsize=12)\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    plt.legend(title='Column', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('distribution_similarity.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    # Plot p-values (higher is better - can't reject null hypothesis of same distribution)\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    chart = sns.barplot(x='Method', y='p-value', hue='Column', data=df_plot)\n",
    "    \n",
    "    plt.title('Distribution Similarity p-values by Method (Higher = More Similar)', fontsize=14)\n",
    "    plt.ylabel('p-value', fontsize=12)\n",
    "    plt.xlabel('Method', fontsize=12)\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    plt.legend(title='Column', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('distribution_pvalues.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "def plot_imputation_histograms(original_df, imputed_dfs, columns_to_impute):\n",
    "    \"\"\"\n",
    "    Plot histograms of imputed values compared to original observed values\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    original_df : pandas.DataFrame\n",
    "        Original data with missing values\n",
    "    imputed_dfs : dict\n",
    "        Dictionary of imputed DataFrames from different methods\n",
    "    columns_to_impute : list\n",
    "        List of column names to impute\n",
    "    \"\"\"\n",
    "    # Filter to only include columns that exist in the original dataframe\n",
    "    valid_columns = [col for col in columns_to_impute if col in original_df.columns]\n",
    "    \n",
    "    if not valid_columns:\n",
    "        print(\"No valid columns found for plotting histograms\")\n",
    "        return\n",
    "        \n",
    "    print(f\"Plotting histograms for columns: {valid_columns}\")\n",
    "    \n",
    "    # Number of columns and methods\n",
    "    n_cols = len(valid_columns)\n",
    "    n_methods = len(imputed_dfs)\n",
    "    \n",
    "    # Create figure with subplots\n",
    "    fig, axes = plt.subplots(n_cols, 1, figsize=(12, 4 * n_cols))\n",
    "    if n_cols == 1:\n",
    "        axes = [axes]\n",
    "    \n",
    "    # Plot for each column\n",
    "    for i, col in enumerate(valid_columns):\n",
    "        ax = axes[i]\n",
    "        \n",
    "        # Plot original observed distribution\n",
    "        observed_values = original_df[col].dropna()\n",
    "        n_observed = len(observed_values)\n",
    "        \n",
    "        sns.histplot(observed_values, ax=ax, \n",
    "                    label=f'Original (observed, n={n_observed})', \n",
    "                    alpha=0.5, color='black', kde=True)\n",
    "        \n",
    "        # Plot imputed distributions (only for previously missing values)\n",
    "        colors = plt.cm.tab10.colors\n",
    "        for j, (method_name, imputed_df) in enumerate(imputed_dfs.items()):\n",
    "            # Make sure the column exists in the imputed dataframe\n",
    "            if col not in imputed_df.columns:\n",
    "                print(f\"Warning: Column {col} not found in {method_name} results\")\n",
    "                continue\n",
    "                \n",
    "            # Get just the imputed values (where original was missing)\n",
    "            missing_mask = original_df[col].isna()\n",
    "            imputed_values = imputed_df.loc[missing_mask, col].dropna()\n",
    "            n_imputed = len(imputed_values)\n",
    "            \n",
    "            if not imputed_values.empty:\n",
    "                color = colors[j % len(colors)]\n",
    "                sns.histplot(imputed_values, ax=ax, \n",
    "                            label=f'{method_name} (imputed, n={n_imputed})', \n",
    "                            alpha=0.5, color=color, kde=True)\n",
    "        \n",
    "        # Add distribution statistics\n",
    "        if not observed_values.empty:\n",
    "            obs_mean = observed_values.mean()\n",
    "            obs_std = observed_values.std()\n",
    "            ax.axvline(obs_mean, color='black', linestyle='--', alpha=0.7)\n",
    "            textstr = f'Observed: Î¼={obs_mean:.2f}, Ïƒ={obs_std:.2f}'\n",
    "            props = dict(boxstyle='round', facecolor='white', alpha=0.5)\n",
    "            ax.text(0.05, 0.95, textstr, transform=ax.transAxes, fontsize=10,\n",
    "                    verticalalignment='top', bbox=props)\n",
    "        \n",
    "        ax.set_title(f'Distribution for {col}', fontsize=14)\n",
    "        ax.legend()\n",
    "        ax.grid(alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('imputation_histograms.png', dpi=300, bbox_inches='tight')\n",
    "    plt.close()  # Close the plot to free memory\n",
    "    print(\"Histogram plot saved to 'imputation_histograms.png'\")\n",
    "\n",
    "def plot_correlation_preservation(original_df, imputed_dfs, columns_to_impute):\n",
    "    \"\"\"\n",
    "    Plot heatmaps showing how well each method preserves correlations\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    original_df : pandas.DataFrame\n",
    "        Original data with missing values\n",
    "    imputed_dfs : dict\n",
    "        Dictionary of imputed DataFrames from different methods\n",
    "    columns_to_impute : list\n",
    "        List of column names to impute\n",
    "    \"\"\"\n",
    "    # Filter to only include columns that exist in all dataframes\n",
    "    valid_columns = []\n",
    "    for col in columns_to_impute:\n",
    "        if col in original_df.columns:\n",
    "            all_valid = True\n",
    "            for method_name, imputed_df in imputed_dfs.items():\n",
    "                if col not in imputed_df.columns:\n",
    "                    all_valid = False\n",
    "                    break\n",
    "            if all_valid:\n",
    "                valid_columns.append(col)\n",
    "    \n",
    "    if not valid_columns:\n",
    "        print(\"No valid columns found for correlation analysis\")\n",
    "        return\n",
    "        \n",
    "    print(f\"Analyzing correlations for columns: {valid_columns}\")\n",
    "    \n",
    "    # Calculate original correlations (using only complete cases)\n",
    "    # This is important for sparse data - we need a baseline for comparison\n",
    "    complete_cases = original_df[valid_columns].dropna()\n",
    "    \n",
    "    if len(complete_cases) < 2:\n",
    "        print(\"Not enough complete cases to calculate original correlations\")\n",
    "        # Use pairwise correlations instead\n",
    "        original_corr = original_df[valid_columns].corr(method='pearson')\n",
    "    else:\n",
    "        original_corr = complete_cases.corr()\n",
    "    \n",
    "    # Set up the figure\n",
    "    n_methods = len(imputed_dfs)\n",
    "    fig, axes = plt.subplots(1, n_methods + 1, figsize=(5 * (n_methods + 1), 4))\n",
    "    \n",
    "    if n_methods == 0:\n",
    "        print(\"No imputed dataframes to plot correlations\")\n",
    "        return\n",
    "    \n",
    "    # Make axes iterable if only one method\n",
    "    if n_methods == 1:\n",
    "        axes = [axes[0], axes[1]]\n",
    "    \n",
    "    # Plot original correlation\n",
    "    sns.heatmap(original_corr, annot=True, cmap='coolwarm', vmin=-1, vmax=1, ax=axes[0])\n",
    "    axes[0].set_title('Original Correlations', fontsize=14)\n",
    "    \n",
    "    # Plot correlations for each imputation method\n",
    "    for i, (method_name, imputed_df) in enumerate(imputed_dfs.items()):\n",
    "        imputed_corr = imputed_df[valid_columns].corr()\n",
    "        sns.heatmap(imputed_corr, annot=True, cmap='coolwarm', vmin=-1, vmax=1, ax=axes[i+1])\n",
    "        axes[i+1].set_title(f'{method_name} Correlations', fontsize=14)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('correlation_preservation.png', dpi=300, bbox_inches='tight')\n",
    "    plt.close()  # Close the plot to free memory\n",
    "    print(\"Correlation plot saved to 'correlation_preservation.png'\")\n",
    "    \n",
    "    # Calculate and plot correlation differences\n",
    "    print(\"Calculating correlation differences...\")\n",
    "    correlation_diffs = {}\n",
    "    \n",
    "    for method_name, imputed_df in imputed_dfs.items():\n",
    "        imputed_corr = imputed_df[valid_columns].corr()\n",
    "        # Calculate absolute differences between original and imputed correlations\n",
    "        diff_matrix = np.abs(original_corr - imputed_corr)\n",
    "        correlation_diffs[method_name] = diff_matrix\n",
    "    \n",
    "    # Plot correlation differences (lower is better - less difference from original)\n",
    "    fig, axes = plt.subplots(1, n_methods, figsize=(5 * n_methods, 4))\n",
    "    \n",
    "    if n_methods == 1:\n",
    "        axes = [axes]\n",
    "    \n",
    "    for i, (method_name, diff_matrix) in enumerate(correlation_diffs.items()):\n",
    "        sns.heatmap(diff_matrix, annot=True, cmap='YlOrRd', vmin=0, vmax=1, ax=axes[i])\n",
    "        axes[i].set_title(f'{method_name} Correlation Differences', fontsize=14)\n",
    "        \n",
    "        # Calculate and display average difference\n",
    "        avg_diff = np.mean(diff_matrix.values)\n",
    "        axes[i].text(0.5, -0.1, f'Avg Diff: {avg_diff:.4f}', \n",
    "                    horizontalalignment='center', transform=axes[i].transAxes)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('correlation_differences.png', dpi=300, bbox_inches='tight')\n",
    "    plt.close()  # Close the plot to free memory\n",
    "    print(\"Correlation difference plot saved to 'correlation_differences.png'\")\n",
    "\n",
    "def plot_validation_results(validation_results):\n",
    "    \"\"\"\n",
    "    Plot validation results\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    validation_results : dict\n",
    "        Dictionary with validation results\n",
    "    \"\"\"\n",
    "    # Prepare data for plotting\n",
    "    methods = [m for m in validation_results.keys() \n",
    "              if 'avg_mae' in validation_results[m] and not np.isnan(validation_results[m]['avg_mae'])]\n",
    "    \n",
    "    if not methods:\n",
    "        print(\"No valid validation results to plot\")\n",
    "        return\n",
    "    \n",
    "    mae_means = [validation_results[m]['avg_mae'] for m in methods]\n",
    "    mae_stds = [validation_results[m]['std_mae'] for m in methods]\n",
    "    rmse_means = [validation_results[m]['avg_rmse'] for m in methods]\n",
    "    rmse_stds = [validation_results[m]['std_rmse'] for m in methods]\n",
    "    time_means = [validation_results[m]['avg_time'] for m in methods]\n",
    "    time_stds = [validation_results[m]['std_time'] for m in methods]\n",
    "    \n",
    "    # Create figure with three subplots\n",
    "    fig, axes = plt.subplots(3, 1, figsize=(12, 15))\n",
    "    \n",
    "    # Plot MAE\n",
    "    bars1 = axes[0].bar(methods, mae_means, yerr=mae_stds, capsize=5, alpha=0.7)\n",
    "    axes[0].set_title('Mean Absolute Error (MAE) by Method', fontsize=14)\n",
    "    axes[0].set_ylabel('MAE', fontsize=12)\n",
    "    axes[0].set_xlabel('Imputation Method', fontsize=12)\n",
    "    axes[0].grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    \n",
    "    # Add values on top of bars\n",
    "    for bar in bars1:\n",
    "        height = bar.get_height()\n",
    "        axes[0].text(bar.get_x() + bar.get_width()/2., height + 0.02 * max(mae_means),\n",
    "                f'{height:.4f}', ha='center', va='bottom', fontsize=10)\n",
    "    \n",
    "    # Plot RMSE\n",
    "    bars2 = axes[1].bar(methods, rmse_means, yerr=rmse_stds, capsize=5, alpha=0.7, color='orange')\n",
    "    axes[1].set_title('Root Mean Squared Error (RMSE) by Method', fontsize=14)\n",
    "    axes[1].set_ylabel('RMSE', fontsize=12)\n",
    "    axes[1].set_xlabel('Imputation Method', fontsize=12)\n",
    "    axes[1].grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    \n",
    "    # Add values on top of bars\n",
    "    for bar in bars2:\n",
    "        height = bar.get_height()\n",
    "        axes[1].text(bar.get_x() + bar.get_width()/2., height + 0.02 * max(rmse_means),\n",
    "                f'{height:.4f}', ha='center', va='bottom', fontsize=10)\n",
    "    \n",
    "    # Plot execution time\n",
    "    bars3 = axes[2].bar(methods, time_means, yerr=time_stds, capsize=5, alpha=0.7, color='green')\n",
    "    axes[2].set_title('Execution Time by Method', fontsize=14)\n",
    "    axes[2].set_ylabel('Time (seconds)', fontsize=12)\n",
    "    axes[2].set_xlabel('Imputation Method', fontsize=12)\n",
    "    axes[2].grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    \n",
    "    # Add values on top of bars\n",
    "    for bar in bars3:\n",
    "        height = bar.get_height()\n",
    "        axes[2].text(bar.get_x() + bar.get_width()/2., height + 0.02 * max(time_means),\n",
    "                f'{height:.2f}s', ha='center', va='bottom', fontsize=10)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('validation_results.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "def create_summary_dataframe(imputed_dfs, validation_results, distribution_similarity, execution_times):\n",
    "    \"\"\"\n",
    "    Create a summary dataframe of all results\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    imputed_dfs : dict\n",
    "        Dictionary of imputed DataFrames\n",
    "    validation_results : dict\n",
    "        Dictionary with validation results\n",
    "    distribution_similarity : dict\n",
    "        Dictionary with distribution similarity results\n",
    "    execution_times : dict\n",
    "        Dictionary with execution times\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    summary_df : pandas.DataFrame\n",
    "        DataFrame with summary of all results\n",
    "    \"\"\"\n",
    "    # Get list of methods\n",
    "    methods = list(imputed_dfs.keys())\n",
    "    \n",
    "    # Initialize summary data\n",
    "    summary_data = []\n",
    "    \n",
    "    for method in methods:\n",
    "        method_summary = {'Method': method}\n",
    "        \n",
    "        # Add validation metrics if available\n",
    "        if method in validation_results and 'avg_mae' in validation_results[method]:\n",
    "            method_summary['MAE'] = validation_results[method]['avg_mae']\n",
    "            method_summary['RMSE'] = validation_results[method]['avg_rmse']\n",
    "        else:\n",
    "            method_summary['MAE'] = np.nan\n",
    "            method_summary['RMSE'] = np.nan\n",
    "        \n",
    "        # Add execution time\n",
    "        if method in execution_times:\n",
    "            method_summary['Time (s)'] = execution_times[method]\n",
    "        else:\n",
    "            method_summary['Time (s)'] = np.nan\n",
    "        \n",
    "        # Add average distribution similarity metrics if available\n",
    "        if method in distribution_similarity:\n",
    "            ks_stats = []\n",
    "            ks_pvals = []\n",
    "            \n",
    "            for col, results in distribution_similarity[method].items():\n",
    "                if 'ks_stat' in results:\n",
    "                    ks_stats.append(results['ks_stat'])\n",
    "                if 'ks_pval' in results:\n",
    "                    ks_pvals.append(results['ks_pval'])\n",
    "            \n",
    "            if ks_stats:\n",
    "                method_summary['Avg KS Stat'] = np.mean(ks_stats)\n",
    "            else:\n",
    "                method_summary['Avg KS Stat'] = np.nan\n",
    "                \n",
    "            if ks_pvals:\n",
    "                method_summary['Avg KS p-value'] = np.mean(ks_pvals)\n",
    "            else:\n",
    "                method_summary['Avg KS p-value'] = np.nan\n",
    "        else:\n",
    "            method_summary['Avg KS Stat'] = np.nan\n",
    "            method_summary['Avg KS p-value'] = np.nan\n",
    "        \n",
    "        summary_data.append(method_summary)\n",
    "    \n",
    "    # Create DataFrame\n",
    "    summary_df = pd.DataFrame(summary_data)\n",
    "    \n",
    "    # Add ranks for each metric (1 is best)\n",
    "    for metric in ['MAE', 'RMSE', 'Time (s)', 'Avg KS Stat']:\n",
    "        if metric == 'Avg KS Stat' or metric == 'Time (s)':\n",
    "            # Lower is better for these metrics\n",
    "            summary_df[f'{metric} Rank'] = summary_df[metric].rank()\n",
    "        else:\n",
    "            # Higher is better for p-value\n",
    "            summary_df[f'{metric} Rank'] = summary_df[metric].rank(ascending=False)\n",
    "    \n",
    "    # Add average rank\n",
    "    rank_columns = [col for col in summary_df.columns if 'Rank' in col]\n",
    "    if rank_columns:\n",
    "        summary_df['Average Rank'] = summary_df[rank_columns].mean(axis=1)\n",
    "        summary_df = summary_df.sort_values('Average Rank')\n",
    "    \n",
    "    return summary_df\n",
    "\n",
    "def main_real_data(df, columns_to_impute=['ge1', 'ge2', 'ge3', 'ge4', 'ge5', 'ge6']):\n",
    "    \"\"\"\n",
    "    Main function to run imputation comparison on real data with high missingness\n",
    "    \n",
    "        Parameters:\n",
    "    -----------\n",
    "    df : pandas.DataFrame\n",
    "        Your real dataset with missing values\n",
    "    columns_to_impute : list\n",
    "        List of column names to impute (default: ge1-ge6)\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    imputed_dfs : dict\n",
    "        Dictionary of imputed DataFrames from each method\n",
    "    summary_df : pandas.DataFrame\n",
    "        DataFrame with summary of all results\n",
    "    \"\"\"\n",
    "    print(\"===== Analyzing Dataset =====\")\n",
    "    print(f\"Dataset shape: {df.shape}\")\n",
    "\n",
    "    # Filter out any derived date columns from columns_to_impute\n",
    "    valid_columns = [col for col in columns_to_impute if col in df.columns and not col.startswith('qol_date_')]\n",
    "    if len(valid_columns) != len(columns_to_impute):\n",
    "        excluded_cols = set(columns_to_impute) - set(valid_columns)\n",
    "        print(f\"Warning: Excluding {len(excluded_cols)} columns that are not in the dataframe or are derived date columns:\")\n",
    "        print(f\"  {sorted(excluded_cols)}\")\n",
    "        print(f\"Using valid columns: {sorted(valid_columns)}\")\n",
    "        columns_to_impute = valid_columns\n",
    "\n",
    "    # Show basic statistics of the dataset\n",
    "    print(\"\\nMissing value counts:\")\n",
    "    missing_counts = df[columns_to_impute].isna().sum()\n",
    "    missing_percents = (missing_counts / len(df)) * 100\n",
    "    for col, count, percent in zip(columns_to_impute, missing_counts, missing_percents):\n",
    "        print(f\"{col}: {count} missing values ({percent:.2f}%)\")\n",
    "\n",
    "    # Print observed values statistics\n",
    "    print(\"\\nObserved values per column:\")\n",
    "    for i, col in enumerate(columns_to_impute):\n",
    "        observed = df[col].dropna()\n",
    "        n_observed = len(observed)\n",
    "        if n_observed > 0:\n",
    "            print(f\"{col}: {n_observed} observed values ({100-missing_percents[i]:.2f}%)\")\n",
    "            print(f\"  Range: {observed.min()} to {observed.max()}\")\n",
    "            print(f\"  Mean: {observed.mean():.4f}, Std: {observed.std():.4f}\")\n",
    "            print(f\"  Unique values: {observed.nunique()} ({observed.nunique()/n_observed*100:.1f}% unique)\")\n",
    "        else:\n",
    "            print(f\"{col}: No observed values\")\n",
    "\n",
    "    print(\"\\n===== Running imputation on real data =====\")\n",
    "    imputed_dfs, original_df, execution_times, distribution_similarity = evaluate_real_data_imputation(\n",
    "        df, columns_to_impute\n",
    "    )\n",
    "\n",
    "    # Create a version of original_df without qol_date for plotting\n",
    "    plotting_df = original_df.copy()\n",
    "    if 'qol_date' in plotting_df.columns:\n",
    "        print(\"\\nRemoving qol_date for plotting functions\")\n",
    "        plotting_df = plotting_df.drop(columns=['qol_date'])\n",
    "\n",
    "    # Also remove any derived date columns if they exist\n",
    "    date_derived_cols = [col for col in plotting_df.columns if col.startswith('qol_date_')]\n",
    "    if date_derived_cols:\n",
    "        print(f\"Removing derived date columns: {date_derived_cols}\")\n",
    "        plotting_df = plotting_df.drop(columns=date_derived_cols)\n",
    "\n",
    "    # Plot execution times\n",
    "    print(\"\\nPlotting execution times...\")\n",
    "    plot_execution_times(execution_times)\n",
    "\n",
    "    # Plot distribution similarity\n",
    "    print(\"\\nPlotting distribution similarity...\")\n",
    "    plot_distribution_similarity(distribution_similarity)\n",
    "\n",
    "    # Plot histograms of imputed values\n",
    "    print(\"\\nPlotting imputation histograms...\")\n",
    "    plot_imputation_histograms(plotting_df, imputed_dfs, columns_to_impute)\n",
    "\n",
    "    # Plot correlation preservation\n",
    "    print(\"\\nPlotting correlation preservation...\")\n",
    "    plot_correlation_preservation(plotting_df, imputed_dfs, columns_to_impute)\n",
    "\n",
    "    print(\"\\n===== Running sparse validation =====\")\n",
    "    validation_results = evaluate_with_sparse_validation(df, columns_to_impute, n_folds=3)\n",
    "\n",
    "    # Plot validation results\n",
    "    print(\"\\nPlotting validation results...\")\n",
    "    plot_validation_results(validation_results)\n",
    "\n",
    "    # Create summary dataframe\n",
    "    print(\"\\nCreating summary of results...\")\n",
    "    summary_df = create_summary_dataframe(\n",
    "        imputed_dfs, \n",
    "        validation_results, \n",
    "        distribution_similarity, \n",
    "        execution_times\n",
    "    )\n",
    "\n",
    "    # Print summary of results\n",
    "    print(\"\\n===== Summary of Results =====\")\n",
    "    print(summary_df.to_string())\n",
    "\n",
    "    # Save summary to CSV\n",
    "    summary_df.to_csv('imputation_summary.csv', index=False)\n",
    "\n",
    "    # Print recommended method based on average rank\n",
    "    if 'Average Rank' in summary_df.columns and not summary_df.empty:\n",
    "        best_method = summary_df.iloc[0]['Method']\n",
    "        print(f\"\\nRecommended imputation method: {best_method}\")\n",
    "\n",
    "    return imputed_dfs, summary_df\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Load your real dataset with missing values here\n",
    "    # df = pd.read_csv('your_dataset.csv')\n",
    "    \n",
    "    # Define columns to impute\n",
    "    columns_to_impute = ['ge1', 'ge2', 'ge3', 'ge4', 'ge5', 'ge6']\n",
    "    \n",
    "    # Run the analysis\n",
    "    imputed_dfs, summary_df = main_real_data(df, columns_to_impute)\n",
    "    \n",
    "    # Example of how to save the best imputed dataset\n",
    "    if not summary_df.empty:\n",
    "        worst_method = summary_df.iloc[0]['Method']\n",
    "        print(f\"\\nSaving imputed dataset from {worst_method}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'MICE':          id   redcap_event_name  age_diagnosis gender overall_primary_tumour  \\\n",
       " 0         1      baseline_arm_1            NaN    1.0                    nan   \n",
       " 1         1                 nan            NaN    NaN                    nan   \n",
       " 2         1                 nan            NaN    NaN                    nan   \n",
       " 3         1                 nan            NaN    NaN                    nan   \n",
       " 4         1                 nan            NaN    NaN                    nan   \n",
       " ...     ...                 ...            ...    ...                    ...   \n",
       " 18182  1770  preoperative_arm_1            NaN    NaN                    nan   \n",
       " 18183  1770  preoperative_arm_1            NaN    NaN                    nan   \n",
       " 18184  1770  preoperative_arm_1            NaN    NaN                    nan   \n",
       " 18185  1770      baseline_arm_1            NaN    1.0                    nan   \n",
       " 18186  1770       surgery_arm_1            NaN    NaN                    nan   \n",
       " \n",
       "       overall_regional_ln overall_distant_metastasis neotx___notx  \\\n",
       " 0                     nan                        nan          NaN   \n",
       " 1                     nan                        nan          NaN   \n",
       " 2                     nan                        nan          NaN   \n",
       " 3                     nan                        nan          NaN   \n",
       " 4                     nan                        nan          NaN   \n",
       " ...                   ...                        ...          ...   \n",
       " 18182                 nan                        nan          0.0   \n",
       " 18183                 nan                        nan          NaN   \n",
       " 18184                 nan                        nan          NaN   \n",
       " 18185                 nan                        nan          NaN   \n",
       " 18186                 nan                        nan          NaN   \n",
       " \n",
       "       neotx___chemo neotx___rads  ... a_e4 a_e5 a_e6 a_e7 a_c6 a_c2 a_act11  \\\n",
       " 0               NaN          NaN  ...  NaN  NaN  NaN  NaN  NaN  NaN     NaN   \n",
       " 1               NaN          NaN  ...  NaN  NaN  NaN  NaN  NaN  NaN     NaN   \n",
       " 2               NaN          NaN  ...  NaN  NaN  NaN  NaN  NaN  NaN     NaN   \n",
       " 3               NaN          NaN  ...  NaN  NaN  NaN  NaN  NaN  NaN     NaN   \n",
       " 4               NaN          NaN  ...  NaN  NaN  NaN  NaN  NaN  NaN     NaN   \n",
       " ...             ...          ...  ...  ...  ...  ...  ...  ...  ...     ...   \n",
       " 18182           0.0          0.0  ...  NaN  NaN  NaN  NaN  NaN  NaN     NaN   \n",
       " 18183           NaN          NaN  ...  NaN  NaN  NaN  NaN  NaN  NaN     NaN   \n",
       " 18184           NaN          NaN  ...  0.0  0.0  0.0  0.0  1.0  3.0     3.0   \n",
       " 18185           NaN          NaN  ...  NaN  NaN  NaN  NaN  NaN  NaN     NaN   \n",
       " 18186           NaN          NaN  ...  NaN  NaN  NaN  NaN  NaN  NaN     NaN   \n",
       " \n",
       "       readmission_30d  postop_comp   los  \n",
       " 0                 NaN          0.0   NaN  \n",
       " 1                 NaN          NaN   NaN  \n",
       " 2                 NaN          NaN  16.0  \n",
       " 3                 NaN          NaN   8.0  \n",
       " 4                 NaN          NaN  13.0  \n",
       " ...               ...          ...   ...  \n",
       " 18182             NaN          NaN   NaN  \n",
       " 18183             NaN          NaN   NaN  \n",
       " 18184             NaN          NaN   NaN  \n",
       " 18185             NaN          NaN   NaN  \n",
       " 18186             NaN          NaN   NaN  \n",
       " \n",
       " [18187 rows x 65 columns],\n",
       " 'GAIN':          id redcap_event_name  age_diagnosis gender overall_primary_tumour  \\\n",
       " 0         1                 8            NaN    1.0                    nan   \n",
       " 1         1                 9            NaN    NaN                    nan   \n",
       " 2         1                 9            NaN    NaN                    nan   \n",
       " 3         1                 9            NaN    NaN                    nan   \n",
       " 4         1                 9            NaN    NaN                    nan   \n",
       " ...     ...               ...            ...    ...                    ...   \n",
       " 18182  1770                10            NaN    NaN                    nan   \n",
       " 18183  1770                10            NaN    NaN                    nan   \n",
       " 18184  1770                10            NaN    NaN                    nan   \n",
       " 18185  1770                 8            NaN    1.0                    nan   \n",
       " 18186  1770                11            NaN    NaN                    nan   \n",
       " \n",
       "       overall_regional_ln overall_distant_metastasis neotx___notx  \\\n",
       " 0                     nan                        nan          NaN   \n",
       " 1                     nan                        nan          NaN   \n",
       " 2                     nan                        nan          NaN   \n",
       " 3                     nan                        nan          NaN   \n",
       " 4                     nan                        nan          NaN   \n",
       " ...                   ...                        ...          ...   \n",
       " 18182                 nan                        nan          0.0   \n",
       " 18183                 nan                        nan          NaN   \n",
       " 18184                 nan                        nan          NaN   \n",
       " 18185                 nan                        nan          NaN   \n",
       " 18186                 nan                        nan          NaN   \n",
       " \n",
       "       neotx___chemo neotx___rads  ... a_e4 a_e5 a_e6 a_e7 a_c6 a_c2 a_act11  \\\n",
       " 0               NaN          NaN  ...  NaN  NaN  NaN  NaN  NaN  NaN     NaN   \n",
       " 1               NaN          NaN  ...  NaN  NaN  NaN  NaN  NaN  NaN     NaN   \n",
       " 2               NaN          NaN  ...  NaN  NaN  NaN  NaN  NaN  NaN     NaN   \n",
       " 3               NaN          NaN  ...  NaN  NaN  NaN  NaN  NaN  NaN     NaN   \n",
       " 4               NaN          NaN  ...  NaN  NaN  NaN  NaN  NaN  NaN     NaN   \n",
       " ...             ...          ...  ...  ...  ...  ...  ...  ...  ...     ...   \n",
       " 18182           0.0          0.0  ...  NaN  NaN  NaN  NaN  NaN  NaN     NaN   \n",
       " 18183           NaN          NaN  ...  NaN  NaN  NaN  NaN  NaN  NaN     NaN   \n",
       " 18184           NaN          NaN  ...  0.0  0.0  0.0  0.0  1.0  3.0     3.0   \n",
       " 18185           NaN          NaN  ...  NaN  NaN  NaN  NaN  NaN  NaN     NaN   \n",
       " 18186           NaN          NaN  ...  NaN  NaN  NaN  NaN  NaN  NaN     NaN   \n",
       " \n",
       "       readmission_30d  postop_comp   los  \n",
       " 0                 NaN          0.0   NaN  \n",
       " 1                 NaN          NaN   NaN  \n",
       " 2                 NaN          NaN  16.0  \n",
       " 3                 NaN          NaN   8.0  \n",
       " 4                 NaN          NaN  13.0  \n",
       " ...               ...          ...   ...  \n",
       " 18182             NaN          NaN   NaN  \n",
       " 18183             NaN          NaN   NaN  \n",
       " 18184             NaN          NaN   NaN  \n",
       " 18185             NaN          NaN   NaN  \n",
       " 18186             NaN          NaN   NaN  \n",
       " \n",
       " [18187 rows x 65 columns],\n",
       " 'Original GAIN':            id  redcap_event_name  age_diagnosis    gender  \\\n",
       " 0         1.0                8.0      94.920326  1.000000   \n",
       " 1         1.0                9.0      94.888451  1.000066   \n",
       " 2         1.0                9.0      94.937500  1.000061   \n",
       " 3         1.0                9.0      94.937004  1.000062   \n",
       " 4         1.0                9.0      94.937317  1.000061   \n",
       " ...       ...                ...            ...       ...   \n",
       " 18182  1770.0               10.0      94.978004  1.000018   \n",
       " 18183  1770.0               10.0      94.862984  1.000049   \n",
       " 18184  1770.0               10.0      94.784492  1.005744   \n",
       " 18185  1770.0                8.0      94.898445  1.000000   \n",
       " 18186  1770.0               11.0      94.924126  1.000023   \n",
       " \n",
       "       overall_primary_tumour overall_regional_ln overall_distant_metastasis  \\\n",
       " 0                        nan                 nan                        nan   \n",
       " 1                        nan                 nan                        nan   \n",
       " 2                        nan                 nan                        nan   \n",
       " 3                        nan                 nan                        nan   \n",
       " 4                        nan                 nan                        nan   \n",
       " ...                      ...                 ...                        ...   \n",
       " 18182                    nan                 nan                        nan   \n",
       " 18183                    nan                 nan                        nan   \n",
       " 18184                    nan                 nan                        nan   \n",
       " 18185                    nan                 nan                        nan   \n",
       " 18186                    nan                 nan                        nan   \n",
       " \n",
       "        neotx___notx  neotx___chemo  neotx___rads  ...      a_e4      a_e5  \\\n",
       " 0          0.999939       0.999503      0.999924  ...  0.009684  0.015438   \n",
       " 1          0.999930       0.999568      0.999907  ...  0.009407  0.016509   \n",
       " 2          0.999937       0.999585      0.999932  ...  0.010012  0.014660   \n",
       " 3          0.999938       0.999584      0.999932  ...  0.010027  0.014629   \n",
       " 4          0.999937       0.999585      0.999932  ...  0.010018  0.014648   \n",
       " ...             ...            ...           ...  ...       ...       ...   \n",
       " 18182      0.000000       0.000000      0.000000  ...  0.007331  0.013290   \n",
       " 18183      0.999931       0.999605      0.999909  ...  0.008063  0.014976   \n",
       " 18184      0.999931       0.996361      0.999889  ...  0.000000  0.000000   \n",
       " 18185      0.999938       0.999600      0.999914  ...  0.007151  0.014301   \n",
       " 18186      0.999931       0.999612      0.999923  ...  0.006821  0.011577   \n",
       " \n",
       "            a_e6      a_e7      a_c6      a_c2   a_act11 readmission_30d  \\\n",
       " 0      0.500004  0.027925  2.858042  2.068872  0.197515        1.999922   \n",
       " 1      0.538392  0.029193  2.869888  2.048409  0.236205        1.999915   \n",
       " 2      0.446607  0.027651  2.992539  2.017362  0.188988        1.999928   \n",
       " 3      0.447808  0.027851  2.994638  2.017884  0.189275        1.999928   \n",
       " 4      0.447057  0.027726  2.993326  2.017557  0.189096        1.999928   \n",
       " ...         ...       ...       ...       ...       ...             ...   \n",
       " 18182  0.977640  0.033473  3.313914  1.744292  0.267624        1.999960   \n",
       " 18183  0.619012  0.026011  2.928266  1.884847  0.238257        1.999928   \n",
       " 18184  0.000000  0.000000  1.000000  3.000000  3.000000        1.999191   \n",
       " 18185  0.514294  0.021851  2.924754  1.927312  0.235560        1.999923   \n",
       " 18186  0.529615  0.022342  2.947128  1.866192  0.201514        1.999956   \n",
       " \n",
       "        postop_comp        los  \n",
       " 0         0.000000   0.000027  \n",
       " 1         0.997535   0.000031  \n",
       " 2         0.997885  16.000000  \n",
       " 3         0.997903   8.000000  \n",
       " 4         0.997892  13.000000  \n",
       " ...            ...        ...  \n",
       " 18182     0.998914   0.000014  \n",
       " 18183     0.997739   0.000022  \n",
       " 18184     0.996054   0.008485  \n",
       " 18185     0.997756   0.000024  \n",
       " 18186     0.998019   0.000014  \n",
       " \n",
       " [18187 rows x 65 columns],\n",
       " 'Autoencoder':          id redcap_event_name  age_diagnosis gender overall_primary_tumour  \\\n",
       " 0         1                 8            NaN    1.0                    nan   \n",
       " 1         1                 9            NaN    NaN                    nan   \n",
       " 2         1                 9            NaN    NaN                    nan   \n",
       " 3         1                 9            NaN    NaN                    nan   \n",
       " 4         1                 9            NaN    NaN                    nan   \n",
       " ...     ...               ...            ...    ...                    ...   \n",
       " 18182  1770                10            NaN    NaN                    nan   \n",
       " 18183  1770                10            NaN    NaN                    nan   \n",
       " 18184  1770                10            NaN    NaN                    nan   \n",
       " 18185  1770                 8            NaN    1.0                    nan   \n",
       " 18186  1770                11            NaN    NaN                    nan   \n",
       " \n",
       "       overall_regional_ln overall_distant_metastasis neotx___notx  \\\n",
       " 0                     nan                        nan          NaN   \n",
       " 1                     nan                        nan          NaN   \n",
       " 2                     nan                        nan          NaN   \n",
       " 3                     nan                        nan          NaN   \n",
       " 4                     nan                        nan          NaN   \n",
       " ...                   ...                        ...          ...   \n",
       " 18182                 nan                        nan          0.0   \n",
       " 18183                 nan                        nan          NaN   \n",
       " 18184                 nan                        nan          NaN   \n",
       " 18185                 nan                        nan          NaN   \n",
       " 18186                 nan                        nan          NaN   \n",
       " \n",
       "       neotx___chemo neotx___rads  ... a_e4 a_e5 a_e6 a_e7 a_c6 a_c2 a_act11  \\\n",
       " 0               NaN          NaN  ...  NaN  NaN  NaN  NaN  NaN  NaN     NaN   \n",
       " 1               NaN          NaN  ...  NaN  NaN  NaN  NaN  NaN  NaN     NaN   \n",
       " 2               NaN          NaN  ...  NaN  NaN  NaN  NaN  NaN  NaN     NaN   \n",
       " 3               NaN          NaN  ...  NaN  NaN  NaN  NaN  NaN  NaN     NaN   \n",
       " 4               NaN          NaN  ...  NaN  NaN  NaN  NaN  NaN  NaN     NaN   \n",
       " ...             ...          ...  ...  ...  ...  ...  ...  ...  ...     ...   \n",
       " 18182           0.0          0.0  ...  NaN  NaN  NaN  NaN  NaN  NaN     NaN   \n",
       " 18183           NaN          NaN  ...  NaN  NaN  NaN  NaN  NaN  NaN     NaN   \n",
       " 18184           NaN          NaN  ...  0.0  0.0  0.0  0.0  1.0  3.0     3.0   \n",
       " 18185           NaN          NaN  ...  NaN  NaN  NaN  NaN  NaN  NaN     NaN   \n",
       " 18186           NaN          NaN  ...  NaN  NaN  NaN  NaN  NaN  NaN     NaN   \n",
       " \n",
       "       readmission_30d  postop_comp   los  \n",
       " 0                 NaN          0.0   NaN  \n",
       " 1                 NaN          NaN   NaN  \n",
       " 2                 NaN          NaN  16.0  \n",
       " 3                 NaN          NaN   8.0  \n",
       " 4                 NaN          NaN  13.0  \n",
       " ...               ...          ...   ...  \n",
       " 18182             NaN          NaN   NaN  \n",
       " 18183             NaN          NaN   NaN  \n",
       " 18184             NaN          NaN   NaN  \n",
       " 18185             NaN          NaN   NaN  \n",
       " 18186             NaN          NaN   NaN  \n",
       " \n",
       " [18187 rows x 65 columns],\n",
       " 'DAE':          id redcap_event_name  age_diagnosis gender overall_primary_tumour  \\\n",
       " 0         1                 8            NaN    1.0                    nan   \n",
       " 1         1                 9            NaN    NaN                    nan   \n",
       " 2         1                 9            NaN    NaN                    nan   \n",
       " 3         1                 9            NaN    NaN                    nan   \n",
       " 4         1                 9            NaN    NaN                    nan   \n",
       " ...     ...               ...            ...    ...                    ...   \n",
       " 18182  1770                10            NaN    NaN                    nan   \n",
       " 18183  1770                10            NaN    NaN                    nan   \n",
       " 18184  1770                10            NaN    NaN                    nan   \n",
       " 18185  1770                 8            NaN    1.0                    nan   \n",
       " 18186  1770                11            NaN    NaN                    nan   \n",
       " \n",
       "       overall_regional_ln overall_distant_metastasis neotx___notx  \\\n",
       " 0                     nan                        nan          NaN   \n",
       " 1                     nan                        nan          NaN   \n",
       " 2                     nan                        nan          NaN   \n",
       " 3                     nan                        nan          NaN   \n",
       " 4                     nan                        nan          NaN   \n",
       " ...                   ...                        ...          ...   \n",
       " 18182                 nan                        nan          0.0   \n",
       " 18183                 nan                        nan          NaN   \n",
       " 18184                 nan                        nan          NaN   \n",
       " 18185                 nan                        nan          NaN   \n",
       " 18186                 nan                        nan          NaN   \n",
       " \n",
       "       neotx___chemo neotx___rads  ... a_e4 a_e5 a_e6 a_e7 a_c6 a_c2 a_act11  \\\n",
       " 0               NaN          NaN  ...  NaN  NaN  NaN  NaN  NaN  NaN     NaN   \n",
       " 1               NaN          NaN  ...  NaN  NaN  NaN  NaN  NaN  NaN     NaN   \n",
       " 2               NaN          NaN  ...  NaN  NaN  NaN  NaN  NaN  NaN     NaN   \n",
       " 3               NaN          NaN  ...  NaN  NaN  NaN  NaN  NaN  NaN     NaN   \n",
       " 4               NaN          NaN  ...  NaN  NaN  NaN  NaN  NaN  NaN     NaN   \n",
       " ...             ...          ...  ...  ...  ...  ...  ...  ...  ...     ...   \n",
       " 18182           0.0          0.0  ...  NaN  NaN  NaN  NaN  NaN  NaN     NaN   \n",
       " 18183           NaN          NaN  ...  NaN  NaN  NaN  NaN  NaN  NaN     NaN   \n",
       " 18184           NaN          NaN  ...  0.0  0.0  0.0  0.0  1.0  3.0     3.0   \n",
       " 18185           NaN          NaN  ...  NaN  NaN  NaN  NaN  NaN  NaN     NaN   \n",
       " 18186           NaN          NaN  ...  NaN  NaN  NaN  NaN  NaN  NaN     NaN   \n",
       " \n",
       "       readmission_30d  postop_comp   los  \n",
       " 0                 NaN          0.0   NaN  \n",
       " 1                 NaN          NaN   NaN  \n",
       " 2                 NaN          NaN  16.0  \n",
       " 3                 NaN          NaN   8.0  \n",
       " 4                 NaN          NaN  13.0  \n",
       " ...               ...          ...   ...  \n",
       " 18182             NaN          NaN   NaN  \n",
       " 18183             NaN          NaN   NaN  \n",
       " 18184             NaN          NaN   NaN  \n",
       " 18185             NaN          NaN   NaN  \n",
       " 18186             NaN          NaN   NaN  \n",
       " \n",
       " [18187 rows x 65 columns]}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imputed_dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Method</th>\n",
       "      <th>MAE</th>\n",
       "      <th>RMSE</th>\n",
       "      <th>Time (s)</th>\n",
       "      <th>Avg KS Stat</th>\n",
       "      <th>Avg KS p-value</th>\n",
       "      <th>MAE Rank</th>\n",
       "      <th>RMSE Rank</th>\n",
       "      <th>Time (s) Rank</th>\n",
       "      <th>Avg KS Stat Rank</th>\n",
       "      <th>Average Rank</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>GAIN</td>\n",
       "      <td>1.154310</td>\n",
       "      <td>1.316881</td>\n",
       "      <td>0.580499</td>\n",
       "      <td>0.789512</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Original GAIN</td>\n",
       "      <td>1.041114</td>\n",
       "      <td>1.391916</td>\n",
       "      <td>9.689930</td>\n",
       "      <td>0.726424</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>DAE</td>\n",
       "      <td>0.966525</td>\n",
       "      <td>1.177642</td>\n",
       "      <td>3.644855</td>\n",
       "      <td>0.590214</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MICE</td>\n",
       "      <td>0.658630</td>\n",
       "      <td>1.019815</td>\n",
       "      <td>27.133862</td>\n",
       "      <td>0.532098</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Autoencoder</td>\n",
       "      <td>0.842492</td>\n",
       "      <td>1.082844</td>\n",
       "      <td>173.436089</td>\n",
       "      <td>0.588847</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.75</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Method       MAE      RMSE    Time (s)  Avg KS Stat  Avg KS p-value  \\\n",
       "1           GAIN  1.154310  1.316881    0.580499     0.789512             0.0   \n",
       "2  Original GAIN  1.041114  1.391916    9.689930     0.726424             0.0   \n",
       "4            DAE  0.966525  1.177642    3.644855     0.590214             0.0   \n",
       "0           MICE  0.658630  1.019815   27.133862     0.532098             0.0   \n",
       "3    Autoencoder  0.842492  1.082844  173.436089     0.588847             0.0   \n",
       "\n",
       "   MAE Rank  RMSE Rank  Time (s) Rank  Avg KS Stat Rank  Average Rank  \n",
       "1       1.0        2.0            1.0               5.0          2.25  \n",
       "2       2.0        1.0            3.0               4.0          2.50  \n",
       "4       3.0        3.0            2.0               3.0          2.75  \n",
       "0       5.0        5.0            4.0               1.0          3.75  \n",
       "3       4.0        4.0            5.0               2.0          3.75  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jinenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
